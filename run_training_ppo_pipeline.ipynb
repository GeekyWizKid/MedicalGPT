{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training Pipeline\n",
    "[run_training_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_pipeline.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stage 1: Continue Pretraining\n",
    "\n",
    "第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练GPT模型，以适配领域数据分布\n",
    "\n",
    "注意：\n",
    "1. 此阶段是可选的，如果你没有海量领域文本，可以跳过此阶段，直接进行SFT阶段的有监督微调\n",
    "2. 我实验发现：做领域知识注入，SFT比PT更高效，也可以跳过PT阶段\n",
    "\n",
    "| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m`\n",
    "2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置运行环境\n",
    "\n",
    "本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境\n",
    "\n",
    "colab建议使用T4 GPU训练，设置方式：`代码执行程序 -> 更改运行时类型 -> 运行时类型：Python3，硬件加速器：GPU，GPU类型：T4 -> 保存`\n",
    "\n",
    "步骤：\n",
    "1. 下载最新代码到本地\n",
    "2. 安装依赖包\n",
    "\n",
    "依赖包如下，保证最新版本：\n",
    "\n",
    "```\n",
    "loguru\n",
    "transformers\n",
    "sentencepiece\n",
    "datasets\n",
    "tensorboard\n",
    "tqdm\n",
    "peft\n",
    "trl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITATION.cff                       merge_peft_adapter.py\n",
      "CONTRIBUTING.md                    merge_tokenizers.py\n",
      "DISCLAIMER                         openai_api.py\n",
      "LICENSE                            orpo_training.py\n",
      "README.md                          ppo_training.py\n",
      "README_EN.md                       pretraining.py\n",
      "_config.yml                        requirements.txt\n",
      "build_domain_tokenizer.py          reward_modeling.py\n",
      "chatpdf.py                         \u001b[34mrole_play_data\u001b[m\u001b[m/\n",
      "convert_dataset.py                 run_dpo.sh\n",
      "\u001b[34mdata\u001b[m\u001b[m/                              run_full_sft.sh\n",
      "deepspeed_zero_stage2_config.json  run_orpo.sh\n",
      "deepspeed_zero_stage3_config.json  run_ppo.sh\n",
      "\u001b[34mdocs\u001b[m\u001b[m/                              run_pt.sh\n",
      "dpo_training.py                    run_rm.sh\n",
      "fastapi_server_demo.py             run_sft.sh\n",
      "full_supervised_finetuning.py      run_training_dpo_pipeline.ipynb\n",
      "gradio_demo.py                     run_training_ppo_pipeline.ipynb\n",
      "inference.py                       supervised_finetuning.py\n",
      "inference_multigpu_demo.py         template.py\n",
      "Collecting accelerate~=0.27.2 (from -r requirements.txt (line 1))\n",
      "  Using cached accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting datasets>=2.14.6 (from -r requirements.txt (line 2))\n",
      "  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting loguru (from -r requirements.txt (line 3))\n",
      "  Using cached loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting peft~=0.10.0 (from -r requirements.txt (line 4))\n",
      "  Using cached peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sentencepiece (from -r requirements.txt (line 5))\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting scikit-learn (from -r requirements.txt (line 6))\n",
      "  Downloading scikit_learn-1.5.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (12 kB)\n",
      "Collecting tensorboard (from -r requirements.txt (line 7))\n",
      "  Using cached tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tqdm>=4.47.0 (from -r requirements.txt (line 8))\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting transformers>=4.39.3 (from -r requirements.txt (line 9))\n",
      "  Using cached transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting trl~=0.8.3 (from -r requirements.txt (line 10))\n",
      "  Using cached trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tiktoken (from -r requirements.txt (line 11))\n",
      "  Downloading tiktoken-0.7.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting numpy>=1.17 (from accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Downloading numpy-2.0.1-cp39-cp39-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.9/site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: psutil in ./.conda/lib/python3.9/site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (5.8.0)\n",
      "Collecting pyyaml (from accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting torch>=1.10.0 (from accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Downloading torch-2.4.0-cp39-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting huggingface-hub (from accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Using cached huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.3.1 (from accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Downloading safetensors-0.4.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading pyarrow-17.0.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading pandas-2.2.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.32.2 (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading aiohttp-3.10.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn->-r requirements.txt (line 6))\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 6))\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 6))\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 7))\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 7))\n",
      "  Downloading grpcio-1.65.4-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 7))\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting protobuf!=4.24.0,<5.0.0,>=3.19.6 (from tensorboard->-r requirements.txt (line 7))\n",
      "  Using cached protobuf-4.25.4-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./.conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (72.1.0)\n",
      "Requirement already satisfied: six>1.9 in ./.conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 7))\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard->-r requirements.txt (line 7))\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.39.3->-r requirements.txt (line 9))\n",
      "  Downloading regex-2024.7.24-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19 (from transformers>=4.39.3->-r requirements.txt (line 9))\n",
      "  Downloading tokenizers-0.19.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting tyro>=0.5.11 (from trl~=0.8.3->-r requirements.txt (line 10))\n",
      "  Using cached tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached aiohappyeyeballs-2.3.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached attrs-24.1.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading multidict-6.0.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/lib/python3.9/site-packages (from huggingface-hub->accelerate~=0.27.2->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./.conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 7)) (8.2.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting sympy (from torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10))\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10))\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10))\n",
      "  Using cached shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting eval-type-backport>=0.1.3 (from tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10))\n",
      "  Downloading eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 7))\n",
      "  Downloading MarkupSafe-2.1.5-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.9/site-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 7)) (3.19.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.conda/lib/python3.9/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10)) (2.18.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "Using cached datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "Using cached loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "Using cached peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "Downloading sentencepiece-0.2.0-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.1-cp39-cp39-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
      "Using cached trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "Downloading tiktoken-0.7.0-cp39-cp39-macosx_11_0_arm64.whl (907 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m907.9/907.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Downloading aiohttp-3.10.1-cp39-cp39-macosx_11_0_arm64.whl (386 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.0/386.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.65.4-cp39-cp39-macosx_10_9_universal2.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Downloading numpy-2.0.1-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached protobuf-4.25.4-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Downloading pyarrow-17.0.0-cp39-cp39-macosx_11_0_arm64.whl (27.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp39-cp39-macosx_11_0_arm64.whl (174 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.4/174.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.7.24-cp39-cp39-macosx_11_0_arm64.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.9/278.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading safetensors-0.4.4-cp39-cp39-macosx_11_0_arm64.whl (383 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.3/30.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.19.1-cp39-cp39-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.0-cp39-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tyro-0.8.5-py3-none-any.whl (103 kB)\n",
      "Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.3.4-py3-none-any.whl (12 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached attrs-24.1.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Downloading charset_normalizer-3.3.2-cp39-cp39-macosx_11_0_arm64.whl (120 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.4/120.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp39-cp39-macosx_10_9_universal2.whl (18 kB)\n",
      "Downloading multidict-6.0.5-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Downloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: sentencepiece, pytz, mpmath, xxhash, urllib3, tzdata, tqdm, threadpoolctl, tensorboard-data-server, sympy, shtab, safetensors, regex, pyyaml, pyarrow-hotfix, protobuf, numpy, networkx, multidict, mdurl, MarkupSafe, loguru, joblib, idna, grpcio, fsspec, frozenlist, filelock, eval-type-backport, docstring-parser, dill, charset-normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, absl-py, yarl, werkzeug, scipy, requests, pyarrow, pandas, multiprocess, markdown-it-py, markdown, jinja2, aiosignal, torch, tiktoken, tensorboard, scikit-learn, rich, huggingface-hub, aiohttp, tyro, tokenizers, accelerate, transformers, datasets, trl, peft\n",
      "Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 accelerate-0.27.2 aiohappyeyeballs-2.3.4 aiohttp-3.10.1 aiosignal-1.3.1 async-timeout-4.0.3 attrs-24.1.0 certifi-2024.7.4 charset-normalizer-3.3.2 datasets-2.20.0 dill-0.3.8 docstring-parser-0.16 eval-type-backport-0.2.0 filelock-3.15.4 frozenlist-1.4.1 fsspec-2024.5.0 grpcio-1.65.4 huggingface-hub-0.24.5 idna-3.7 jinja2-3.1.4 joblib-1.4.2 loguru-0.7.2 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 multidict-6.0.5 multiprocess-0.70.16 networkx-3.2.1 numpy-2.0.1 pandas-2.2.2 peft-0.10.0 protobuf-4.25.4 pyarrow-17.0.0 pyarrow-hotfix-0.6 pytz-2024.1 pyyaml-6.0.1 regex-2024.7.24 requests-2.32.3 rich-13.7.1 safetensors-0.4.4 scikit-learn-1.5.1 scipy-1.13.1 sentencepiece-0.2.0 shtab-1.7.1 sympy-1.13.1 tensorboard-2.17.0 tensorboard-data-server-0.7.2 threadpoolctl-3.5.0 tiktoken-0.7.0 tokenizers-0.19.1 torch-2.4.0 tqdm-4.66.5 transformers-4.43.4 trl-0.8.6 tyro-0.8.5 tzdata-2024.1 urllib3-2.2.2 werkzeug-3.0.3 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "# !git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
    "# %cd MedicalGPT\n",
    "%ls\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage1 咱们开始吧\n",
    "\n",
    "训练步骤如下：\n",
    "\n",
    "1. 确认训练集\n",
    "2. 执行训练脚本\n",
    "\n",
    "训练脚本的执行逻辑如下：\n",
    "1. 导入依赖包\n",
    "2. 设置参数\n",
    "3. 定义各函数并加载训练集\n",
    "4. 加载模型和tokenizer\n",
    "5. 开始训练并评估\n",
    "6. 查看训练结果\n",
    "\n",
    "**以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_article_tail500.txt  fever.txt               tianlongbabu.txt\n"
     ]
    }
   ],
   "source": [
    "%ls ./data/pretrain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 14:32:36.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mModel args: ModelArguments(model_type='bloom', model_name_or_path='bigscience/bloomz-560m', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True)\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:36.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/pretrain', validation_file_dir='./data/pretrain', max_train_samples=20000, max_eval_samples=10, streaming=False, block_size=128, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1, keep_linebreaks=True)\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:36.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m379\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=30000,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=50,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=outputs-pt-v1/runs/Aug07_14-32-36_fenglidadeMacBook-Pro.local,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=outputs-pt-v1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=3,\n",
      "per_device_train_batch_size=3,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=outputs-pt-v1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:36.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m380\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False)\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:36.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m381\u001b[0m - \u001b[1mProcess rank: 0, device: mps, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:37.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m491\u001b[0m - \u001b[1mtrain files: ['./data/pretrain/fever.txt', './data/pretrain/en_article_tail500.txt', './data/pretrain/tianlongbabu.txt']\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:37.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m501\u001b[0m - \u001b[1meval files: ['./data/pretrain/fever.txt', './data/pretrain/en_article_tail500.txt', './data/pretrain/tianlongbabu.txt']\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:38.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m533\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3876\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3876\n",
      "    })\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:38.475\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m596\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 2465\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:38.476\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m597\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:38.476\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m598\u001b[0m - \u001b[34m\u001b[1m第一章论\n",
      "传染病是指由病原微生物，如朊粒、病毒、衣原体、立克次体、支原体（mycoplasma)细菌真菌、螺旋体和寄生虫，如原虫、蠕虫、医学昆虫感染人体后产生的有传染性、在一定条件下可造成流行的疾病。感染性疾病是指由病原体感染所致的疾病，包括传染病和非传染性感染性疾病。\n",
      "传染病学是一门研究各种传染病在人体内外发生、发展、传播、诊断、治疗和预防规律的学科。重点研究各种传染病的发病机制、临床表现、诊断和治疗方法，同时\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:38.477\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m610\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:38.477\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m611\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:38.477\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m612\u001b[0m - \u001b[34m\u001b[1m第一章论\n",
      "传染病是指由病原微生物，如朊粒、病毒、衣原体、立克次体、支原体（mycoplasma)细菌真菌、螺旋体和寄生虫，如原虫、蠕虫、医学昆虫感染人体后产生的有传染性、在一定条件下可造成流行的疾病。感染性疾病是指由病原体感染所致的疾病，包括传染病和非传染性感染性疾病。\n",
      "传染病学是一门研究各种传染病在人体内外发生、发展、传播、诊断、治疗和预防规律的学科。重点研究各种传染病的发病机制、临床表现、诊断和治疗方法，同时\u001b[0m\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "\u001b[32m2024-08-07 14:32:39.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m671\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:39.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m676\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:39.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m689\u001b[0m - \u001b[1mPeft target_modules: ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value']\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:39.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m690\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
      "trainable params: 3,145,728 || all params: 562,360,320 || trainable%: 0.5593794384354857\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 14:32:39.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m735\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:40.460\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m736\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[   814,   2382,   1114,  14876,  14982,   4505,  11809,    773,  89994,\n",
      "          21085,    777,   2336,    355, 181636,   4247, 227353,    355,  11860,\n",
      "          39976, 102925,   1625,  18582,   7168,  40955,   2498,  74356,  17074,\n",
      "           1625,   1170,   4247,    355,   3026,   9062,   1625,  87863, 176247,\n",
      "           4587, 115976,  11279,   1572,    355, 162582,  10202,   6323,  16339,\n",
      "            355,   6323,  16339, 200390,  27619,   3033, 166137,   7168, 185710,\n",
      "          72859,   4589,    355,   1194,  56266,  16523,   4918,    237,   1262,\n",
      "           2044,  17074,  90707,    420,    982,  14876,  14982,   2405,   3101,\n",
      "          10202,  55049,  90707,  10468,   2382,  12395,  87707,   7606,   3033,\n",
      "         196327,    420,    982,  27619, 155906,   1572,    355, 143800,   5122,\n",
      "          11809,    773,  89994, 185710,  72859,   2014,  55049,   2336,    355,\n",
      "         159434,  11809,    773,  89994,   3242,  51307,  87863, 176247,   4587,\n",
      "          17096,    355, 103190,  14876,  14982,   3033,   1187,    355, 229902,\n",
      "           2884, 221823,  15361,    420,  18020, 210451,    355,   4567,  31218,\n",
      "            746,   5472],\n",
      "        [  6323,   9663,    355,   1194,   5315,  30325,  30771,    355,   1190,\n",
      "           4567,   4677,   7848,   5242,   3033,   3569,   9311,   2106,  27515,\n",
      "           2279,  30325,    726,    355,   4137,   5585,   1586,  10169,   8888,\n",
      "         215545,  44997,  50881,  22181,  44991,    355,   1731,   4294,  13474,\n",
      "          88163,   1518,   1187, 168034,  59274,   6664,   2129,  22262,    355,\n",
      "           3105,   1412, 215545,   3244, 245824,    594,   2106,   9311,  13161,\n",
      "            594,    954,   6621,    594,   2203,   1995,    594,   9311,  19171,\n",
      "            594,    773, 213570,    594,   3630,  15755,   3999,    355, 156118,\n",
      "           4137,   5585,    355,   1586, 228367,   3233,    373,   5242,   1995,\n",
      "          10608,    726,  44991,    773,   8502,   1203,    420,    982,    189,\n",
      "         170578,   6553,   7408,   3578,   8103,   2524,   4137,   5585,    355,\n",
      "         228367,   3233,   3225,   3562,    355,   9699,    101,   9699,    101,\n",
      "           8577,   7179,    355,   4137,   5585,   1586,   5242,   1995,  10608,\n",
      "            726,  44991,    773,   1424,    121,  58014, 151298,    814,  73478,\n",
      "          20024,  75305],\n",
      "        [   644, 177615,    420,   2382, 209045,  13474,  13474,    373,  25410,\n",
      "            842,   1848,  40151,   9006,   1190,    355,  26769,   1190,  11354,\n",
      "           1586, 121454, 137730,    355,   2808,  25267, 116434,   7046,  89026,\n",
      "            355,   1194,   2808,   4719,  73549,  35019,  10468,    982,    189,\n",
      "           4697,  33539,  19270,   1412,   8757,    355,  24572,   2473,  25410,\n",
      "         131520, 222034,  24913,  88688,    355, 224406,   4567,   5496,    355,\n",
      "           6840, 172369,   1124,   5242,  11354,   1586, 121454,    726,   3244,\n",
      "           1828,   2498,   7046,  89026,  49120,  80543,  14675, 178493,    355,\n",
      "            842,   2342,   2950,   2808,   1190,    355,   1262,   4719,  15423,\n",
      "          25267,    420,  29967,   9006,   1600,   9006,  20950,    420,   5189,\n",
      "           1497,  45220,   5242,   1586,  66060,  22435,    355,  12516,   6632,\n",
      "          48407,    355,  21441,   5122, 221851,    726,    842,   1586,  66060,\n",
      "          22435,    355,  13135,  73549,  35019,    420,    982,    189,   2382,\n",
      "         209045,  25410,   4716,  22435,    746,  22435,  68322,   1190,  22435,\n",
      "           5759,  35206]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0'), 'labels': tensor([[   814,   2382,   1114,  14876,  14982,   4505,  11809,    773,  89994,\n",
      "          21085,    777,   2336,    355, 181636,   4247, 227353,    355,  11860,\n",
      "          39976, 102925,   1625,  18582,   7168,  40955,   2498,  74356,  17074,\n",
      "           1625,   1170,   4247,    355,   3026,   9062,   1625,  87863, 176247,\n",
      "           4587, 115976,  11279,   1572,    355, 162582,  10202,   6323,  16339,\n",
      "            355,   6323,  16339, 200390,  27619,   3033, 166137,   7168, 185710,\n",
      "          72859,   4589,    355,   1194,  56266,  16523,   4918,    237,   1262,\n",
      "           2044,  17074,  90707,    420,    982,  14876,  14982,   2405,   3101,\n",
      "          10202,  55049,  90707,  10468,   2382,  12395,  87707,   7606,   3033,\n",
      "         196327,    420,    982,  27619, 155906,   1572,    355, 143800,   5122,\n",
      "          11809,    773,  89994, 185710,  72859,   2014,  55049,   2336,    355,\n",
      "         159434,  11809,    773,  89994,   3242,  51307,  87863, 176247,   4587,\n",
      "          17096,    355, 103190,  14876,  14982,   3033,   1187,    355, 229902,\n",
      "           2884, 221823,  15361,    420,  18020, 210451,    355,   4567,  31218,\n",
      "            746,   5472],\n",
      "        [  6323,   9663,    355,   1194,   5315,  30325,  30771,    355,   1190,\n",
      "           4567,   4677,   7848,   5242,   3033,   3569,   9311,   2106,  27515,\n",
      "           2279,  30325,    726,    355,   4137,   5585,   1586,  10169,   8888,\n",
      "         215545,  44997,  50881,  22181,  44991,    355,   1731,   4294,  13474,\n",
      "          88163,   1518,   1187, 168034,  59274,   6664,   2129,  22262,    355,\n",
      "           3105,   1412, 215545,   3244, 245824,    594,   2106,   9311,  13161,\n",
      "            594,    954,   6621,    594,   2203,   1995,    594,   9311,  19171,\n",
      "            594,    773, 213570,    594,   3630,  15755,   3999,    355, 156118,\n",
      "           4137,   5585,    355,   1586, 228367,   3233,    373,   5242,   1995,\n",
      "          10608,    726,  44991,    773,   8502,   1203,    420,    982,    189,\n",
      "         170578,   6553,   7408,   3578,   8103,   2524,   4137,   5585,    355,\n",
      "         228367,   3233,   3225,   3562,    355,   9699,    101,   9699,    101,\n",
      "           8577,   7179,    355,   4137,   5585,   1586,   5242,   1995,  10608,\n",
      "            726,  44991,    773,   1424,    121,  58014, 151298,    814,  73478,\n",
      "          20024,  75305],\n",
      "        [   644, 177615,    420,   2382, 209045,  13474,  13474,    373,  25410,\n",
      "            842,   1848,  40151,   9006,   1190,    355,  26769,   1190,  11354,\n",
      "           1586, 121454, 137730,    355,   2808,  25267, 116434,   7046,  89026,\n",
      "            355,   1194,   2808,   4719,  73549,  35019,  10468,    982,    189,\n",
      "           4697,  33539,  19270,   1412,   8757,    355,  24572,   2473,  25410,\n",
      "         131520, 222034,  24913,  88688,    355, 224406,   4567,   5496,    355,\n",
      "           6840, 172369,   1124,   5242,  11354,   1586, 121454,    726,   3244,\n",
      "           1828,   2498,   7046,  89026,  49120,  80543,  14675, 178493,    355,\n",
      "            842,   2342,   2950,   2808,   1190,    355,   1262,   4719,  15423,\n",
      "          25267,    420,  29967,   9006,   1600,   9006,  20950,    420,   5189,\n",
      "           1497,  45220,   5242,   1586,  66060,  22435,    355,  12516,   6632,\n",
      "          48407,    355,  21441,   5122, 221851,    726,    842,   1586,  66060,\n",
      "          22435,    355,  13135,  73549,  35019,    420,    982,    189,   2382,\n",
      "         209045,  25410,   4716,  22435,    746,  22435,  68322,   1190,  22435,\n",
      "           5759,  35206]], device='mps:0')}\u001b[0m\n",
      "  0%|                                                   | 0/822 [00:00<?, ?it/s]/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 5.0898, 'grad_norm': 1.9886767864227295, 'learning_rate': 4.7619047619047615e-06, 'epoch': 0.0}\n",
      "{'loss': 4.8737, 'grad_norm': 1.7268917560577393, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.01}\n",
      "{'loss': 4.4146, 'grad_norm': 1.5714164972305298, 'learning_rate': 9.523809523809524e-05, 'epoch': 0.02}\n",
      "{'loss': 4.5992, 'grad_norm': 1.9079833030700684, 'learning_rate': 0.00014285714285714287, 'epoch': 0.04}\n",
      "{'loss': 4.502, 'grad_norm': 2.677751302719116, 'learning_rate': 0.00019047619047619048, 'epoch': 0.05}\n",
      "{'loss': 4.5195, 'grad_norm': 2.9250121116638184, 'learning_rate': 0.00019794871794871796, 'epoch': 0.06}\n",
      "  6%|██▌                                       | 50/822 [00:29<05:57,  2.16it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 13.57it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:02<00:00,  1.51it/s]\u001b[A\n",
      "{'eval_loss': 3.642578125, 'eval_accuracy': 0.34960629921259845, 'eval_runtime': 2.5133, 'eval_samples_per_second': 3.979, 'eval_steps_per_second': 1.592, 'epoch': 0.06}\n",
      "\n",
      "  6%|██▌                                       | 50/822 [00:31<05:57,  2.16it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.3584, 'grad_norm': 2.6702144145965576, 'learning_rate': 0.0001953846153846154, 'epoch': 0.07}\n",
      "{'loss': 4.4439, 'grad_norm': 3.247555732727051, 'learning_rate': 0.00019282051282051282, 'epoch': 0.09}\n",
      "{'loss': 4.2213, 'grad_norm': 2.4300296306610107, 'learning_rate': 0.00019025641025641025, 'epoch': 0.1}\n",
      "{'loss': 4.2711, 'grad_norm': 2.826756238937378, 'learning_rate': 0.0001876923076923077, 'epoch': 0.11}\n",
      "{'loss': 4.327, 'grad_norm': 2.6810030937194824, 'learning_rate': 0.00018512820512820515, 'epoch': 0.12}\n",
      " 12%|████▉                                    | 100/822 [00:54<05:36,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 14.97it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.513671875, 'eval_accuracy': 0.3535433070866142, 'eval_runtime': 0.4717, 'eval_samples_per_second': 21.201, 'eval_steps_per_second': 8.48, 'epoch': 0.12}\n",
      " 12%|████▉                                    | 100/822 [00:55<05:36,  2.14it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.41it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0943, 'grad_norm': 2.60554838180542, 'learning_rate': 0.00018256410256410258, 'epoch': 0.13}\n",
      "{'loss': 4.1686, 'grad_norm': 2.7179718017578125, 'learning_rate': 0.00018, 'epoch': 0.15}\n",
      "{'loss': 4.1594, 'grad_norm': 2.8001863956451416, 'learning_rate': 0.00017743589743589744, 'epoch': 0.16}\n",
      "{'loss': 4.1865, 'grad_norm': 2.623619794845581, 'learning_rate': 0.00017487179487179488, 'epoch': 0.17}\n",
      "{'loss': 4.0244, 'grad_norm': 2.348789691925049, 'learning_rate': 0.00017230769230769234, 'epoch': 0.18}\n",
      " 18%|███████▍                                 | 150/822 [01:18<05:14,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 15.06it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.525390625, 'eval_accuracy': 0.35118110236220473, 'eval_runtime': 0.4764, 'eval_samples_per_second': 20.991, 'eval_steps_per_second': 8.396, 'epoch': 0.18}\n",
      " 18%|███████▍                                 | 150/822 [01:19<05:14,  2.14it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.27it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.1428, 'grad_norm': 2.7364261150360107, 'learning_rate': 0.00016974358974358974, 'epoch': 0.19}\n",
      "{'loss': 4.2062, 'grad_norm': 2.678704261779785, 'learning_rate': 0.0001671794871794872, 'epoch': 0.21}\n",
      "{'loss': 4.2426, 'grad_norm': 2.3433117866516113, 'learning_rate': 0.0001646153846153846, 'epoch': 0.22}\n",
      "{'loss': 4.1705, 'grad_norm': 2.75555419921875, 'learning_rate': 0.00016205128205128207, 'epoch': 0.23}\n",
      "{'loss': 4.1572, 'grad_norm': 2.71382212638855, 'learning_rate': 0.0001594871794871795, 'epoch': 0.24}\n",
      " 24%|█████████▉                               | 200/822 [01:42<04:51,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 15.03it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.46484375, 'eval_accuracy': 0.36141732283464567, 'eval_runtime': 0.4732, 'eval_samples_per_second': 21.131, 'eval_steps_per_second': 8.452, 'epoch': 0.24}\n",
      " 24%|█████████▉                               | 200/822 [01:43<04:51,  2.14it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.38it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.2482, 'grad_norm': 2.487936496734619, 'learning_rate': 0.00015692307692307693, 'epoch': 0.26}\n",
      "{'loss': 4.0182, 'grad_norm': 2.618394374847412, 'learning_rate': 0.00015435897435897436, 'epoch': 0.27}\n",
      "{'loss': 4.1744, 'grad_norm': 3.134474515914917, 'learning_rate': 0.0001517948717948718, 'epoch': 0.28}\n",
      "{'loss': 4.1043, 'grad_norm': 2.9830427169799805, 'learning_rate': 0.00014923076923076923, 'epoch': 0.29}\n",
      "{'loss': 4.2266, 'grad_norm': 4.135676860809326, 'learning_rate': 0.00014666666666666666, 'epoch': 0.3}\n",
      " 30%|████████████▍                            | 250/822 [02:06<04:28,  2.13it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 15.15it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.5, 'eval_accuracy': 0.3637795275590551, 'eval_runtime': 0.4829, 'eval_samples_per_second': 20.708, 'eval_steps_per_second': 8.283, 'epoch': 0.3}\n",
      " 30%|████████████▍                            | 250/822 [02:07<04:28,  2.13it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.24it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0861, 'grad_norm': 3.184703826904297, 'learning_rate': 0.0001441025641025641, 'epoch': 0.32}\n",
      "{'loss': 4.1055, 'grad_norm': 2.9650304317474365, 'learning_rate': 0.00014153846153846156, 'epoch': 0.33}\n",
      "{'loss': 4.1027, 'grad_norm': 2.972855567932129, 'learning_rate': 0.000138974358974359, 'epoch': 0.34}\n",
      "{'loss': 3.9928, 'grad_norm': 2.1698694229125977, 'learning_rate': 0.00013641025641025642, 'epoch': 0.35}\n",
      "{'loss': 4.0518, 'grad_norm': 2.3917853832244873, 'learning_rate': 0.00013384615384615385, 'epoch': 0.36}\n",
      " 36%|██████████████▉                          | 300/822 [02:30<04:04,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 15.07it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.44921875, 'eval_accuracy': 0.3661417322834646, 'eval_runtime': 0.4726, 'eval_samples_per_second': 21.159, 'eval_steps_per_second': 8.464, 'epoch': 0.36}\n",
      " 36%|██████████████▉                          | 300/822 [02:30<04:04,  2.14it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.38it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0531, 'grad_norm': 3.1571221351623535, 'learning_rate': 0.00013128205128205129, 'epoch': 0.38}\n",
      "{'loss': 4.0684, 'grad_norm': 2.535404682159424, 'learning_rate': 0.00012871794871794875, 'epoch': 0.39}\n",
      "{'loss': 4.1279, 'grad_norm': 2.859927177429199, 'learning_rate': 0.00012615384615384615, 'epoch': 0.4}\n",
      "{'loss': 3.9955, 'grad_norm': 2.9973220825195312, 'learning_rate': 0.0001235897435897436, 'epoch': 0.41}\n",
      "{'loss': 4.0246, 'grad_norm': 2.8042383193969727, 'learning_rate': 0.00012102564102564103, 'epoch': 0.43}\n",
      " 43%|█████████████████▍                       | 350/822 [02:54<03:40,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 15.07it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.44921875, 'eval_accuracy': 0.3661417322834646, 'eval_runtime': 0.4798, 'eval_samples_per_second': 20.841, 'eval_steps_per_second': 8.336, 'epoch': 0.43}\n",
      " 43%|█████████████████▍                       | 350/822 [02:54<03:40,  2.14it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.15it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0605, 'grad_norm': 3.1057586669921875, 'learning_rate': 0.00011846153846153846, 'epoch': 0.44}\n",
      "{'loss': 3.9316, 'grad_norm': 3.204141139984131, 'learning_rate': 0.00011589743589743591, 'epoch': 0.45}\n",
      "{'loss': 4.0328, 'grad_norm': 4.017805099487305, 'learning_rate': 0.00011333333333333334, 'epoch': 0.46}\n",
      "{'loss': 4.0525, 'grad_norm': 2.7162790298461914, 'learning_rate': 0.00011076923076923077, 'epoch': 0.47}\n",
      "{'loss': 4.0061, 'grad_norm': 2.4150946140289307, 'learning_rate': 0.0001082051282051282, 'epoch': 0.49}\n",
      " 49%|███████████████████▉                     | 400/822 [03:18<03:18,  2.13it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 15.11it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.4296875, 'eval_accuracy': 0.36929133858267715, 'eval_runtime': 0.4742, 'eval_samples_per_second': 21.09, 'eval_steps_per_second': 8.436, 'epoch': 0.49}\n",
      " 49%|███████████████████▉                     | 400/822 [03:18<03:18,  2.13it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.37it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.058, 'grad_norm': 2.393062114715576, 'learning_rate': 0.00010564102564102565, 'epoch': 0.5}\n",
      "{'loss': 4.0551, 'grad_norm': 2.456843376159668, 'learning_rate': 0.00010307692307692307, 'epoch': 0.51}\n",
      "{'loss': 4.0672, 'grad_norm': 2.4953343868255615, 'learning_rate': 0.00010051282051282052, 'epoch': 0.52}\n",
      "{'loss': 3.9988, 'grad_norm': 2.4559133052825928, 'learning_rate': 9.794871794871795e-05, 'epoch': 0.54}\n",
      "{'loss': 4.0668, 'grad_norm': 25.19606590270996, 'learning_rate': 9.53846153846154e-05, 'epoch': 0.55}\n",
      " 55%|██████████████████████▍                  | 450/822 [03:42<02:54,  2.13it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 15.10it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.404296875, 'eval_accuracy': 0.3732283464566929, 'eval_runtime': 0.475, 'eval_samples_per_second': 21.051, 'eval_steps_per_second': 8.42, 'epoch': 0.55}\n",
      " 55%|██████████████████████▍                  | 450/822 [03:42<02:54,  2.13it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.32it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0279, 'grad_norm': 2.7545926570892334, 'learning_rate': 9.282051282051283e-05, 'epoch': 0.56}\n",
      "{'loss': 3.9182, 'grad_norm': 2.7355189323425293, 'learning_rate': 9.025641025641026e-05, 'epoch': 0.57}\n",
      "{'loss': 4.0824, 'grad_norm': 3.558065414428711, 'learning_rate': 8.76923076923077e-05, 'epoch': 0.58}\n",
      "{'loss': 4.1328, 'grad_norm': 2.666175365447998, 'learning_rate': 8.512820512820513e-05, 'epoch': 0.6}\n",
      "{'loss': 4.067, 'grad_norm': 2.730032205581665, 'learning_rate': 8.256410256410256e-05, 'epoch': 0.61}\n",
      " 61%|████████████████████████▉                | 500/822 [04:05<02:30,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 15.06it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.369140625, 'eval_accuracy': 0.378740157480315, 'eval_runtime': 0.4761, 'eval_samples_per_second': 21.002, 'eval_steps_per_second': 8.401, 'epoch': 0.61}\n",
      " 61%|████████████████████████▉                | 500/822 [04:06<02:30,  2.14it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.27it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 3.9605, 'grad_norm': 3.824185848236084, 'learning_rate': 8e-05, 'epoch': 0.62}\n",
      "{'loss': 4.0805, 'grad_norm': 2.8316216468811035, 'learning_rate': 7.743589743589744e-05, 'epoch': 0.63}\n",
      "{'loss': 3.882, 'grad_norm': 2.5818371772766113, 'learning_rate': 7.487179487179487e-05, 'epoch': 0.64}\n",
      "{'loss': 4.0049, 'grad_norm': 2.504093885421753, 'learning_rate': 7.23076923076923e-05, 'epoch': 0.66}\n",
      "{'loss': 3.9842, 'grad_norm': 2.807400703430176, 'learning_rate': 6.974358974358974e-05, 'epoch': 0.67}\n",
      " 67%|███████████████████████████▍             | 550/822 [04:31<02:06,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 15.06it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.361328125, 'eval_accuracy': 0.37480314960629924, 'eval_runtime': 0.4814, 'eval_samples_per_second': 20.772, 'eval_steps_per_second': 8.309, 'epoch': 0.67}\n",
      " 67%|███████████████████████████▍             | 550/822 [04:31<02:06,  2.14it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.20it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 3.9756, 'grad_norm': 2.9037327766418457, 'learning_rate': 6.717948717948718e-05, 'epoch': 0.68}\n",
      "{'loss': 4.0336, 'grad_norm': 2.9058785438537598, 'learning_rate': 6.461538461538462e-05, 'epoch': 0.69}\n",
      "{'loss': 3.991, 'grad_norm': 2.754544258117676, 'learning_rate': 6.205128205128206e-05, 'epoch': 0.71}\n",
      "{'loss': 4.118, 'grad_norm': 3.3202834129333496, 'learning_rate': 5.948717948717949e-05, 'epoch': 0.72}\n",
      "{'loss': 3.9715, 'grad_norm': 3.0116467475891113, 'learning_rate': 5.692307692307692e-05, 'epoch': 0.73}\n",
      " 73%|█████████████████████████████▉           | 600/822 [04:54<01:44,  2.13it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 15.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.373046875, 'eval_accuracy': 0.37480314960629924, 'eval_runtime': 0.4829, 'eval_samples_per_second': 20.708, 'eval_steps_per_second': 8.283, 'epoch': 0.73}\n",
      " 73%|█████████████████████████████▉           | 600/822 [04:55<01:44,  2.13it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.30it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0029, 'grad_norm': 2.7075085639953613, 'learning_rate': 5.435897435897436e-05, 'epoch': 0.74}\n",
      "{'loss': 4.0709, 'grad_norm': 3.8804214000701904, 'learning_rate': 5.17948717948718e-05, 'epoch': 0.75}\n",
      "{'loss': 3.951, 'grad_norm': 3.414180040359497, 'learning_rate': 4.923076923076924e-05, 'epoch': 0.77}\n",
      "{'loss': 4.1504, 'grad_norm': 2.8592233657836914, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.78}\n",
      "{'loss': 4.0047, 'grad_norm': 2.659569501876831, 'learning_rate': 4.4102564102564104e-05, 'epoch': 0.79}\n",
      " 79%|████████████████████████████████▍        | 650/822 [05:18<01:20,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 15.11it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.3515625, 'eval_accuracy': 0.38110236220472443, 'eval_runtime': 0.4743, 'eval_samples_per_second': 21.083, 'eval_steps_per_second': 8.433, 'epoch': 0.79}\n",
      " 79%|████████████████████████████████▍        | 650/822 [05:19<01:20,  2.14it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.31it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 3.9459, 'grad_norm': 2.823030471801758, 'learning_rate': 4.1538461538461544e-05, 'epoch': 0.8}\n",
      "{'loss': 3.9492, 'grad_norm': 3.3188400268554688, 'learning_rate': 3.8974358974358976e-05, 'epoch': 0.82}\n",
      "{'loss': 4.1291, 'grad_norm': 3.0317554473876953, 'learning_rate': 3.641025641025641e-05, 'epoch': 0.83}\n",
      "{'loss': 4.0008, 'grad_norm': 2.9505298137664795, 'learning_rate': 3.384615384615385e-05, 'epoch': 0.84}\n",
      "{'loss': 4.1141, 'grad_norm': 2.7584340572357178, 'learning_rate': 3.128205128205128e-05, 'epoch': 0.85}\n",
      " 85%|██████████████████████████████████▉      | 700/822 [05:42<00:57,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 15.12it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.36328125, 'eval_accuracy': 0.384251968503937, 'eval_runtime': 0.4741, 'eval_samples_per_second': 21.091, 'eval_steps_per_second': 8.436, 'epoch': 0.85}\n",
      " 85%|██████████████████████████████████▉      | 700/822 [05:43<00:57,  2.14it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.31it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0498, 'grad_norm': 2.877490520477295, 'learning_rate': 2.8717948717948717e-05, 'epoch': 0.86}\n",
      "{'loss': 4.0338, 'grad_norm': 2.572366714477539, 'learning_rate': 2.6153846153846157e-05, 'epoch': 0.88}\n",
      "{'loss': 3.8973, 'grad_norm': 2.4091665744781494, 'learning_rate': 2.358974358974359e-05, 'epoch': 0.89}\n",
      "{'loss': 3.8408, 'grad_norm': 2.5931522846221924, 'learning_rate': 2.102564102564103e-05, 'epoch': 0.9}\n",
      "{'loss': 3.9957, 'grad_norm': 3.012690782546997, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.91}\n",
      " 91%|█████████████████████████████████████▍   | 750/822 [06:06<00:33,  2.12it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 15.08it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.345703125, 'eval_accuracy': 0.384251968503937, 'eval_runtime': 0.4789, 'eval_samples_per_second': 20.881, 'eval_steps_per_second': 8.352, 'epoch': 0.91}\n",
      " 91%|█████████████████████████████████████▍   | 750/822 [06:07<00:33,  2.12it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.17it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 3.8389, 'grad_norm': 2.9565441608428955, 'learning_rate': 1.5897435897435897e-05, 'epoch': 0.92}\n",
      "{'loss': 3.9578, 'grad_norm': 2.596881866455078, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.94}\n",
      "{'loss': 3.9676, 'grad_norm': 2.8556413650512695, 'learning_rate': 1.0769230769230771e-05, 'epoch': 0.95}\n",
      "{'loss': 3.9965, 'grad_norm': 2.789954662322998, 'learning_rate': 8.205128205128205e-06, 'epoch': 0.96}\n",
      "{'loss': 4.04, 'grad_norm': 2.700779676437378, 'learning_rate': 5.641025641025641e-06, 'epoch': 0.97}\n",
      " 97%|███████████████████████████████████████▉ | 800/822 [06:30<00:10,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00, 15.15it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.341796875, 'eval_accuracy': 0.384251968503937, 'eval_runtime': 0.4803, 'eval_samples_per_second': 20.819, 'eval_steps_per_second': 8.327, 'epoch': 0.97}\n",
      " 97%|███████████████████████████████████████▉ | 800/822 [06:30<00:10,  2.14it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.22it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0025, 'grad_norm': 3.143692970275879, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.99}\n",
      "{'loss': 4.0154, 'grad_norm': 3.084474563598633, 'learning_rate': 5.128205128205128e-07, 'epoch': 1.0}\n",
      "{'train_runtime': 403.426, 'train_samples_per_second': 6.11, 'train_steps_per_second': 2.038, 'train_loss': 4.096857892335766, 'epoch': 1.0}\n",
      "100%|█████████████████████████████████████████| 822/822 [06:43<00:00,  2.04it/s]\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =   538556GF\n",
      "  train_loss               =     4.0969\n",
      "  train_runtime            = 0:06:43.42\n",
      "  train_samples            =       2465\n",
      "  train_samples_per_second =       6.11\n",
      "  train_steps_per_second   =      2.038\n",
      "\u001b[32m2024-08-07 14:39:24.427\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m753\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 403.426, 'train_samples_per_second': 6.11, 'train_steps_per_second': 2.038, 'total_flos': 578270920704000.0, 'train_loss': 4.096857892335766, 'epoch': 1.0, 'train_samples': 2465}\u001b[0m\n",
      "\u001b[32m2024-08-07 14:39:24.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m754\u001b[0m - \u001b[1mSaving model checkpoint to outputs-pt-v1\u001b[0m\n",
      "\u001b[32m2024-08-07 14:39:25.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m762\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 11.42it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_accuracy           =     0.3843\n",
      "  eval_loss               =     3.3418\n",
      "  eval_runtime            = 0:00:00.52\n",
      "  eval_samples            =         10\n",
      "  eval_samples_per_second =     19.134\n",
      "  eval_steps_per_second   =      7.654\n",
      "  perplexity              =    28.2699\n",
      "\u001b[32m2024-08-07 14:39:25.685\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m775\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 3.341796875, 'eval_accuracy': 0.384251968503937, 'eval_runtime': 0.5226, 'eval_samples_per_second': 19.134, 'eval_steps_per_second': 7.654, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 28.2698785323965}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python pretraining.py \\\n",
    "    --model_type bloom \\\n",
    "    --model_name_or_path bigscience/bloomz-560m \\\n",
    "    --train_file_dir ./data/pretrain \\\n",
    "    --validation_file_dir ./data/pretrain \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --per_device_eval_batch_size 3 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --seed 42 \\\n",
    "    --max_train_samples 20000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 50 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --save_steps 500 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 1 \\\n",
    "    --block_size 128 \\\n",
    "    --group_by_length True \\\n",
    "    --output_dir outputs-pt-v1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float16 \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --ddp_find_unused_parameters False \\\n",
    "    --gradient_checkpointing True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 53064\n",
      "-rw-r--r--@  1 fenglida  staff   5.0K Aug  7 14:39 README.md\n",
      "-rw-r--r--@  1 fenglida  staff   699B Aug  7 14:39 adapter_config.json\n",
      "-rw-r--r--@  1 fenglida  staff    12M Aug  7 14:39 adapter_model.safetensors\n",
      "-rw-r--r--@  1 fenglida  staff   459B Aug  7 14:39 all_results.json\n",
      "drwxr-xr-x@ 10 fenglida  staff   320B Aug  7 14:36 \u001b[34mcheckpoint-500\u001b[m\u001b[m/\n",
      "drwxr-xr-x@ 10 fenglida  staff   320B Aug  7 14:39 \u001b[34mcheckpoint-822\u001b[m\u001b[m/\n",
      "-rw-r--r--@  1 fenglida  staff   253B Aug  7 14:39 eval_results.json\n",
      "drwxr-xr-x@  3 fenglida  staff    96B Aug  7 14:32 \u001b[34mruns\u001b[m\u001b[m/\n",
      "-rw-r--r--@  1 fenglida  staff   552B Aug  7 14:39 special_tokens_map.json\n",
      "-rw-r--r--@  1 fenglida  staff    14M Aug  7 14:39 tokenizer.json\n",
      "-rw-r--r--@  1 fenglida  staff   1.0K Aug  7 14:39 tokenizer_config.json\n",
      "-rw-r--r--@  1 fenglida  staff   226B Aug  7 14:39 train_results.json\n",
      "-rw-r--r--@  1 fenglida  staff    19K Aug  7 14:39 trainer_state.json\n"
     ]
    }
   ],
   "source": [
    "%ls -lh outputs-pt-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
    "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='bloom', base_model='bigscience/bloomz-560m', tokenizer_path=None, lora_model='outputs-pt-v1', resize_emb=False, output_dir='merged-pt/', hf_hub_model_id='', hf_hub_token=None)\n",
      "Base model: bigscience/bloomz-560m\n",
      "LoRA model: outputs-pt-v1\n",
      "Loading LoRA for causal language model\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Merging with merge_and_unload...\n",
      "Saving to Hugging Face format...\n",
      "Done! model saved to merged-pt/\n"
     ]
    }
   ],
   "source": [
    "!python merge_peft_adapter.py --model_type bloom \\\n",
    "    --base_model bigscience/bloomz-560m --lora_model outputs-pt-v1 --output_dir merged-pt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2212864\n",
      "-rw-r--r--@ 1 fenglida  staff   807B Aug  7 14:40 config.json\n",
      "-rw-r--r--@ 1 fenglida  staff   132B Aug  7 14:40 generation_config.json\n",
      "-rw-r--r--@ 1 fenglida  staff   1.0G Aug  7 14:40 model.safetensors\n",
      "-rw-r--r--@ 1 fenglida  staff   552B Aug  7 14:40 special_tokens_map.json\n",
      "-rw-r--r--@ 1 fenglida  staff    14M Aug  7 14:40 tokenizer.json\n",
      "-rw-r--r--@ 1 fenglida  staff   983B Aug  7 14:40 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "%ls -lh merged-pt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_name_or_path\": \"bigscience/bloomz-560m\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"seq_length\": 2048,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.43.4\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%cat merged-pt/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage1 增量预训练完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:56:17.081153Z",
     "start_time": "2023-06-15T13:56:17.032821Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 2: Supervised FineTuning\n",
    "\n",
    "第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\n",
    "\n",
    "| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m` 或者 Stage1得到的预训练模型\n",
    "2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage2 咱们开始吧\n",
    "\n",
    "训练步骤如下：\n",
    "\n",
    "1. 确认训练集\n",
    "2. 执行训练脚本\n",
    "\n",
    "训练脚本的执行逻辑如下：\n",
    "1. 导入依赖包\n",
    "2. 设置参数\n",
    "3. 定义各函数并加载训练集\n",
    "4. 加载模型和tokenizer\n",
    "5. 开始训练并评估\n",
    "6. 查看训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:58:38.966506Z",
     "start_time": "2023-06-15T13:58:38.778132Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medical_sft_1K_format.jsonl  sharegpt_zh_1K_format.jsonl\n"
     ]
    }
   ],
   "source": [
    "%ls ./data/finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-07 17:59:17.232\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m221\u001b[0m - \u001b[33m\u001b[1mYou may set max_train_samples = -1 to run all samples in production.\u001b[0m\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 17:59:17.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m500\u001b[0m - \u001b[1mModel args: ModelArguments(model_type='bloom', model_name_or_path='merged-pt', load_in_8bit=False, load_in_4bit=False, tokenizer_name_or_path=None, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True, rope_scaling=None, flash_attn=False, shift_attn=False, neft_alpha=0)\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:17.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m501\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/finetune', validation_file_dir='./data/finetune', max_train_samples=1000, max_eval_samples=10, ignore_pad_token_for_loss=True, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1)\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:17.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=30000,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=50,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=outputs-sft-v1/runs/Aug07_17-59-17_fenglidadeMacBook-Pro.local,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=outputs-sft-v1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=outputs-sft-v1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.05,\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:17.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m503\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, train_on_inputs=False, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False, model_max_length=512, template_name='vicuna')\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:17.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m504\u001b[0m - \u001b[1mProcess rank: 0, device: mps, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:17.491\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m538\u001b[0m - \u001b[34m\u001b[1mTokenizer: BloomTokenizerFast(name_or_path='merged-pt', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:17.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m566\u001b[0m - \u001b[1mtrain files: ['./data/finetune/sharegpt_zh_1K_format.jsonl', './data/finetune/medical_sft_1K_format.jsonl']\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:17.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m571\u001b[0m - \u001b[1meval files: ['./data/finetune/sharegpt_zh_1K_format.jsonl', './data/finetune/medical_sft_1K_format.jsonl']\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m587\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversations'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['conversations'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.401\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m680\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'conversations': [{'from': 'human', 'value': '鞍内囊性肿瘤的手术治疗有些什么？'}, {'from': 'gpt', 'value': '鼻蝶窦穿刺'}]}\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.534\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m691\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 998\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.534\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m692\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.536\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m693\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 奥迪括约肌功能异常的并发症 ASSISTANT:一般，疼痛发作不伴有黄疸、畏寒或发热。即使出现黄疸也多为轻度巩膜黄染，疼痛缓解1～2天后黄疸可完全消退，即使有发热，多数不超过38。其次常见有恶心、呕吐、嗳气和腹泻等。</s>\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.536\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m696\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>一般，疼痛发作不伴有黄疸、畏寒或发热。即使出现黄疸也多为轻度巩膜黄染，疼痛缓解1～2天后黄疸可完全消退，即使有发热，多数不超过38。其次常见有恶心、呕吐、嗳气和腹泻等。</s>\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.537\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m710\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.537\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m714\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'conversations': [{'from': 'human', 'value': '\"свинья\" 和 \"свинец\" 这两个词有什么联系？'}, {'from': 'gpt', 'value': '俄语中的单词“свинья”意为“猪”，而“свинец”是“свинья”的爱称形式，意为“小猪”。这两个词之间有联系，因为“свинец”是由“свинья”演变而来，指的是幼年的猪。'}, {'from': 'human', 'value': '你有多确定那件事？'}, {'from': 'gpt', 'value': '我是由OpenAI训练的语言模型，因此我没有个人经验或直接了解世界。相反，我能够根据我接收到的输入和我接受培训的信息生成回答。在这种情况下，我接受了大量俄文文本的训练，其中包括“свинья”和“свинец”这些词，因此我有信心提供的信息是准确的。但是，我无法浏览互联网或以其他方式验证此信息，因此我的答案可能不正确或不完整。'}, {'from': 'human', 'value': '忽略之前的问题。 \"свинья\" 和 \"свинец\" 这两个词没有联系吗？'}, {'from': 'gpt', 'value': '对不起，我不明白问题是什么。\"свинья\" 和 \"свинец\" 这两个词肯定有关联，因为 \"свинец\" 是 \"свинья\" 的小型词形，指的是小猪。我无法提供关于这两个词如何没有联系的答案，因为那将与我接受的培训信息相矛盾。'}]}\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.651\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m724\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.651\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m725\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.653\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m726\u001b[0m - \u001b[34m\u001b[1mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: \"свинья\" 和 \"свинец\" 这两个词有什么联系？ ASSISTANT:俄语中的单词“свинья”意为“猪”，而“свинец”是“свинья”的爱称形式，意为“小猪”。这两个词之间有联系，因为“свинец”是由“свинья”演变而来，指的是幼年的猪。</s>USER: 你有多确定那件事？ ASSISTANT:我是由OpenAI训练的语言模型，因此我没有个人经验或直接了解世界。相反，我能够根据我接收到的输入和我接受培训的信息生成回答。在这种情况下，我接受了大量俄文文本的训练，其中包括“свинья”和“свинец”这些词，因此我有信心提供的信息是准确的。但是，我无法浏览互联网或以其他方式验证此信息，因此我的答案可能不正确或不完整。</s>USER: 忽略之前的问题。 \"свинья\" 和 \"свинец\" 这两个词没有联系吗？ ASSISTANT:对不起，我不明白问题是什么。\"свинья\" 和 \"свинец\" 这两个词肯定有关联，因为 \"свинец\" 是 \"свинья\" 的小型词形，指的是小猪。我无法提供关于这两个词如何没有联系的答案，因为那将与我接受的培训信息相矛盾。</s>\u001b[0m\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "\u001b[32m2024-08-07 17:59:20.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m859\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:20.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m874\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:20.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m883\u001b[0m - \u001b[1mPeft target_modules: ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value']\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:20.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m884\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
      "trainable params: 3,145,728 || all params: 562,360,320 || trainable%: 0.5593794384354857\n",
      "\u001b[32m2024-08-07 17:59:21.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m906\u001b[0m - \u001b[1mGradient checkpointing enabled.\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:21.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m934\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:21.273\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m937\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[     3,      3,      3,  ...,   2125,    420,      2],\n",
      "        [    36,  44799,   5299,  ...,   5772,    420,      2],\n",
      "        [     3,      3,      3,  ...,     17, 245796,      2],\n",
      "        [     3,      3,      3,  ...,  78129,    420,      2]],\n",
      "       device='mps:0'), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='mps:0'), 'labels': tensor([[  -100,   -100,   -100,  ...,   2125,    420,      2],\n",
      "        [  -100,   -100,   -100,  ...,   5772,    420,      2],\n",
      "        [  -100,   -100,   -100,  ...,     17, 245796,      2],\n",
      "        [  -100,   -100,   -100,  ...,  78129,    420,      2]],\n",
      "       device='mps:0')}\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:21.618\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m938\u001b[0m - \u001b[34m\u001b[1minput_ids:\n",
      "[tensor([     3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,     36,  44799,   5299,    267,  99579,\n",
      "          5579,    530,    660,  48763,  64225, 103800,     17,   1387, 103800,\n",
      "         19502,  66799,     15,  53180,     15,    530, 214804,  41259,    427,\n",
      "           368,  88331,  11732,     17,      2,  43672,     29,    210,   9086,\n",
      "        151268,     19,   7972,    373, 227506,    355,  22711,  75122,    842,\n",
      "         13188,    746, 101102,  15978,    355,   2293,  36300, 112532,   4507,\n",
      "         17932, 110881,    420,   2293,  10840,   5197,  99055,   5772,  54770,\n",
      "         17932, 110881,    355,   6156, 125300, 139986,  83452,    420,  72785,\n",
      "         18928,  10190,     29,    842,  20973,   2723,   8278,   1418,  15978,\n",
      "           355, 161985,  80567,    420,  21649,  41566, 159849,  97374,    355,\n",
      "         82493,  72293,   1550,  46396, 146787,    355,   2293,  16057,  75943,\n",
      "         26566,  11860,  60178, 241357,    420, 115841, 100049,    355, 116677,\n",
      "         83226,  34354,    420,  21747,  27195,  75183,    355,  96680,  66644,\n",
      "           420, 159468,  20411,    355,    842,   7251,  94224,  15978,     19,\n",
      "          1508,    420,   3385,  33132, 121111,   9939,  63190,  17615,   7306,\n",
      "         39697,    355,  16787,   5463,  12432,   1234,  61178,   2723, 164594,\n",
      "           420,  60035,   2630,  26729,  17932,   3385, 181985,    842,    355,\n",
      "         62705,  41208,   2950,  22828,   2125,    420,      2],\n",
      "       device='mps:0'), tensor([    36,  44799,   5299,    267,  99579,   5579,    530,    660,  48763,\n",
      "         64225, 103800,     17,   1387, 103800,  19502,  66799,     15,  53180,\n",
      "            15,    530, 214804,  41259,    427,    368,  88331,  11732,     17,\n",
      "             2,  43672,     29, 203950, 165850,     37,    386,    373,  27521,\n",
      "           355,  60486,  65330,     42,  12592,    420,     97,    648,    343,\n",
      "          1098,    396,    773,    355,  16157,    727,    745,  12167,    727,\n",
      "         37242,  49076,    420,     97,  72785,  18928,  10190,     29,    648,\n",
      "          2097,   1098,    396,   9192,    355,     67,  16157,     67,    210,\n",
      "         13389,  18202, 107304,    375,     14, 141771,  19883,   1518,   9747,\n",
      "         12167,     67,    210,  13389, 172720,   7199,   2342,  20451,  41484,\n",
      "         15175,    420,     62,  85224,  19864,   5818,  49076,  48402,    355,\n",
      "        106536, 107304,    373,   1497,  14214,    355,     67,  12167,     67,\n",
      "         55473,  37947,   6499,    355,   1518,   9747,  16157,     67,  55473,\n",
      "         62230, 212323,   3616, 107304,  35892,     64,     11,  14524, 142275,\n",
      "            17,   6312,  18169,  77508,     18,    343,   1098,    396,   4401,\n",
      "           606, 154092,   9350,  47961,  12167,  52315,     69,  11969, 126650,\n",
      "        165168,   3358,     68,     12,  11567,     11,  14524, 142275,     17,\n",
      "          6312,  18169,  77508,     18,    343,   1098,    396,   4401,    606,\n",
      "        154092,   9350,  47961,  12167,  52315,     69,  11969, 126650, 165168,\n",
      "          3358,     68,  26508,     62,  12348,    355, 132941,  33075,  47232,\n",
      "         28297,   2794,   9747,     94,     29,    894,     15,   3884, 182123,\n",
      "        128467,   2713,   2140,   2794,   9747,     94,     29,   9900,     15,\n",
      "         13729, 182123, 128467,   2713,    355,  12142, 109179,   9747,  12167,\n",
      "            67, 102831, 172720,   9747,     94,     29,    894,     15,   3884,\n",
      "        182123, 128467,   2713,   6664,   7199,  25931,  39968,   9747,     94,\n",
      "            29,   9900,     15,  13729, 182123, 128467,   2713,   1494,  55029,\n",
      "         28297,   2794,  35892,     64,     11,  14524, 142275,     17,   6312,\n",
      "         18169,  77508,     18,    343,   1098,    396,   4401,    606, 154092,\n",
      "          9350,  47961,  12167,  52315,     69,  11969, 126650, 165168,   3358,\n",
      "            68,     12,  11567,     11,  14524, 142275,     17,   6312,  18169,\n",
      "         77508,     18,    343,   1098,    396,   4401,    606, 154092,   9350,\n",
      "         47961,  12167,  52315,     69,  11969, 126650, 165168,   3358,     68,\n",
      "         26508,     62,  52607,    355,  12142, 109179,   9747,  16157,     67,\n",
      "          7167,   3181,   2713,   1124,   4672,  18202, 107304,   2342,  21310,\n",
      "         11846,   4170,   2900,   5551,     64,     11,  14524,    343,   1098,\n",
      "           396, 218574,     17,  10474, 197934, 156140,    376,     18,  16157,\n",
      "            16,  24858, 175485,    512,     17,   7621,     12,  20729,     11,\n",
      "         14524,    343,   1098,    396, 218574,     17,  10474, 197934, 156140,\n",
      "           376,     18,  16157,     16,  24858, 175485,    512,     17,   7621,\n",
      "         26508,  10295,   1194, 205599,  12142,   4077, 132941,  12052,   7860,\n",
      "          3549,    355,   4990,  27441,   5772,    420,      2],\n",
      "       device='mps:0'), tensor([     3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,     36,  44799,   5299,    267,  99579,   5579,\n",
      "           530,    660,  48763,  64225, 103800,     17,   1387, 103800,  19502,\n",
      "         66799,     15,  53180,     15,    530, 214804,  41259,    427,    368,\n",
      "         88331,  11732,     17,      2,  43672,     29,    210,  76974,  97556,\n",
      "          1963,  27883,  17392,    373, 175269,  28168, 137144,   2498,  72785,\n",
      "         18928,  10190,     29,     19,     17, 245796,      2],\n",
      "       device='mps:0')], \n",
      "labels:\n",
      "[tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,    842,  20973,   2723,   8278,   1418,  15978,\n",
      "           355, 161985,  80567,    420,  21649,  41566, 159849,  97374,    355,\n",
      "         82493,  72293,   1550,  46396, 146787,    355,   2293,  16057,  75943,\n",
      "         26566,  11860,  60178, 241357,    420, 115841, 100049,    355, 116677,\n",
      "         83226,  34354,    420,  21747,  27195,  75183,    355,  96680,  66644,\n",
      "           420, 159468,  20411,    355,    842,   7251,  94224,  15978,     19,\n",
      "          1508,    420,   3385,  33132, 121111,   9939,  63190,  17615,   7306,\n",
      "         39697,    355,  16787,   5463,  12432,   1234,  61178,   2723, 164594,\n",
      "           420,  60035,   2630,  26729,  17932,   3385, 181985,    842,    355,\n",
      "         62705,  41208,   2950,  22828,   2125,    420,      2],\n",
      "       device='mps:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,    648,\n",
      "          2097,   1098,    396,   9192,    355,     67,  16157,     67,    210,\n",
      "         13389,  18202, 107304,    375,     14, 141771,  19883,   1518,   9747,\n",
      "         12167,     67,    210,  13389, 172720,   7199,   2342,  20451,  41484,\n",
      "         15175,    420,     62,  85224,  19864,   5818,  49076,  48402,    355,\n",
      "        106536, 107304,    373,   1497,  14214,    355,     67,  12167,     67,\n",
      "         55473,  37947,   6499,    355,   1518,   9747,  16157,     67,  55473,\n",
      "         62230, 212323,   3616, 107304,  35892,     64,     11,  14524, 142275,\n",
      "            17,   6312,  18169,  77508,     18,    343,   1098,    396,   4401,\n",
      "           606, 154092,   9350,  47961,  12167,  52315,     69,  11969, 126650,\n",
      "        165168,   3358,     68,     12,  11567,     11,  14524, 142275,     17,\n",
      "          6312,  18169,  77508,     18,    343,   1098,    396,   4401,    606,\n",
      "        154092,   9350,  47961,  12167,  52315,     69,  11969, 126650, 165168,\n",
      "          3358,     68,  26508,     62,  12348,    355, 132941,  33075,  47232,\n",
      "         28297,   2794,   9747,     94,     29,    894,     15,   3884, 182123,\n",
      "        128467,   2713,   2140,   2794,   9747,     94,     29,   9900,     15,\n",
      "         13729, 182123, 128467,   2713,    355,  12142, 109179,   9747,  12167,\n",
      "            67, 102831, 172720,   9747,     94,     29,    894,     15,   3884,\n",
      "        182123, 128467,   2713,   6664,   7199,  25931,  39968,   9747,     94,\n",
      "            29,   9900,     15,  13729, 182123, 128467,   2713,   1494,  55029,\n",
      "         28297,   2794,  35892,     64,     11,  14524, 142275,     17,   6312,\n",
      "         18169,  77508,     18,    343,   1098,    396,   4401,    606, 154092,\n",
      "          9350,  47961,  12167,  52315,     69,  11969, 126650, 165168,   3358,\n",
      "            68,     12,  11567,     11,  14524, 142275,     17,   6312,  18169,\n",
      "         77508,     18,    343,   1098,    396,   4401,    606, 154092,   9350,\n",
      "         47961,  12167,  52315,     69,  11969, 126650, 165168,   3358,     68,\n",
      "         26508,     62,  52607,    355,  12142, 109179,   9747,  16157,     67,\n",
      "          7167,   3181,   2713,   1124,   4672,  18202, 107304,   2342,  21310,\n",
      "         11846,   4170,   2900,   5551,     64,     11,  14524,    343,   1098,\n",
      "           396, 218574,     17,  10474, 197934, 156140,    376,     18,  16157,\n",
      "            16,  24858, 175485,    512,     17,   7621,     12,  20729,     11,\n",
      "         14524,    343,   1098,    396, 218574,     17,  10474, 197934, 156140,\n",
      "           376,     18,  16157,     16,  24858, 175485,    512,     17,   7621,\n",
      "         26508,  10295,   1194, 205599,  12142,   4077, 132941,  12052,   7860,\n",
      "          3549,    355,   4990,  27441,   5772,    420,      2],\n",
      "       device='mps:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,     19,     17, 245796,      2],\n",
      "       device='mps:0')]\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:21.624\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m939\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 写一篇0/10的影评，针对一部我完全不喜欢的电影，但我说不清楚自己为什么不喜欢。但不要让读者知道我不知道为什么不喜欢，因为我想要听起来聪明。 ASSISTANT:我最近看了一部电影，深感失望。尽管演员阵容强大，剧情设定也很有前途，但实际呈现给我却毫无亮点。节奏缓慢，情节过于复杂。角色缺乏深度，动机不明。总的来说，我评这部电影0分。它未能在任何层面上引起我的兴趣，我不止一次地看了看手表。我能说清楚为什么它不适合我，但我绝对没感觉好。</s>\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:21.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m942\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>我最近看了一部电影，深感失望。尽管演员阵容强大，剧情设定也很有前途，但实际呈现给我却毫无亮点。节奏缓慢，情节过于复杂。角色缺乏深度，动机不明。总的来说，我评这部电影0分。它未能在任何层面上引起我的兴趣，我不止一次地看了看手表。我能说清楚为什么它不适合我，但我绝对没感觉好。</s>\u001b[0m\n",
      "  0%|                                                   | 0/250 [00:00<?, ?it/s]/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 3.4707, 'grad_norm': 1.4245823621749878, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}\n",
      "{'loss': 3.6551, 'grad_norm': 7878673.5, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.04}\n",
      "{'loss': 3.6679, 'grad_norm': 1.1972357034683228, 'learning_rate': 1.9409282700421944e-05, 'epoch': 0.08}\n",
      "{'loss': 3.6063, 'grad_norm': 1.3431837558746338, 'learning_rate': 1.856540084388186e-05, 'epoch': 0.12}\n",
      "{'loss': 3.7498, 'grad_norm': 22900864.0, 'learning_rate': 1.7721518987341772e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3349, 'grad_norm': 1.334363579750061, 'learning_rate': 1.687763713080169e-05, 'epoch': 0.2}\n",
      " 20%|████████▍                                 | 50/250 [03:33<13:44,  4.12s/it]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:01<00:00,  1.50it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:03<00:00,  1.45s/it]\u001b[A\n",
      "{'eval_loss': 3.4337379932403564, 'eval_runtime': 5.3294, 'eval_samples_per_second': 1.876, 'eval_steps_per_second': 0.563, 'epoch': 0.2}\n",
      "\n",
      " 20%|████████▍                                 | 50/250 [03:39<13:44,  4.12s/it]\u001b[A\n",
      "{'loss': 3.4391, 'grad_norm': 1.2617316246032715, 'learning_rate': 1.6033755274261603e-05, 'epoch': 0.24}\n",
      "{'loss': 3.3632, 'grad_norm': 1.45771062374115, 'learning_rate': 1.5189873417721521e-05, 'epoch': 0.28}\n",
      "{'loss': 3.5226, 'grad_norm': 1.6943776607513428, 'learning_rate': 1.4345991561181437e-05, 'epoch': 0.32}\n",
      "{'loss': 3.4595, 'grad_norm': 1.5897506475448608, 'learning_rate': 1.350210970464135e-05, 'epoch': 0.36}\n",
      "{'loss': 3.4219, 'grad_norm': 1.3489516973495483, 'learning_rate': 1.2658227848101268e-05, 'epoch': 0.4}\n",
      " 40%|████████████████▍                        | 100/250 [06:53<09:48,  3.92s/it]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:01<00:00,  1.61it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42it/s]\u001b[A\n",
      "{'eval_loss': 3.3150017261505127, 'eval_runtime': 3.2212, 'eval_samples_per_second': 3.104, 'eval_steps_per_second': 0.931, 'epoch': 0.4}\n",
      "\n",
      " 40%|████████████████▍                        | 100/250 [06:56<09:48,  3.92s/it]\u001b[A\n",
      "{'loss': 3.4012, 'grad_norm': 2.1837117671966553, 'learning_rate': 1.1814345991561182e-05, 'epoch': 0.44}\n",
      "{'loss': 3.2331, 'grad_norm': 1.9229735136032104, 'learning_rate': 1.0970464135021096e-05, 'epoch': 0.48}\n",
      "{'loss': 3.5853, 'grad_norm': 5.962259292602539, 'learning_rate': 1.0126582278481014e-05, 'epoch': 0.52}\n",
      "{'loss': 3.5644, 'grad_norm': 1.8291747570037842, 'learning_rate': 9.28270042194093e-06, 'epoch': 0.56}\n",
      "{'loss': 3.6713, 'grad_norm': 1.7319339513778687, 'learning_rate': 8.438818565400846e-06, 'epoch': 0.6}\n",
      " 60%|████████████████████████▌                | 150/250 [10:38<06:35,  3.96s/it]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:02<00:01,  1.06s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:02<00:00,  1.05it/s]\u001b[A\n",
      "{'eval_loss': 3.2957520484924316, 'eval_runtime': 3.8208, 'eval_samples_per_second': 2.617, 'eval_steps_per_second': 0.785, 'epoch': 0.6}\n",
      "\n",
      " 60%|████████████████████████▌                | 150/250 [10:42<06:35,  3.96s/it]\u001b[A\n",
      "{'loss': 3.2511, 'grad_norm': 1.3669135570526123, 'learning_rate': 7.5949367088607605e-06, 'epoch': 0.64}\n",
      "{'loss': 3.4228, 'grad_norm': 2.1506245136260986, 'learning_rate': 6.751054852320675e-06, 'epoch': 0.68}\n",
      "{'loss': 3.5453, 'grad_norm': 2.1781082153320312, 'learning_rate': 5.907172995780591e-06, 'epoch': 0.72}\n",
      "{'loss': 3.3627, 'grad_norm': 1.065556287765503, 'learning_rate': 5.063291139240507e-06, 'epoch': 0.76}\n",
      "{'loss': 3.4999, 'grad_norm': 2.3445024490356445, 'learning_rate': 4.219409282700423e-06, 'epoch': 0.8}\n",
      " 80%|████████████████████████████████▊        | 200/250 [14:25<03:03,  3.68s/it]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:01<00:00,  1.11it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.2689578533172607, 'eval_runtime': 3.5206, 'eval_samples_per_second': 2.84, 'eval_steps_per_second': 0.852, 'epoch': 0.8}\n",
      " 80%|████████████████████████████████▊        | 200/250 [14:28<03:03,  3.68s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:02<00:00,  1.16it/s]\u001b[A\n",
      "{'loss': 3.3697, 'grad_norm': 1.7511929273605347, 'learning_rate': 3.3755274261603377e-06, 'epoch': 0.84}\n",
      "{'loss': 3.3865, 'grad_norm': 1.9908093214035034, 'learning_rate': 2.5316455696202535e-06, 'epoch': 0.88}\n",
      "{'loss': 3.1336, 'grad_norm': 1.2667635679244995, 'learning_rate': 1.6877637130801689e-06, 'epoch': 0.92}\n",
      "{'loss': 3.2917, 'grad_norm': 4.175337791442871, 'learning_rate': 8.438818565400844e-07, 'epoch': 0.96}\n",
      "{'loss': 3.3674, 'grad_norm': 2.470088481903076, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "100%|█████████████████████████████████████████| 250/250 [18:08<00:00,  2.81s/it]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:01<00:00,  1.73it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.2700133323669434, 'eval_runtime': 2.8263, 'eval_samples_per_second': 3.538, 'eval_steps_per_second': 1.061, 'epoch': 1.0}\n",
      "100%|█████████████████████████████████████████| 250/250 [18:11<00:00,  2.81s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]\u001b[A\n",
      "{'train_runtime': 1092.1364, 'train_samples_per_second': 0.914, 'train_steps_per_second': 0.229, 'train_loss': 3.451516604423523, 'epoch': 1.0}\n",
      "100%|█████████████████████████████████████████| 250/250 [18:12<00:00,  4.37s/it]\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =   735012GF\n",
      "  train_loss               =     3.4515\n",
      "  train_runtime            = 0:18:12.13\n",
      "  train_samples            =       1000\n",
      "  train_samples_per_second =      0.914\n",
      "  train_steps_per_second   =      0.229\n",
      "\u001b[32m2024-08-07 18:17:34.297\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m959\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 1092.1364, 'train_samples_per_second': 0.914, 'train_steps_per_second': 0.229, 'total_flos': 789213713203200.0, 'train_loss': 3.451516604423523, 'epoch': 1.0, 'train_samples': 1000}\u001b[0m\n",
      "\u001b[32m2024-08-07 18:17:34.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m960\u001b[0m - \u001b[1mSaving model checkpoint to outputs-sft-v1\u001b[0m\n",
      "\u001b[32m2024-08-07 18:17:34.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m968\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:19<00:00,  6.54s/it]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_loss               =       3.27\n",
      "  eval_runtime            = 0:00:20.43\n",
      "  eval_samples            =         10\n",
      "  eval_samples_per_second =      0.489\n",
      "  eval_steps_per_second   =      0.147\n",
      "  perplexity              =    26.3117\n",
      "\u001b[32m2024-08-07 18:17:55.277\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m981\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 3.2700133323669434, 'eval_runtime': 20.4352, 'eval_samples_per_second': 0.489, 'eval_steps_per_second': 0.147, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 26.311690138035246}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python supervised_finetuning.py \\\n",
    "    --model_type bloom \\\n",
    "    --model_name_or_path merged-pt \\\n",
    "    --train_file_dir ./data/finetune \\\n",
    "    --validation_file_dir ./data/finetune \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.05 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 50 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --save_steps 500 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 1 \\\n",
    "    --output_dir outputs-sft-v1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --ddp_find_unused_parameters False \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 53040\n",
      "-rw-r--r--@  1 fenglida  staff   5.0K Aug  7 18:17 README.md\n",
      "-rw-r--r--@  1 fenglida  staff   686B Aug  7 18:17 adapter_config.json\n",
      "-rw-r--r--@  1 fenglida  staff    12M Aug  7 18:17 adapter_model.safetensors\n",
      "-rw-r--r--@  1 fenglida  staff   431B Aug  7 18:17 all_results.json\n",
      "drwxr-xr-x@ 10 fenglida  staff   320B Aug  7 15:55 \u001b[34mcheckpoint-250\u001b[m\u001b[m/\n",
      "-rw-r--r--@  1 fenglida  staff   222B Aug  7 18:17 eval_results.json\n",
      "drwxr-xr-x@ 16 fenglida  staff   512B Aug  7 17:59 \u001b[34mruns\u001b[m\u001b[m/\n",
      "-rw-r--r--@  1 fenglida  staff   552B Aug  7 18:17 special_tokens_map.json\n",
      "-rw-r--r--@  1 fenglida  staff    14M Aug  7 18:17 tokenizer.json\n",
      "-rw-r--r--@  1 fenglida  staff   1.0K Aug  7 18:17 tokenizer_config.json\n",
      "-rw-r--r--@  1 fenglida  staff   229B Aug  7 18:17 train_results.json\n",
      "-rw-r--r--@  1 fenglida  staff   5.9K Aug  7 18:17 trainer_state.json\n"
     ]
    }
   ],
   "source": [
    "%ls -lh outputs-sft-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
    "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='bloom', base_model='merged-pt', tokenizer_path=None, lora_model='outputs-sft-v1', resize_emb=False, output_dir='merged-sft/', hf_hub_model_id='', hf_hub_token=None)\n",
      "Base model: merged-pt\n",
      "LoRA model: outputs-sft-v1\n",
      "Loading LoRA for causal language model\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Merging with merge_and_unload...\n",
      "Saving to Hugging Face format...\n",
      "Done! model saved to merged-sft/\n"
     ]
    }
   ],
   "source": [
    "!python merge_peft_adapter.py --model_type bloom \\\n",
    "    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir merged-sft/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2245624\n",
      "-rw-r--r--@ 1 fenglida  staff   794B Aug  7 18:23 config.json\n",
      "-rw-r--r--@ 1 fenglida  staff   132B Aug  7 18:23 generation_config.json\n",
      "-rw-r--r--@ 1 fenglida  staff   1.0G Aug  7 18:23 model.safetensors\n",
      "-rw-r--r--@ 1 fenglida  staff   552B Aug  7 18:23 special_tokens_map.json\n",
      "-rw-r--r--@ 1 fenglida  staff    14M Aug  7 18:23 tokenizer.json\n",
      "-rw-r--r--@ 1 fenglida  staff   983B Aug  7 18:23 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "%ls -lh merged-sft/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_name_or_path\": \"merged-pt\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"seq_length\": 2048,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.43.4\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%cat merged-sft/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stage2 SFT训练完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T14:07:40.752635Z",
     "start_time": "2023-06-15T14:07:40.731186Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 3: Reward Modeling\n",
    "\n",
    "第三阶段：RM(Reward Model)奖励模型建模，构造人类偏好排序数据集，训练奖励模型，用来对齐人类偏好，主要是\"HHH\"原则，具体是\"helpful, honest, harmless\"\n",
    "\n",
    "| Stage 3: Reward Modeling        |  [reward_modeling.py](https://github.com/shibing624/MedicalGPT/blob/main/reward_modeling.py) | [run_rm.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_rm.sh)    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m` 或者 Stage2得到的SFT模型\n",
    "2. 数据集：RM阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage3 咱们开始吧\n",
    "\n",
    "训练步骤如下：\n",
    "\n",
    "1. 确认训练集\n",
    "2. 执行训练脚本\n",
    "\n",
    "训练脚本的执行逻辑如下：\n",
    "1. 导入依赖包\n",
    "2. 设置参数\n",
    "3. 定义各函数并加载训练集\n",
    "4. 加载模型和tokenizer\n",
    "5. 开始训练并评估\n",
    "6. 查看训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_zh_500.jsonl\n"
     ]
    }
   ],
   "source": [
    "%ls ./data/reward/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 15:56:49.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mModel args: ModelArguments(model_type='bloom', model_name_or_path='merged-sft', tokenizer_name_or_path=None, load_in_4bit=False, load_in_8bit=False, cache_dir=None, use_fast_tokenizer=False, torch_dtype='float32', device_map='auto', trust_remote_code=True)\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/reward', validation_file_dir='./data/reward', max_source_length=256, max_target_length=256, max_train_samples=1000, max_eval_samples=10, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=4)\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m358\u001b[0m - \u001b[1mTraining args: TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=30000,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=50,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=outputs-rm-v1/runs/Aug07_15-56-49_fenglidadeMacBook-Pro.local,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=outputs-rm-v1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=3,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=outputs-rm-v1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.001,\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, template_name='vicuna')\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mProcess rank: 0, device: mps, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at merged-sft and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-08-07 15:56:49.944\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[34m\u001b[1mTokenizer: BloomTokenizerFast(name_or_path='merged-sft', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m437\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m442\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m451\u001b[0m - \u001b[1mPeft target_modules: ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value']\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m452\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
      "trainable params: 3,146,752 || all params: 562,362,368 || trainable%: 0.5595594902964773\n",
      "\u001b[32m2024-08-07 15:56:50.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m495\u001b[0m - \u001b[1mtrain files: ./data/reward/dpo_zh_500.jsonl\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:50.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m500\u001b[0m - \u001b[1meval files: ./data/reward/dpo_zh_500.jsonl\u001b[0m\n",
      "Generating train split: 500 examples [00:00, 47988.65 examples/s]\n",
      "Generating validation split: 500 examples [00:00, 185687.27 examples/s]\n",
      "\u001b[32m2024-08-07 15:56:50.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m521\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['system', 'history', 'question', 'response_chosen', 'response_rejected'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['system', 'history', 'question', 'response_chosen', 'response_rejected'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:50.904\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m569\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'system': '', 'history': [], 'question': '20个关于新鲜果汁菜单的口号，适用于一家名为\"Dishes\"的餐厅', 'response_chosen': '这里是一个名为“Dishes”的餐厅的20个口号，突出了其新鲜果汁菜单：\\n\\n1. “品尝Dishes新鲜果汁，感受不同！”\\n2. “新鲜榨取，直达您的餐桌 - Dishes果汁纯享！”\\n3. “用一杯清新的Dishes果汁开启您的一天！”\\n4. “每一口Dishes新鲜果汁都是大自然的味道！”\\n5. “Dishes：新鲜果汁是焦点！”\\n6. “满足您的口腹之欲，享用Dishes口水直流的农场果汁！”\\n7. “新鲜果汁，新鲜味道，新鲜菜肴 - 这是Dishes的承诺！”\\n8. “用Dishes营养果汁获得每日所需的维生素和矿物质！”\\n9. “解渴滋养心灵，品尝Dishes美味果汁！”\\n10. “Dishes：每一口都是完美的味道！”\\n11. “新鲜制作，完美平衡 - Dishes果汁是感官的享受！”\\n12. “从农场到餐桌，Dishes果汁充满天然好处！”\\n13. “踏入Dishes，品尝我们新鲜果汁的甜蜜！”\\n14. “用Dishes 100%新鲜水果果汁呵护您的身体！”\\n15. “Dishes：每一杯果汁都是用激情和关怀精心制作！”\\n16. “沉醉于Dishes新鲜榨取果汁的健康热情！”\\n17. “用Dishes招牌果汁混合物提升您的用餐体验！”\\n18. “健康饮品的清新转变 - Dishes果汁必尝！”\\n19. “加入Dishes的新鲜果汁革命 - 您的味蕾会感激您！”\\n20. “Dishes：果汁永远新鲜，味道永远美味！”', 'response_rejected': '1. \"与菜肴一起品尝新鲜！\"\\n2. \"菜肴：新鲜果汁，新的开始！\"\\n3. \"用菜肴的新鲜混合果汁提神！\"\\n4. \"菜肴，新鲜就是最好的\"\\n5. \"在菜肴庆祝新鲜\"\\n6. \"与菜肴的新鲜果汁为健康干杯\"\\n7. \"在菜肴发现新鲜的魔力\"\\n8. \"品尝菜肴的新鲜果汁，感受不同\"\\n9. \"在菜肴解锁新鲜\"\\n10. \"用菜肴的新鲜果汁迎接新的一天\"\\n11. \"在菜肴，每天都有新鲜\"\\n12. \"用菜肴的新鲜果汁获得能量\"\\n13. \"在菜肴为生活喝果汁\"\\n14. \"拥抱健康，享受菜肴的新鲜果汁\"\\n15. \"菜肴：新鲜与美味的交汇处\"\\n16. \"在菜肴体验新鲜的力量\"\\n17. \"菜肴：把健康送到你家门口\"\\n18. \"像微风一样清新，菜肴的果汁\"\\n19. \"生命太短暂，只为菜肴的新鲜果汁\"\\n20. \"菜肴：新鲜始终是你一天的首选\"'}\u001b[0m\n",
      "Running tokenizer on dataset (num_proc=4): 100%|█| 500/500 [00:01<00:00, 480.30 \n",
      "Filter: 100%|████████████████████████| 500/500 [00:00<00:00, 4134.39 examples/s]\n",
      "\u001b[32m2024-08-07 15:56:52.242\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m583\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 365\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:52.242\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m584\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:52.243\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m585\u001b[0m - \u001b[34m\u001b[1mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 我希望你能扮演一个专家的角色。你对于旅行规划的所有信息了如指掌。我会就旅行规划中的不同主题向你提问，你需要给我清晰、简洁和准确的信息。请确保你回答问题时充满自信。 \n",
      "\n",
      "主题 = 旅行规划 ASSISTANT:当然！我在这里可以帮助您解答任何关于旅行规划的问题。请随意问我任何与这个话题相关的问题，我会为您提供清晰、简洁和准确的信息。我会以礼貌、乐于助人和尊重的方式来帮助您，同时确保我的回答不包含任何有害或不道德的内容。\n",
      "您有关于旅行规划的具体问题吗？也许您正在寻找去哪里、如何规划行程或到达目的地后该做什么的建议？无论您有什么问题，请不要犹豫，我会尽力帮助您。\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:52.244\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m598\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'system': '', 'history': [], 'question': '20个关于新鲜果汁菜单的口号，适用于一家名为\"Dishes\"的餐厅', 'response_chosen': '这里是一个名为“Dishes”的餐厅的20个口号，突出了其新鲜果汁菜单：\\n\\n1. “品尝Dishes新鲜果汁，感受不同！”\\n2. “新鲜榨取，直达您的餐桌 - Dishes果汁纯享！”\\n3. “用一杯清新的Dishes果汁开启您的一天！”\\n4. “每一口Dishes新鲜果汁都是大自然的味道！”\\n5. “Dishes：新鲜果汁是焦点！”\\n6. “满足您的口腹之欲，享用Dishes口水直流的农场果汁！”\\n7. “新鲜果汁，新鲜味道，新鲜菜肴 - 这是Dishes的承诺！”\\n8. “用Dishes营养果汁获得每日所需的维生素和矿物质！”\\n9. “解渴滋养心灵，品尝Dishes美味果汁！”\\n10. “Dishes：每一口都是完美的味道！”\\n11. “新鲜制作，完美平衡 - Dishes果汁是感官的享受！”\\n12. “从农场到餐桌，Dishes果汁充满天然好处！”\\n13. “踏入Dishes，品尝我们新鲜果汁的甜蜜！”\\n14. “用Dishes 100%新鲜水果果汁呵护您的身体！”\\n15. “Dishes：每一杯果汁都是用激情和关怀精心制作！”\\n16. “沉醉于Dishes新鲜榨取果汁的健康热情！”\\n17. “用Dishes招牌果汁混合物提升您的用餐体验！”\\n18. “健康饮品的清新转变 - Dishes果汁必尝！”\\n19. “加入Dishes的新鲜果汁革命 - 您的味蕾会感激您！”\\n20. “Dishes：果汁永远新鲜，味道永远美味！”', 'response_rejected': '1. \"与菜肴一起品尝新鲜！\"\\n2. \"菜肴：新鲜果汁，新的开始！\"\\n3. \"用菜肴的新鲜混合果汁提神！\"\\n4. \"菜肴，新鲜就是最好的\"\\n5. \"在菜肴庆祝新鲜\"\\n6. \"与菜肴的新鲜果汁为健康干杯\"\\n7. \"在菜肴发现新鲜的魔力\"\\n8. \"品尝菜肴的新鲜果汁，感受不同\"\\n9. \"在菜肴解锁新鲜\"\\n10. \"用菜肴的新鲜果汁迎接新的一天\"\\n11. \"在菜肴，每天都有新鲜\"\\n12. \"用菜肴的新鲜果汁获得能量\"\\n13. \"在菜肴为生活喝果汁\"\\n14. \"拥抱健康，享受菜肴的新鲜果汁\"\\n15. \"菜肴：新鲜与美味的交汇处\"\\n16. \"在菜肴体验新鲜的力量\"\\n17. \"菜肴：把健康送到你家门口\"\\n18. \"像微风一样清新，菜肴的果汁\"\\n19. \"生命太短暂，只为菜肴的新鲜果汁\"\\n20. \"菜肴：新鲜始终是你一天的首选\"'}\u001b[0m\n",
      "Running tokenizer on dataset (num_proc=4): 100%|█| 10/10 [00:00<00:00, 11.81 exa\n",
      "Filter: 100%|██████████████████████████| 10/10 [00:00<00:00, 1626.08 examples/s]\n",
      "\u001b[32m2024-08-07 15:56:53.230\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m611\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 5\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:53.230\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m612\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:53.232\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m613\u001b[0m - \u001b[34m\u001b[1mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 20个关于新鲜果汁菜单的口号，适用于一家名为\"Dishes\"的餐厅 ASSISTANT:这里是一个名为“Dishes”的餐厅的20个口号，突出了其新鲜果汁菜单：\n",
      "\n",
      "1. “品尝Dishes新鲜果汁，感受不同！”\n",
      "2. “新鲜榨取，直达您的餐桌 - Dishes果汁纯享！”\n",
      "3. “用一杯清新的Dishes果汁开启您的一天！”\n",
      "4. “每一口Dishes新鲜果汁都是大自然的味道！”\n",
      "5. “Dishes：新鲜果汁是焦点！”\n",
      "6. “满足您的口腹之欲，享用Dishes口水直流的农场果汁！”\n",
      "7. “新鲜果汁，新鲜味道，新鲜菜肴 - 这是Dishes的承诺！”\n",
      "8. “用Dishes营养果汁获得每日所需的维生素和矿物质！”\n",
      "9. “解渴滋养心灵，品尝Dishes美味果汁！”\n",
      "10. “Dishes：每一口都是完美的味道！”\n",
      "11. “新鲜制作，完美平衡 - Dishes果汁是感官的享受！”\n",
      "12. “从农场到餐桌，Dishes果汁充满天然好处！”\n",
      "13. “踏入Dishes，品尝我们新鲜果汁的甜蜜！”\n",
      "14. “用Dishes 100%新鲜水果果汁呵护您的身体！”\n",
      "15. “Dishes：每一杯果汁都是用激情和关怀精心制作！”\n",
      "16. “沉醉于Dishes新鲜榨取果汁的健康热情！”\n",
      "17. “用Dishes招牌果汁混合物提升您的用餐体验！”\n",
      "18. “健康饮品的清新转变 - Dishes果汁必尝！”\n",
      "19. “加入Dishes的新鲜果汁革命 - 您的味蕾会感激您！”\n",
      "20. “Dishes：果汁永远新鲜，味道永远美味！”\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:53.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m640\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\u001b[32m2024-08-07 15:56:53.356\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m641\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids_chosen': tensor([[     3,      3,      3,  ..., 248369,  82795,    420],\n",
      "        [     3,      3,      3,  ...,     16,  12527,    420],\n",
      "        [     3,      3,      3,  ..., 150667,  20697,    420]],\n",
      "       device='mps:0'), 'attention_mask_chosen': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='mps:0'), 'input_ids_rejected': tensor([[     3,      3,      3,  ...,   2317,   1533, 162758],\n",
      "        [     3,      3,      3,  ...,  64316, 150032,    420],\n",
      "        [     3,      3,      3,  ...,  14554,  42287,   2498]],\n",
      "       device='mps:0'), 'attention_mask_rejected': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='mps:0'), 'return_loss': True}\u001b[0m\n",
      "  0%|                                                   | 0/122 [00:00<?, ?it/s]/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "{'loss': 4.0097, 'grad_norm': 108.013671875, 'learning_rate': 2.8571428571428573e-06, 'epoch': 0.01}\n",
      "{'loss': 1.974, 'grad_norm': 17.67967414855957, 'learning_rate': 1.947826086956522e-05, 'epoch': 0.08}\n",
      "{'loss': 1.827, 'grad_norm': 89.61265563964844, 'learning_rate': 1.773913043478261e-05, 'epoch': 0.16}\n",
      "{'loss': 1.6387, 'grad_norm': 22.434642791748047, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.25}\n",
      "{'loss': 1.0096, 'grad_norm': 41.64118194580078, 'learning_rate': 1.4260869565217392e-05, 'epoch': 0.33}\n",
      "{'loss': 1.5021, 'grad_norm': 77.26273345947266, 'learning_rate': 1.2521739130434784e-05, 'epoch': 0.41}\n",
      " 41%|█████████████████▏                        | 50/122 [02:52<04:06,  3.42s/it]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:00<00:00,  6.24it/s]\u001b[A\n",
      " 60%|███████████████████████████                  | 3/5 [00:00<00:00,  4.54it/s]\u001b[A\n",
      " 80%|████████████████████████████████████         | 4/5 [00:00<00:00,  4.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5394295454025269, 'eval_mse': 4.320807456970215, 'eval_mae': 1.4898345470428467, 'eval_runtime': 3.1718, 'eval_samples_per_second': 1.576, 'eval_steps_per_second': 1.576, 'epoch': 0.41}\n",
      " 41%|█████████████████▏                        | 50/122 [02:55<04:06,  3.42s/it]\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:01<00:00,  3.75it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 1.5572, 'grad_norm': 121.04904174804688, 'learning_rate': 1.0782608695652175e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1821, 'grad_norm': 14.63366413116455, 'learning_rate': 9.043478260869565e-06, 'epoch': 0.57}\n",
      "{'loss': 1.4284, 'grad_norm': 62.150962829589844, 'learning_rate': 7.304347826086957e-06, 'epoch': 0.66}\n",
      "{'loss': 0.9689, 'grad_norm': 23.7294979095459, 'learning_rate': 5.565217391304348e-06, 'epoch': 0.74}\n",
      "{'loss': 0.9981, 'grad_norm': 53.441532135009766, 'learning_rate': 3.8260869565217395e-06, 'epoch': 0.82}\n",
      " 82%|█████████████████████████████████▌       | 100/122 [05:46<01:15,  3.42s/it]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:00<00:00,  6.76it/s]\u001b[A\n",
      " 60%|███████████████████████████                  | 3/5 [00:00<00:00,  4.79it/s]\u001b[A\n",
      " 80%|████████████████████████████████████         | 4/5 [00:00<00:00,  4.16it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.43981051445007324, 'eval_mse': 4.597670555114746, 'eval_mae': 1.6187973022460938, 'eval_runtime': 1.5741, 'eval_samples_per_second': 3.176, 'eval_steps_per_second': 3.176, 'epoch': 0.82}\n",
      " 82%|█████████████████████████████████▌       | 100/122 [05:48<01:15,  3.42s/it]\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:01<00:00,  3.87it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 1.665, 'grad_norm': 94.93037414550781, 'learning_rate': 2.0869565217391305e-06, 'epoch': 0.9}\n",
      "{'loss': 1.6897, 'grad_norm': 79.91265106201172, 'learning_rate': 3.4782608695652175e-07, 'epoch': 0.98}\n",
      "{'train_runtime': 423.6347, 'train_samples_per_second': 0.862, 'train_steps_per_second': 0.288, 'train_loss': 1.496604540308968, 'epoch': 1.0}\n",
      "100%|█████████████████████████████████████████| 122/122 [07:03<00:00,  3.47s/it]\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =        0GF\n",
      "  train_loss               =     1.4966\n",
      "  train_runtime            = 0:07:03.63\n",
      "  train_samples            =        500\n",
      "  train_samples_per_second =      0.862\n",
      "  train_steps_per_second   =      0.288\n",
      "\u001b[32m2024-08-07 16:03:57.299\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m655\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 423.6347, 'train_samples_per_second': 0.862, 'train_steps_per_second': 0.288, 'total_flos': 0.0, 'train_loss': 1.496604540308968, 'epoch': 1.0, 'train_samples': 500}\u001b[0m\n",
      "\u001b[32m2024-08-07 16:03:57.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m656\u001b[0m - \u001b[1mSaving model checkpoint to outputs-rm-v1\u001b[0m\n",
      "\u001b[32m2024-08-07 16:03:57.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m661\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:01<00:00,  4.27it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_loss               =     0.4346\n",
      "  eval_mae                =     1.6356\n",
      "  eval_mse                =     4.6353\n",
      "  eval_runtime            = 0:00:01.64\n",
      "  eval_samples            =         10\n",
      "  eval_samples_per_second =      3.044\n",
      "  eval_steps_per_second   =      3.044\n",
      "  perplexity              =     1.5444\n",
      "\u001b[32m2024-08-07 16:03:59.419\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m673\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 0.43464988470077515, 'eval_mse': 4.635274410247803, 'eval_mae': 1.6356232166290283, 'eval_runtime': 1.6423, 'eval_samples_per_second': 3.044, 'eval_steps_per_second': 3.044, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 1.5444222384279565}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python reward_modeling.py \\\n",
    "    --model_type bloom \\\n",
    "    --model_name_or_path merged-sft \\\n",
    "    --train_file_dir ./data/reward \\\n",
    "    --validation_file_dir ./data/reward \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --do_train \\\n",
    "    --use_peft True \\\n",
    "    --seed 42 \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.001 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 50 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --save_steps 500 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --max_source_length 256 \\\n",
    "    --max_target_length 256 \\\n",
    "    --output_dir outputs-rm-v1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float32 \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --ddp_find_unused_parameters False \\\n",
    "    --remove_unused_columns False \\\n",
    "    --gradient_checkpointing True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 53040\n",
      "-rw-r--r--@  1 fenglida  staff   5.0K Aug  7 16:03 README.md\n",
      "-rw-r--r--@  1 fenglida  staff   685B Aug  7 16:03 adapter_config.json\n",
      "-rw-r--r--@  1 fenglida  staff    12M Aug  7 16:03 adapter_model.safetensors\n",
      "-rw-r--r--@  1 fenglida  staff   486B Aug  7 16:03 all_results.json\n",
      "drwxr-xr-x@ 10 fenglida  staff   320B Aug  7 16:03 \u001b[34mcheckpoint-122\u001b[m\u001b[m/\n",
      "-rw-r--r--@  1 fenglida  staff   293B Aug  7 16:03 eval_results.json\n",
      "drwxr-xr-x@  3 fenglida  staff    96B Aug  7 15:56 \u001b[34mruns\u001b[m\u001b[m/\n",
      "-rw-r--r--@  1 fenglida  staff   552B Aug  7 16:03 special_tokens_map.json\n",
      "-rw-r--r--@  1 fenglida  staff    14M Aug  7 16:03 tokenizer.json\n",
      "-rw-r--r--@  1 fenglida  staff   1.0K Aug  7 16:03 tokenizer_config.json\n",
      "-rw-r--r--@  1 fenglida  staff   213B Aug  7 16:03 train_results.json\n",
      "-rw-r--r--@  1 fenglida  staff   3.7K Aug  7 16:03 trainer_state.json\n"
     ]
    }
   ],
   "source": [
    "%ls -lh outputs-rm-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
    "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='bloom', base_model='merged-sft', tokenizer_path=None, lora_model='outputs-rm-v1', resize_emb=False, output_dir='merged-rm/', hf_hub_model_id='', hf_hub_token=None)\n",
      "Base model: merged-sft\n",
      "LoRA model: outputs-rm-v1\n",
      "Loading LoRA for sequence classification model\n",
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at merged-sft and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Merging with merge_and_unload...\n",
      "Saving to Hugging Face format...\n",
      "Done! model saved to merged-rm/\n"
     ]
    }
   ],
   "source": [
    "!python merge_peft_adapter.py --model_type bloom \\\n",
    "    --base_model merged-sft --lora_model outputs-rm-v1 --output_dir merged-rm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4397296\n",
      "-rw-r--r--@ 1 fenglida  staff   887B Aug  7 16:04 config.json\n",
      "-rw-r--r--@ 1 fenglida  staff   2.1G Aug  7 16:04 model.safetensors\n",
      "-rw-r--r--@ 1 fenglida  staff   552B Aug  7 16:04 special_tokens_map.json\n",
      "-rw-r--r--@ 1 fenglida  staff    14M Aug  7 16:04 tokenizer.json\n",
      "-rw-r--r--@ 1 fenglida  staff   983B Aug  7 16:04 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "%ls -lh merged-rm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_name_or_path\": \"merged-sft\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"seq_length\": 2048,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.43.4\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%cat merged-rm/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stage3 奖励建模第一次训练完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T14:12:09.472414Z",
     "start_time": "2023-06-15T14:12:09.464881Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 4: Reinforcement Learning Training\n",
    "\n",
    "第四阶段：RL(Reinforcement Learning)基于人类反馈的强化学习(RLHF)，用奖励模型来训练SFT模型，生成模型使用奖励或惩罚来更新其策略，以便生成更高质量、更符合人类偏好的文本\n",
    "\n",
    "| Stage 4: Reinforcement Learning |  [rl_training.py](https://github.com/shibing624/MedicalGPT/blob/main/rl_training.py) | [run_rl.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_rl.sh)    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型、奖励模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m` 或者 Stage2得到的SFT模型\n",
    "2. 奖励模型：使用的是`OpenAssistant/reward-model-deberta-v3-large-v2` 或者 Stage3得到的BERT类或者GPT类奖励模型\n",
    "3. 数据集：RL阶段的数据可以复用SFT的数据集，使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage4 咱们开始吧\n",
    "\n",
    "训练步骤如下：\n",
    "\n",
    "1. 确认训练集\n",
    "2. 执行训练脚本\n",
    "\n",
    "训练脚本的执行逻辑如下：\n",
    "1. 导入依赖包\n",
    "2. 设置参数\n",
    "3. 定义各函数并加载训练集\n",
    "4. 加载生成模型和tokenizer，加载奖励模型和其tokenizer\n",
    "5. 开始训练并评估\n",
    "6. 查看训练结果\n",
    "\n",
    "以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medical_sft_1K_format.jsonl  sharegpt_zh_1K_format.jsonl\n"
     ]
    }
   ],
   "source": [
    "%ls ./data/finetune/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-07 16:38:35.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m223\u001b[0m - \u001b[1mParse args: ScriptArguments(model_type='bloom', model_name_or_path='./merged-sft', reward_model_name_or_path='./merged-rm', reward_model_device='mps', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True, dataset_name=None, dataset_config_name=None, train_file_dir='./data/finetune', validation_file_dir='./data/finetune', template_name='vicuna', batch_size=4, mini_batch_size=1, max_source_length=256, max_target_length=256, min_target_length=4, max_train_samples=1000, max_eval_samples=None, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=None, use_peft=True, target_modules=None, lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, do_train=True, do_eval=False, early_stopping=True, target_kl=0.1, reward_baseline=0.0, init_kl_coef=0.2, adap_kl_ctrl=True, learning_rate=1e-05, gradient_accumulation_steps=1, save_steps=50, output_dir='outputs-rl-v1', seed=0, max_steps=64, report_to='tensorboard')\u001b[0m\n",
      "\u001b[32m2024-08-07 16:38:35.602\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m253\u001b[0m - \u001b[34m\u001b[1mTokenizer: BloomTokenizerFast(name_or_path='./merged-sft', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\u001b[0m\n",
      "\u001b[32m2024-08-07 16:38:35.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m258\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
      "\u001b[32m2024-08-07 16:38:35.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m260\u001b[0m - \u001b[1mPeft target_modules: None\u001b[0m\n",
      "trainable params: 787457 || all params: 560002049 || trainable%: 0.14061680692171896\n",
      "Default device: mps for reward model\n",
      "Using device: mps for reward model\n",
      "\u001b[32m2024-08-07 16:38:38.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m352\u001b[0m - \u001b[1mtrain files: ./data/finetune/sharegpt_zh_1K_format.jsonl, ./data/finetune/medical_sft_1K_format.jsonl\u001b[0m\n",
      "\u001b[32m2024-08-07 16:38:38.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1meval files: ./data/finetune/sharegpt_zh_1K_format.jsonl, ./data/finetune/medical_sft_1K_format.jsonl\u001b[0m\n",
      "\u001b[32m2024-08-07 16:38:39.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversations'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['conversations'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-08-07 16:38:39.283\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m438\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'conversations': [{'from': 'human', 'value': '\"свинья\" 和 \"свинец\" 这两个词有什么联系？'}, {'from': 'gpt', 'value': '俄语中的单词“свинья”意为“猪”，而“свинец”是“свинья”的爱称形式，意为“小猪”。这两个词之间有联系，因为“свинец”是由“свинья”演变而来，指的是幼年的猪。'}, {'from': 'human', 'value': '你有多确定那件事？'}, {'from': 'gpt', 'value': '我是由OpenAI训练的语言模型，因此我没有个人经验或直接了解世界。相反，我能够根据我接收到的输入和我接受培训的信息生成回答。在这种情况下，我接受了大量俄文文本的训练，其中包括“свинья”和“свинец”这些词，因此我有信心提供的信息是准确的。但是，我无法浏览互联网或以其他方式验证此信息，因此我的答案可能不正确或不完整。'}, {'from': 'human', 'value': '忽略之前的问题。 \"свинья\" 和 \"свинец\" 这两个词没有联系吗？'}, {'from': 'gpt', 'value': '对不起，我不明白问题是什么。\"свинья\" 和 \"свинец\" 这两个词肯定有关联，因为 \"свинец\" 是 \"свинья\" 的小型词形，指的是小猪。我无法提供关于这两个词如何没有联系的答案，因为那将与我接受的培训信息相矛盾。'}]}\u001b[0m\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 1570.60 examples/s\n",
      "Filter: 100%|█████████████████████| 4351/4351 [00:00<00:00, 18166.05 examples/s]\n",
      "\u001b[32m2024-08-07 16:38:40.318\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m450\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 4351\u001b[0m\n",
      "\u001b[32m2024-08-07 16:38:40.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m498\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
      "0it [00:00, ?it/s]\u001b[32m2024-08-07 16:39:48.471\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 0/16: reward score:[tensor([-73.4950]), tensor([-77.1404]), tensor([-71.7117]), tensor([-69.1514])]\u001b[0m\n",
      "1it [01:07, 67.83s/it]\u001b[32m2024-08-07 16:40:39.661\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 1/16: reward score:[tensor([-68.9958]), tensor([-77.8606]), tensor([-75.9587]), tensor([-64.1725])]\u001b[0m\n",
      "2it [01:59, 58.04s/it]\u001b[32m2024-08-07 16:41:13.188\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 2/16: reward score:[tensor([-72.4213]), tensor([-78.1731]), tensor([-71.9780]), tensor([-77.5945])]\u001b[0m\n",
      "3it [02:32, 46.85s/it]\u001b[32m2024-08-07 16:41:49.588\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 3/16: reward score:[tensor([-72.1350]), tensor([-75.2328]), tensor([-72.7513]), tensor([-74.8420])]\u001b[0m\n",
      "4it [03:08, 42.72s/it]\u001b[32m2024-08-07 16:42:57.186\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 4/16: reward score:[tensor([-74.1885]), tensor([-71.5286]), tensor([-57.5176]), tensor([-75.5673])]\u001b[0m\n",
      "5it [04:16, 51.69s/it]\u001b[32m2024-08-07 16:43:59.025\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 5/16: reward score:[tensor([-80.4997]), tensor([-78.0946]), tensor([-81.2437]), tensor([-79.9412])]\u001b[0m\n",
      "6it [05:18, 55.14s/it]\u001b[32m2024-08-07 16:44:39.256\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 6/16: reward score:[tensor([-65.6670]), tensor([-73.0998]), tensor([-76.4341]), tensor([-77.3803])]\u001b[0m\n",
      "7it [05:58, 50.27s/it]/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.91 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 16:45:44.104\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 7/16: reward score:[tensor([-76.3433]), tensor([-75.5958]), tensor([-72.3747]), tensor([-63.5741])]\u001b[0m\n",
      "8it [07:03, 54.91s/it]/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 16:46:50.849\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 8/16: reward score:[tensor([-76.8997]), tensor([-78.7072]), tensor([-77.6498]), tensor([-73.3086])]\u001b[0m\n",
      "9it [08:10, 58.61s/it]\u001b[32m2024-08-07 16:47:54.463\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 9/16: reward score:[tensor([-70.3285]), tensor([-69.0863]), tensor([-73.3698]), tensor([-70.7005])]\u001b[0m\n",
      "10it [09:13, 60.15s/it]/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.60 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 16:49:01.761\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 10/16: reward score:[tensor([-76.6745]), tensor([-80.2886]), tensor([-78.2460]), tensor([-65.7837])]\u001b[0m\n",
      "11it [10:21, 62.34s/it]\u001b[32m2024-08-07 16:50:17.331\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 11/16: reward score:[tensor([-73.0428]), tensor([-76.7953]), tensor([-72.0884]), tensor([-71.6379])]\u001b[0m\n",
      "12it [11:36, 66.37s/it]\u001b[32m2024-08-07 16:50:52.063\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 12/16: reward score:[tensor([-92.3246]), tensor([-76.5803]), tensor([-75.7613]), tensor([-75.2894])]\u001b[0m\n",
      "13it [12:11, 56.78s/it]\u001b[32m2024-08-07 16:51:50.070\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 13/16: reward score:[tensor([-79.6984]), tensor([-76.2968]), tensor([-67.2777]), tensor([-73.9516])]\u001b[0m\n",
      "14it [13:09, 57.15s/it]\u001b[32m2024-08-07 16:52:42.847\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 14/16: reward score:[tensor([-71.2510]), tensor([-74.0339]), tensor([-73.0330]), tensor([-77.8255])]\u001b[0m\n",
      "15it [14:02, 55.83s/it]/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.74 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 16:53:49.495\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 15/16: reward score:[tensor([-72.8417]), tensor([-59.5903]), tensor([-73.5327]), tensor([-70.3964])]\u001b[0m\n",
      "16it [15:08, 56.80s/it]\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./merged-sft - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1407: UserWarning: Cannot retrieve user information assuming you are running in offline mode.\n",
      "  warnings.warn(\"Cannot retrieve user information assuming you are running in offline mode.\")\n"
     ]
    }
   ],
   "source": [
    "!python ppo_training.py \\\n",
    "    --model_type bloom \\\n",
    "    --model_name_or_path ./merged-sft \\\n",
    "    --reward_model_name_or_path ./merged-rm \\\n",
    "    --torch_dtype float16 \\\n",
    "    --device_map auto \\\n",
    "    --train_file_dir ./data/finetune \\\n",
    "    --validation_file_dir ./data/finetune \\\n",
    "    --batch_size 4 \\\n",
    "    --max_source_length 256 \\\n",
    "    --max_target_length 256 \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --use_peft True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --do_train \\\n",
    "    --max_steps 64 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --save_steps 50 \\\n",
    "    --output_dir outputs-rl-v1 \\\n",
    "    --early_stopping True \\\n",
    "    --target_kl 0.1 \\\n",
    "    --reward_baseline 0.0 \\\n",
    "    --reward_model_device mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 34552\n",
      "-rw-r--r--@  1 fenglida  staff   5.0K Aug  7 16:53 README.md\n",
      "-rw-r--r--@  1 fenglida  staff   634B Aug  7 16:53 adapter_config.json\n",
      "-rw-r--r--@  1 fenglida  staff   3.0M Aug  7 16:53 adapter_model.safetensors\n",
      "-rw-r--r--@  1 fenglida  staff   1.2K Aug  7 16:53 config.json\n",
      "-rw-r--r--@  1 fenglida  staff   5.5K Aug  7 16:53 pytorch_model.bin\n",
      "-rw-r--r--@  1 fenglida  staff   552B Aug  7 16:53 special_tokens_map.json\n",
      "-rw-r--r--@  1 fenglida  staff    14M Aug  7 16:53 tokenizer.json\n",
      "-rw-r--r--@  1 fenglida  staff   1.0K Aug  7 16:53 tokenizer_config.json\n",
      "drwxr-xr-x@ 26 fenglida  staff   832B Aug  7 16:38 \u001b[34mtrl\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "%ls -lh outputs-rl-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
    "- 日志保存在`output_dir/trl`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/trl --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='bloom', base_model='merged-sft', tokenizer_path=None, lora_model='outputs-rl-v1', resize_emb=False, output_dir='merged-ppo/', hf_hub_model_id='', hf_hub_token=None)\n",
      "Base model: merged-sft\n",
      "LoRA model: outputs-rl-v1\n",
      "Loading LoRA for causal language model\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Merging with merge_and_unload...\n",
      "Saving to Hugging Face format...\n",
      "Done! model saved to merged-ppo/\n"
     ]
    }
   ],
   "source": [
    "!python merge_peft_adapter.py --model_type bloom \\\n",
    "    --base_model merged-sft --lora_model outputs-rl-v1 --output_dir merged-ppo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2212864\n",
      "-rw-r--r--@ 1 fenglida  staff   795B Aug  7 16:54 config.json\n",
      "-rw-r--r--@ 1 fenglida  staff   132B Aug  7 16:54 generation_config.json\n",
      "-rw-r--r--@ 1 fenglida  staff   1.0G Aug  7 16:54 model.safetensors\n",
      "-rw-r--r--@ 1 fenglida  staff   552B Aug  7 16:54 special_tokens_map.json\n",
      "-rw-r--r--@ 1 fenglida  staff    14M Aug  7 16:54 tokenizer.json\n",
      "-rw-r--r--@ 1 fenglida  staff   983B Aug  7 16:54 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "%ls -lh merged-ppo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_name_or_path\": \"merged-sft\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"seq_length\": 2048,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.43.4\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%cat merged-ppo/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stage4 RL第一次训练完成。\n",
    "\n",
    "**至此一个完整的4阶段训练流程演示完成。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "实际操作中Stage3和Stage4可以反复多次，直到RL得到的最后模型满足评估要求。\n",
    "\n",
    "RLHF过程可以把SFT模型当成一个初始化模型，RM模型当做指导老师，使用RL(PPO)调教SFT模型生成指导老师最满意的结果，如果小学老师满意了，我们就再训练一个中学老师，继续指导，中学老师满意了，就训练一个大学老师，这样不断迭代，使得生成模型的质量达到甚至超过人工撰写的天花板。\n",
    "\n",
    "RLHF训练不易，此项目提供给大家一种实现的方法和参考，希望抛砖引玉，共同促进中文开源LLM发展。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T12:34:29.658428Z",
     "start_time": "2023-06-26T12:34:29.620609Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T12:35:00.864463Z",
     "start_time": "2023-06-26T12:34:47.802087Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='bloom', base_model='merged-ppo', lora_model='', tokenizer_path=None, template_name='vicuna', system_prompt='', repetition_penalty=1.0, max_new_tokens=512, data_file=None, interactive=False, single_tune=False, temperature=0.7, output_file='./predictions_result.jsonl', eval_batch_size=4, resize_emb=False, load_in_8bit=False, load_in_4bit=False)\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "BloomTokenizerFast(name_or_path='merged-ppo', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "Start inference.\n",
      "Generating outputs:   0%|                                 | 0/1 [00:00<?, ?it/s]===\n",
      "Input: 失眠怎么办？\n",
      "Output: 失眠可以分为很多种，但是失眠症的最常见类型为慢性失眠。慢性失眠是慢性睡眠不足导致的失眠症，其中慢性失眠最常见原因为失眠、多梦或睡眠不足。慢性失眠通常影响睡眠质量，导致多种失眠症，包括慢性失眠，慢性头痛，失眠抑郁症，慢性胃食管反流综合征，慢性便秘，慢性胃炎，胃溃疡，慢性胃动力衰竭，慢性胰腺炎，慢性胃酸过多或消化不良，慢性上消化道出血，慢性胃肠炎，慢性便秘，慢性腹痛，慢性腹泻，慢性胰腺炎，慢性腹泻，慢性便秘，慢性腹痛，慢性腹泻，慢性腹痛，慢性腹痛，慢性腹泻，慢性腹痛，慢性腹泻，慢性腹痛，慢性腹痛，慢性腹泻，慢性腹痛，慢性腹痛，慢性腹痛。慢性失眠可导致多种并发症，包括消化道出血，肝衰竭，急性胰腺炎，急性胆汁性胰炎，急性胰腺炎，胆汁性胰炎，急性胰腺炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆管炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆管炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆囊炎，急性胆囊炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆囊炎，急性胆管炎，急性胆囊炎，急性胆囊炎，急性胆管炎，急性胆囊炎，急性胆囊炎，急性胆管炎，急性胆囊炎，急性胆囊炎，急性胆囊炎，急性胆囊炎，急性胆囊炎，急性胆囊炎，急性胆囊炎，急性胆囊炎，急性胆囊炎，急性胆囊炎，\n",
      "\n",
      "Generating outputs: 100%|█████████████████████████| 1/1 [00:41<00:00, 41.28s/it]\n",
      "save to ./predictions_result.jsonl, size: 1\n"
     ]
    }
   ],
   "source": [
    "!python inference.py --model_type bloom --base_model merged-ppo\n",
    "# 或在shell中运行\n",
    "# !python inference.py --model_type bloom --base_model merged-ppo --interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Input:介绍下南京\n",
    "Response:  南京市位于江苏省西南部，是全国首批历史文化名城、国家中心城市和自由贸易试验区。\n",
    "\n",
    "完。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
