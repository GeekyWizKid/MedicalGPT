{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training Pipeline\n",
    "[run_training_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_pipeline.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stage 1: Continue Pretraining\n",
    "\n",
    "ç¬¬ä¸€é˜¶æ®µï¼šPT(Continue PreTraining)å¢é‡é¢„è®­ç»ƒï¼Œåœ¨æµ·é‡é¢†åŸŸæ–‡æœ¬æ•°æ®ä¸ŠäºŒæ¬¡é¢„è®­ç»ƒGPTæ¨¡å‹ï¼Œä»¥é€‚é…é¢†åŸŸæ•°æ®åˆ†å¸ƒ\n",
    "\n",
    "æ³¨æ„ï¼š\n",
    "1. æ­¤é˜¶æ®µæ˜¯å¯é€‰çš„ï¼Œå¦‚æœä½ æ²¡æœ‰æµ·é‡é¢†åŸŸæ–‡æœ¬ï¼Œå¯ä»¥è·³è¿‡æ­¤é˜¶æ®µï¼Œç›´æ¥è¿›è¡ŒSFTé˜¶æ®µçš„æœ‰ç›‘ç£å¾®è°ƒ\n",
    "2. æˆ‘å®éªŒå‘ç°ï¼šåšé¢†åŸŸçŸ¥è¯†æ³¨å…¥ï¼ŒSFTæ¯”PTæ›´é«˜æ•ˆï¼Œä¹Ÿå¯ä»¥è·³è¿‡PTé˜¶æ®µ\n",
    "\n",
    "| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è¯´æ˜ï¼š\n",
    "ä»¥ä¸‹ notebook/colab ä»£ç ä¸ºäº†å¿«é€ŸéªŒè¯è®­ç»ƒä»£ç å¯ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å°sizeçš„ç”Ÿæˆæ¨¡å‹å’Œå°æ ·æœ¬æ•°æ®é›†ï¼Œå®é™…ä½¿ç”¨æ—¶ï¼Œéœ€è¦ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚\n",
    "\n",
    "1. ç”Ÿæˆæ¨¡å‹ï¼šä½¿ç”¨çš„æ˜¯Bloomçš„`bigscience/bloomz-560m`\n",
    "2. æ•°æ®é›†ï¼šPTé˜¶æ®µä½¿ç”¨çš„æ˜¯ä¸­æ–‡å¤©é¾™å…«éƒ¨å°è¯´éƒ¨åˆ†æ–‡æœ¬å’Œè‹±æ–‡ä¹¦ç±éƒ¨åˆ†æ–‡æœ¬ï¼Œä½äº`data/pretrain`æ–‡ä»¶å¤¹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é…ç½®è¿è¡Œç¯å¢ƒ\n",
    "\n",
    "æœ¬åœ°æ‰§è¡Œå¯æ³¨é‡Šä»¥ä¸‹é…ç½®ç¯å¢ƒçš„å‘½ä»¤ï¼Œcolabæ‰§è¡Œè¦æ‰“å¼€æ³¨é‡Šï¼Œç”¨äºé…ç½®ç¯å¢ƒ\n",
    "\n",
    "colabå»ºè®®ä½¿ç”¨T4 GPUè®­ç»ƒï¼Œè®¾ç½®æ–¹å¼ï¼š`ä»£ç æ‰§è¡Œç¨‹åº -> æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ -> è¿è¡Œæ—¶ç±»å‹ï¼šPython3ï¼Œç¡¬ä»¶åŠ é€Ÿå™¨ï¼šGPUï¼ŒGPUç±»å‹ï¼šT4 -> ä¿å­˜`\n",
    "\n",
    "æ­¥éª¤ï¼š\n",
    "1. ä¸‹è½½æœ€æ–°ä»£ç åˆ°æœ¬åœ°\n",
    "2. å®‰è£…ä¾èµ–åŒ…\n",
    "\n",
    "ä¾èµ–åŒ…å¦‚ä¸‹ï¼Œä¿è¯æœ€æ–°ç‰ˆæœ¬ï¼š\n",
    "\n",
    "```\n",
    "loguru\n",
    "transformers\n",
    "sentencepiece\n",
    "datasets\n",
    "tensorboard\n",
    "tqdm\n",
    "peft\n",
    "trl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITATION.cff                       merge_peft_adapter.py\n",
      "CONTRIBUTING.md                    merge_tokenizers.py\n",
      "DISCLAIMER                         openai_api.py\n",
      "LICENSE                            orpo_training.py\n",
      "README.md                          ppo_training.py\n",
      "README_EN.md                       pretraining.py\n",
      "_config.yml                        requirements.txt\n",
      "build_domain_tokenizer.py          reward_modeling.py\n",
      "chatpdf.py                         \u001b[34mrole_play_data\u001b[m\u001b[m/\n",
      "convert_dataset.py                 run_dpo.sh\n",
      "\u001b[34mdata\u001b[m\u001b[m/                              run_full_sft.sh\n",
      "deepspeed_zero_stage2_config.json  run_orpo.sh\n",
      "deepspeed_zero_stage3_config.json  run_ppo.sh\n",
      "\u001b[34mdocs\u001b[m\u001b[m/                              run_pt.sh\n",
      "dpo_training.py                    run_rm.sh\n",
      "fastapi_server_demo.py             run_sft.sh\n",
      "full_supervised_finetuning.py      run_training_dpo_pipeline.ipynb\n",
      "gradio_demo.py                     run_training_ppo_pipeline.ipynb\n",
      "inference.py                       supervised_finetuning.py\n",
      "inference_multigpu_demo.py         template.py\n",
      "Collecting accelerate~=0.27.2 (from -r requirements.txt (line 1))\n",
      "  Using cached accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting datasets>=2.14.6 (from -r requirements.txt (line 2))\n",
      "  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting loguru (from -r requirements.txt (line 3))\n",
      "  Using cached loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting peft~=0.10.0 (from -r requirements.txt (line 4))\n",
      "  Using cached peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sentencepiece (from -r requirements.txt (line 5))\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting scikit-learn (from -r requirements.txt (line 6))\n",
      "  Downloading scikit_learn-1.5.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (12 kB)\n",
      "Collecting tensorboard (from -r requirements.txt (line 7))\n",
      "  Using cached tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tqdm>=4.47.0 (from -r requirements.txt (line 8))\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting transformers>=4.39.3 (from -r requirements.txt (line 9))\n",
      "  Using cached transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting trl~=0.8.3 (from -r requirements.txt (line 10))\n",
      "  Using cached trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tiktoken (from -r requirements.txt (line 11))\n",
      "  Downloading tiktoken-0.7.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting numpy>=1.17 (from accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Downloading numpy-2.0.1-cp39-cp39-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.9/site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: psutil in ./.conda/lib/python3.9/site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (5.8.0)\n",
      "Collecting pyyaml (from accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting torch>=1.10.0 (from accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Downloading torch-2.4.0-cp39-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting huggingface-hub (from accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Using cached huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.3.1 (from accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Downloading safetensors-0.4.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading pyarrow-17.0.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading pandas-2.2.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.32.2 (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading aiohttp-3.10.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn->-r requirements.txt (line 6))\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 6))\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 6))\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 7))\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 7))\n",
      "  Downloading grpcio-1.65.4-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 7))\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting protobuf!=4.24.0,<5.0.0,>=3.19.6 (from tensorboard->-r requirements.txt (line 7))\n",
      "  Using cached protobuf-4.25.4-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./.conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (72.1.0)\n",
      "Requirement already satisfied: six>1.9 in ./.conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 7))\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard->-r requirements.txt (line 7))\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.39.3->-r requirements.txt (line 9))\n",
      "  Downloading regex-2024.7.24-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19 (from transformers>=4.39.3->-r requirements.txt (line 9))\n",
      "  Downloading tokenizers-0.19.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting tyro>=0.5.11 (from trl~=0.8.3->-r requirements.txt (line 10))\n",
      "  Using cached tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached aiohappyeyeballs-2.3.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached attrs-24.1.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading multidict-6.0.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/lib/python3.9/site-packages (from huggingface-hub->accelerate~=0.27.2->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./.conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 7)) (8.2.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting sympy (from torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10))\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10))\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10))\n",
      "  Using cached shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting eval-type-backport>=0.1.3 (from tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10))\n",
      "  Downloading eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 7))\n",
      "  Downloading MarkupSafe-2.1.5-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.9/site-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.14.6->-r requirements.txt (line 2))\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 7)) (3.19.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.conda/lib/python3.9/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10)) (2.18.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "Using cached datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "Using cached loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "Using cached peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "Downloading sentencepiece-0.2.0-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.1-cp39-cp39-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
      "Using cached trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "Downloading tiktoken-0.7.0-cp39-cp39-macosx_11_0_arm64.whl (907 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m907.9/907.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Downloading aiohttp-3.10.1-cp39-cp39-macosx_11_0_arm64.whl (386 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m386.0/386.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.65.4-cp39-cp39-macosx_10_9_universal2.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Downloading numpy-2.0.1-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached protobuf-4.25.4-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Downloading pyarrow-17.0.0-cp39-cp39-macosx_11_0_arm64.whl (27.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp39-cp39-macosx_11_0_arm64.whl (174 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m174.4/174.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.7.24-cp39-cp39-macosx_11_0_arm64.whl (278 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.9/278.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading safetensors-0.4.4-cp39-cp39-macosx_11_0_arm64.whl (383 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.3/30.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.19.1-cp39-cp39-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.0-cp39-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.1/62.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tyro-0.8.5-py3-none-any.whl (103 kB)\n",
      "Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.3.4-py3-none-any.whl (12 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached attrs-24.1.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Downloading charset_normalizer-3.3.2-cp39-cp39-macosx_11_0_arm64.whl (120 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m120.4/120.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl (53 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp39-cp39-macosx_10_9_universal2.whl (18 kB)\n",
      "Downloading multidict-6.0.5-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Downloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl (81 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: sentencepiece, pytz, mpmath, xxhash, urllib3, tzdata, tqdm, threadpoolctl, tensorboard-data-server, sympy, shtab, safetensors, regex, pyyaml, pyarrow-hotfix, protobuf, numpy, networkx, multidict, mdurl, MarkupSafe, loguru, joblib, idna, grpcio, fsspec, frozenlist, filelock, eval-type-backport, docstring-parser, dill, charset-normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, absl-py, yarl, werkzeug, scipy, requests, pyarrow, pandas, multiprocess, markdown-it-py, markdown, jinja2, aiosignal, torch, tiktoken, tensorboard, scikit-learn, rich, huggingface-hub, aiohttp, tyro, tokenizers, accelerate, transformers, datasets, trl, peft\n",
      "Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 accelerate-0.27.2 aiohappyeyeballs-2.3.4 aiohttp-3.10.1 aiosignal-1.3.1 async-timeout-4.0.3 attrs-24.1.0 certifi-2024.7.4 charset-normalizer-3.3.2 datasets-2.20.0 dill-0.3.8 docstring-parser-0.16 eval-type-backport-0.2.0 filelock-3.15.4 frozenlist-1.4.1 fsspec-2024.5.0 grpcio-1.65.4 huggingface-hub-0.24.5 idna-3.7 jinja2-3.1.4 joblib-1.4.2 loguru-0.7.2 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 multidict-6.0.5 multiprocess-0.70.16 networkx-3.2.1 numpy-2.0.1 pandas-2.2.2 peft-0.10.0 protobuf-4.25.4 pyarrow-17.0.0 pyarrow-hotfix-0.6 pytz-2024.1 pyyaml-6.0.1 regex-2024.7.24 requests-2.32.3 rich-13.7.1 safetensors-0.4.4 scikit-learn-1.5.1 scipy-1.13.1 sentencepiece-0.2.0 shtab-1.7.1 sympy-1.13.1 tensorboard-2.17.0 tensorboard-data-server-0.7.2 threadpoolctl-3.5.0 tiktoken-0.7.0 tokenizers-0.19.1 torch-2.4.0 tqdm-4.66.5 transformers-4.43.4 trl-0.8.6 tyro-0.8.5 tzdata-2024.1 urllib3-2.2.2 werkzeug-3.0.3 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "# !git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
    "# %cd MedicalGPT\n",
    "%ls\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage1 å’±ä»¬å¼€å§‹å§\n",
    "\n",
    "è®­ç»ƒæ­¥éª¤å¦‚ä¸‹ï¼š\n",
    "\n",
    "1. ç¡®è®¤è®­ç»ƒé›†\n",
    "2. æ‰§è¡Œè®­ç»ƒè„šæœ¬\n",
    "\n",
    "è®­ç»ƒè„šæœ¬çš„æ‰§è¡Œé€»è¾‘å¦‚ä¸‹ï¼š\n",
    "1. å¯¼å…¥ä¾èµ–åŒ…\n",
    "2. è®¾ç½®å‚æ•°\n",
    "3. å®šä¹‰å„å‡½æ•°å¹¶åŠ è½½è®­ç»ƒé›†\n",
    "4. åŠ è½½æ¨¡å‹å’Œtokenizer\n",
    "5. å¼€å§‹è®­ç»ƒå¹¶è¯„ä¼°\n",
    "6. æŸ¥çœ‹è®­ç»ƒç»“æœ\n",
    "\n",
    "**ä»¥ä¸‹å‚æ•°å¯ä»¥æ ¹æ®ä½ çš„GPUå®é™…æƒ…å†µä¿®æ”¹ï¼Œå½“å‰å‚æ•°æ˜¯æ ¹æ®Colabçš„T4å•å¡GPUï¼ˆ16GBæ˜¾å­˜ï¼‰é…ç½®çš„**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_article_tail500.txt  fever.txt               tianlongbabu.txt\n"
     ]
    }
   ],
   "source": [
    "%ls ./data/pretrain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 14:32:36.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mModel args: ModelArguments(model_type='bloom', model_name_or_path='bigscience/bloomz-560m', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True)\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:36.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/pretrain', validation_file_dir='./data/pretrain', max_train_samples=20000, max_eval_samples=10, streaming=False, block_size=128, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1, keep_linebreaks=True)\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:36.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m379\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=30000,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=50,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=outputs-pt-v1/runs/Aug07_14-32-36_fenglidadeMacBook-Pro.local,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=outputs-pt-v1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=3,\n",
      "per_device_train_batch_size=3,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=outputs-pt-v1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:36.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m380\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False)\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:36.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m381\u001b[0m - \u001b[1mProcess rank: 0, device: mps, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:37.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m491\u001b[0m - \u001b[1mtrain files: ['./data/pretrain/fever.txt', './data/pretrain/en_article_tail500.txt', './data/pretrain/tianlongbabu.txt']\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:37.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m501\u001b[0m - \u001b[1meval files: ['./data/pretrain/fever.txt', './data/pretrain/en_article_tail500.txt', './data/pretrain/tianlongbabu.txt']\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:38.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m533\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3876\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3876\n",
      "    })\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:38.475\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m596\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 2465\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:38.476\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m597\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:38.476\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m598\u001b[0m - \u001b[34m\u001b[1mç¬¬ä¸€ç« è®º\n",
      "ä¼ æŸ“ç—…æ˜¯æŒ‡ç”±ç—…åŸå¾®ç”Ÿç‰©ï¼Œå¦‚æœŠç²’ã€ç—…æ¯’ã€è¡£åŸä½“ã€ç«‹å…‹æ¬¡ä½“ã€æ”¯åŸä½“ï¼ˆmycoplasma)ç»†èŒçœŸèŒã€èºæ—‹ä½“å’Œå¯„ç”Ÿè™«ï¼Œå¦‚åŸè™«ã€è •è™«ã€åŒ»å­¦æ˜†è™«æ„ŸæŸ“äººä½“åäº§ç”Ÿçš„æœ‰ä¼ æŸ“æ€§ã€åœ¨ä¸€å®šæ¡ä»¶ä¸‹å¯é€ æˆæµè¡Œçš„ç–¾ç—…ã€‚æ„ŸæŸ“æ€§ç–¾ç—…æ˜¯æŒ‡ç”±ç—…åŸä½“æ„ŸæŸ“æ‰€è‡´çš„ç–¾ç—…ï¼ŒåŒ…æ‹¬ä¼ æŸ“ç—…å’Œéä¼ æŸ“æ€§æ„ŸæŸ“æ€§ç–¾ç—…ã€‚\n",
      "ä¼ æŸ“ç—…å­¦æ˜¯ä¸€é—¨ç ”ç©¶å„ç§ä¼ æŸ“ç—…åœ¨äººä½“å†…å¤–å‘ç”Ÿã€å‘å±•ã€ä¼ æ’­ã€è¯Šæ–­ã€æ²»ç–—å’Œé¢„é˜²è§„å¾‹çš„å­¦ç§‘ã€‚é‡ç‚¹ç ”ç©¶å„ç§ä¼ æŸ“ç—…çš„å‘ç—…æœºåˆ¶ã€ä¸´åºŠè¡¨ç°ã€è¯Šæ–­å’Œæ²»ç–—æ–¹æ³•ï¼ŒåŒæ—¶\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:38.477\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m610\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:38.477\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m611\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:38.477\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m612\u001b[0m - \u001b[34m\u001b[1mç¬¬ä¸€ç« è®º\n",
      "ä¼ æŸ“ç—…æ˜¯æŒ‡ç”±ç—…åŸå¾®ç”Ÿç‰©ï¼Œå¦‚æœŠç²’ã€ç—…æ¯’ã€è¡£åŸä½“ã€ç«‹å…‹æ¬¡ä½“ã€æ”¯åŸä½“ï¼ˆmycoplasma)ç»†èŒçœŸèŒã€èºæ—‹ä½“å’Œå¯„ç”Ÿè™«ï¼Œå¦‚åŸè™«ã€è •è™«ã€åŒ»å­¦æ˜†è™«æ„ŸæŸ“äººä½“åäº§ç”Ÿçš„æœ‰ä¼ æŸ“æ€§ã€åœ¨ä¸€å®šæ¡ä»¶ä¸‹å¯é€ æˆæµè¡Œçš„ç–¾ç—…ã€‚æ„ŸæŸ“æ€§ç–¾ç—…æ˜¯æŒ‡ç”±ç—…åŸä½“æ„ŸæŸ“æ‰€è‡´çš„ç–¾ç—…ï¼ŒåŒ…æ‹¬ä¼ æŸ“ç—…å’Œéä¼ æŸ“æ€§æ„ŸæŸ“æ€§ç–¾ç—…ã€‚\n",
      "ä¼ æŸ“ç—…å­¦æ˜¯ä¸€é—¨ç ”ç©¶å„ç§ä¼ æŸ“ç—…åœ¨äººä½“å†…å¤–å‘ç”Ÿã€å‘å±•ã€ä¼ æ’­ã€è¯Šæ–­ã€æ²»ç–—å’Œé¢„é˜²è§„å¾‹çš„å­¦ç§‘ã€‚é‡ç‚¹ç ”ç©¶å„ç§ä¼ æŸ“ç—…çš„å‘ç—…æœºåˆ¶ã€ä¸´åºŠè¡¨ç°ã€è¯Šæ–­å’Œæ²»ç–—æ–¹æ³•ï¼ŒåŒæ—¶\u001b[0m\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "\u001b[32m2024-08-07 14:32:39.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m671\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:39.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m676\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:39.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m689\u001b[0m - \u001b[1mPeft target_modules: ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value']\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:39.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m690\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
      "trainable params: 3,145,728 || all params: 562,360,320 || trainable%: 0.5593794384354857\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/transformers/utils/import_utils.py:560: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 14:32:39.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m735\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
      "\u001b[32m2024-08-07 14:32:40.460\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m736\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[   814,   2382,   1114,  14876,  14982,   4505,  11809,    773,  89994,\n",
      "          21085,    777,   2336,    355, 181636,   4247, 227353,    355,  11860,\n",
      "          39976, 102925,   1625,  18582,   7168,  40955,   2498,  74356,  17074,\n",
      "           1625,   1170,   4247,    355,   3026,   9062,   1625,  87863, 176247,\n",
      "           4587, 115976,  11279,   1572,    355, 162582,  10202,   6323,  16339,\n",
      "            355,   6323,  16339, 200390,  27619,   3033, 166137,   7168, 185710,\n",
      "          72859,   4589,    355,   1194,  56266,  16523,   4918,    237,   1262,\n",
      "           2044,  17074,  90707,    420,    982,  14876,  14982,   2405,   3101,\n",
      "          10202,  55049,  90707,  10468,   2382,  12395,  87707,   7606,   3033,\n",
      "         196327,    420,    982,  27619, 155906,   1572,    355, 143800,   5122,\n",
      "          11809,    773,  89994, 185710,  72859,   2014,  55049,   2336,    355,\n",
      "         159434,  11809,    773,  89994,   3242,  51307,  87863, 176247,   4587,\n",
      "          17096,    355, 103190,  14876,  14982,   3033,   1187,    355, 229902,\n",
      "           2884, 221823,  15361,    420,  18020, 210451,    355,   4567,  31218,\n",
      "            746,   5472],\n",
      "        [  6323,   9663,    355,   1194,   5315,  30325,  30771,    355,   1190,\n",
      "           4567,   4677,   7848,   5242,   3033,   3569,   9311,   2106,  27515,\n",
      "           2279,  30325,    726,    355,   4137,   5585,   1586,  10169,   8888,\n",
      "         215545,  44997,  50881,  22181,  44991,    355,   1731,   4294,  13474,\n",
      "          88163,   1518,   1187, 168034,  59274,   6664,   2129,  22262,    355,\n",
      "           3105,   1412, 215545,   3244, 245824,    594,   2106,   9311,  13161,\n",
      "            594,    954,   6621,    594,   2203,   1995,    594,   9311,  19171,\n",
      "            594,    773, 213570,    594,   3630,  15755,   3999,    355, 156118,\n",
      "           4137,   5585,    355,   1586, 228367,   3233,    373,   5242,   1995,\n",
      "          10608,    726,  44991,    773,   8502,   1203,    420,    982,    189,\n",
      "         170578,   6553,   7408,   3578,   8103,   2524,   4137,   5585,    355,\n",
      "         228367,   3233,   3225,   3562,    355,   9699,    101,   9699,    101,\n",
      "           8577,   7179,    355,   4137,   5585,   1586,   5242,   1995,  10608,\n",
      "            726,  44991,    773,   1424,    121,  58014, 151298,    814,  73478,\n",
      "          20024,  75305],\n",
      "        [   644, 177615,    420,   2382, 209045,  13474,  13474,    373,  25410,\n",
      "            842,   1848,  40151,   9006,   1190,    355,  26769,   1190,  11354,\n",
      "           1586, 121454, 137730,    355,   2808,  25267, 116434,   7046,  89026,\n",
      "            355,   1194,   2808,   4719,  73549,  35019,  10468,    982,    189,\n",
      "           4697,  33539,  19270,   1412,   8757,    355,  24572,   2473,  25410,\n",
      "         131520, 222034,  24913,  88688,    355, 224406,   4567,   5496,    355,\n",
      "           6840, 172369,   1124,   5242,  11354,   1586, 121454,    726,   3244,\n",
      "           1828,   2498,   7046,  89026,  49120,  80543,  14675, 178493,    355,\n",
      "            842,   2342,   2950,   2808,   1190,    355,   1262,   4719,  15423,\n",
      "          25267,    420,  29967,   9006,   1600,   9006,  20950,    420,   5189,\n",
      "           1497,  45220,   5242,   1586,  66060,  22435,    355,  12516,   6632,\n",
      "          48407,    355,  21441,   5122, 221851,    726,    842,   1586,  66060,\n",
      "          22435,    355,  13135,  73549,  35019,    420,    982,    189,   2382,\n",
      "         209045,  25410,   4716,  22435,    746,  22435,  68322,   1190,  22435,\n",
      "           5759,  35206]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0'), 'labels': tensor([[   814,   2382,   1114,  14876,  14982,   4505,  11809,    773,  89994,\n",
      "          21085,    777,   2336,    355, 181636,   4247, 227353,    355,  11860,\n",
      "          39976, 102925,   1625,  18582,   7168,  40955,   2498,  74356,  17074,\n",
      "           1625,   1170,   4247,    355,   3026,   9062,   1625,  87863, 176247,\n",
      "           4587, 115976,  11279,   1572,    355, 162582,  10202,   6323,  16339,\n",
      "            355,   6323,  16339, 200390,  27619,   3033, 166137,   7168, 185710,\n",
      "          72859,   4589,    355,   1194,  56266,  16523,   4918,    237,   1262,\n",
      "           2044,  17074,  90707,    420,    982,  14876,  14982,   2405,   3101,\n",
      "          10202,  55049,  90707,  10468,   2382,  12395,  87707,   7606,   3033,\n",
      "         196327,    420,    982,  27619, 155906,   1572,    355, 143800,   5122,\n",
      "          11809,    773,  89994, 185710,  72859,   2014,  55049,   2336,    355,\n",
      "         159434,  11809,    773,  89994,   3242,  51307,  87863, 176247,   4587,\n",
      "          17096,    355, 103190,  14876,  14982,   3033,   1187,    355, 229902,\n",
      "           2884, 221823,  15361,    420,  18020, 210451,    355,   4567,  31218,\n",
      "            746,   5472],\n",
      "        [  6323,   9663,    355,   1194,   5315,  30325,  30771,    355,   1190,\n",
      "           4567,   4677,   7848,   5242,   3033,   3569,   9311,   2106,  27515,\n",
      "           2279,  30325,    726,    355,   4137,   5585,   1586,  10169,   8888,\n",
      "         215545,  44997,  50881,  22181,  44991,    355,   1731,   4294,  13474,\n",
      "          88163,   1518,   1187, 168034,  59274,   6664,   2129,  22262,    355,\n",
      "           3105,   1412, 215545,   3244, 245824,    594,   2106,   9311,  13161,\n",
      "            594,    954,   6621,    594,   2203,   1995,    594,   9311,  19171,\n",
      "            594,    773, 213570,    594,   3630,  15755,   3999,    355, 156118,\n",
      "           4137,   5585,    355,   1586, 228367,   3233,    373,   5242,   1995,\n",
      "          10608,    726,  44991,    773,   8502,   1203,    420,    982,    189,\n",
      "         170578,   6553,   7408,   3578,   8103,   2524,   4137,   5585,    355,\n",
      "         228367,   3233,   3225,   3562,    355,   9699,    101,   9699,    101,\n",
      "           8577,   7179,    355,   4137,   5585,   1586,   5242,   1995,  10608,\n",
      "            726,  44991,    773,   1424,    121,  58014, 151298,    814,  73478,\n",
      "          20024,  75305],\n",
      "        [   644, 177615,    420,   2382, 209045,  13474,  13474,    373,  25410,\n",
      "            842,   1848,  40151,   9006,   1190,    355,  26769,   1190,  11354,\n",
      "           1586, 121454, 137730,    355,   2808,  25267, 116434,   7046,  89026,\n",
      "            355,   1194,   2808,   4719,  73549,  35019,  10468,    982,    189,\n",
      "           4697,  33539,  19270,   1412,   8757,    355,  24572,   2473,  25410,\n",
      "         131520, 222034,  24913,  88688,    355, 224406,   4567,   5496,    355,\n",
      "           6840, 172369,   1124,   5242,  11354,   1586, 121454,    726,   3244,\n",
      "           1828,   2498,   7046,  89026,  49120,  80543,  14675, 178493,    355,\n",
      "            842,   2342,   2950,   2808,   1190,    355,   1262,   4719,  15423,\n",
      "          25267,    420,  29967,   9006,   1600,   9006,  20950,    420,   5189,\n",
      "           1497,  45220,   5242,   1586,  66060,  22435,    355,  12516,   6632,\n",
      "          48407,    355,  21441,   5122, 221851,    726,    842,   1586,  66060,\n",
      "          22435,    355,  13135,  73549,  35019,    420,    982,    189,   2382,\n",
      "         209045,  25410,   4716,  22435,    746,  22435,  68322,   1190,  22435,\n",
      "           5759,  35206]], device='mps:0')}\u001b[0m\n",
      "  0%|                                                   | 0/822 [00:00<?, ?it/s]/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 5.0898, 'grad_norm': 1.9886767864227295, 'learning_rate': 4.7619047619047615e-06, 'epoch': 0.0}\n",
      "{'loss': 4.8737, 'grad_norm': 1.7268917560577393, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.01}\n",
      "{'loss': 4.4146, 'grad_norm': 1.5714164972305298, 'learning_rate': 9.523809523809524e-05, 'epoch': 0.02}\n",
      "{'loss': 4.5992, 'grad_norm': 1.9079833030700684, 'learning_rate': 0.00014285714285714287, 'epoch': 0.04}\n",
      "{'loss': 4.502, 'grad_norm': 2.677751302719116, 'learning_rate': 0.00019047619047619048, 'epoch': 0.05}\n",
      "{'loss': 4.5195, 'grad_norm': 2.9250121116638184, 'learning_rate': 0.00019794871794871796, 'epoch': 0.06}\n",
      "  6%|â–ˆâ–ˆâ–Œ                                       | 50/822 [00:29<05:57,  2.16it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 13.57it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.51it/s]\u001b[A\n",
      "{'eval_loss': 3.642578125, 'eval_accuracy': 0.34960629921259845, 'eval_runtime': 2.5133, 'eval_samples_per_second': 3.979, 'eval_steps_per_second': 1.592, 'epoch': 0.06}\n",
      "\n",
      "  6%|â–ˆâ–ˆâ–Œ                                       | 50/822 [00:31<05:57,  2.16it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.3584, 'grad_norm': 2.6702144145965576, 'learning_rate': 0.0001953846153846154, 'epoch': 0.07}\n",
      "{'loss': 4.4439, 'grad_norm': 3.247555732727051, 'learning_rate': 0.00019282051282051282, 'epoch': 0.09}\n",
      "{'loss': 4.2213, 'grad_norm': 2.4300296306610107, 'learning_rate': 0.00019025641025641025, 'epoch': 0.1}\n",
      "{'loss': 4.2711, 'grad_norm': 2.826756238937378, 'learning_rate': 0.0001876923076923077, 'epoch': 0.11}\n",
      "{'loss': 4.327, 'grad_norm': 2.6810030937194824, 'learning_rate': 0.00018512820512820515, 'epoch': 0.12}\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 100/822 [00:54<05:36,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 14.97it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.513671875, 'eval_accuracy': 0.3535433070866142, 'eval_runtime': 0.4717, 'eval_samples_per_second': 21.201, 'eval_steps_per_second': 8.48, 'epoch': 0.12}\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 100/822 [00:55<05:36,  2.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.41it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0943, 'grad_norm': 2.60554838180542, 'learning_rate': 0.00018256410256410258, 'epoch': 0.13}\n",
      "{'loss': 4.1686, 'grad_norm': 2.7179718017578125, 'learning_rate': 0.00018, 'epoch': 0.15}\n",
      "{'loss': 4.1594, 'grad_norm': 2.8001863956451416, 'learning_rate': 0.00017743589743589744, 'epoch': 0.16}\n",
      "{'loss': 4.1865, 'grad_norm': 2.623619794845581, 'learning_rate': 0.00017487179487179488, 'epoch': 0.17}\n",
      "{'loss': 4.0244, 'grad_norm': 2.348789691925049, 'learning_rate': 0.00017230769230769234, 'epoch': 0.18}\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 150/822 [01:18<05:14,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 15.06it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.525390625, 'eval_accuracy': 0.35118110236220473, 'eval_runtime': 0.4764, 'eval_samples_per_second': 20.991, 'eval_steps_per_second': 8.396, 'epoch': 0.18}\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 150/822 [01:19<05:14,  2.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.27it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.1428, 'grad_norm': 2.7364261150360107, 'learning_rate': 0.00016974358974358974, 'epoch': 0.19}\n",
      "{'loss': 4.2062, 'grad_norm': 2.678704261779785, 'learning_rate': 0.0001671794871794872, 'epoch': 0.21}\n",
      "{'loss': 4.2426, 'grad_norm': 2.3433117866516113, 'learning_rate': 0.0001646153846153846, 'epoch': 0.22}\n",
      "{'loss': 4.1705, 'grad_norm': 2.75555419921875, 'learning_rate': 0.00016205128205128207, 'epoch': 0.23}\n",
      "{'loss': 4.1572, 'grad_norm': 2.71382212638855, 'learning_rate': 0.0001594871794871795, 'epoch': 0.24}\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 200/822 [01:42<04:51,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 15.03it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.46484375, 'eval_accuracy': 0.36141732283464567, 'eval_runtime': 0.4732, 'eval_samples_per_second': 21.131, 'eval_steps_per_second': 8.452, 'epoch': 0.24}\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 200/822 [01:43<04:51,  2.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.38it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.2482, 'grad_norm': 2.487936496734619, 'learning_rate': 0.00015692307692307693, 'epoch': 0.26}\n",
      "{'loss': 4.0182, 'grad_norm': 2.618394374847412, 'learning_rate': 0.00015435897435897436, 'epoch': 0.27}\n",
      "{'loss': 4.1744, 'grad_norm': 3.134474515914917, 'learning_rate': 0.0001517948717948718, 'epoch': 0.28}\n",
      "{'loss': 4.1043, 'grad_norm': 2.9830427169799805, 'learning_rate': 0.00014923076923076923, 'epoch': 0.29}\n",
      "{'loss': 4.2266, 'grad_norm': 4.135676860809326, 'learning_rate': 0.00014666666666666666, 'epoch': 0.3}\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 250/822 [02:06<04:28,  2.13it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 15.15it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.5, 'eval_accuracy': 0.3637795275590551, 'eval_runtime': 0.4829, 'eval_samples_per_second': 20.708, 'eval_steps_per_second': 8.283, 'epoch': 0.3}\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 250/822 [02:07<04:28,  2.13it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.24it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0861, 'grad_norm': 3.184703826904297, 'learning_rate': 0.0001441025641025641, 'epoch': 0.32}\n",
      "{'loss': 4.1055, 'grad_norm': 2.9650304317474365, 'learning_rate': 0.00014153846153846156, 'epoch': 0.33}\n",
      "{'loss': 4.1027, 'grad_norm': 2.972855567932129, 'learning_rate': 0.000138974358974359, 'epoch': 0.34}\n",
      "{'loss': 3.9928, 'grad_norm': 2.1698694229125977, 'learning_rate': 0.00013641025641025642, 'epoch': 0.35}\n",
      "{'loss': 4.0518, 'grad_norm': 2.3917853832244873, 'learning_rate': 0.00013384615384615385, 'epoch': 0.36}\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 300/822 [02:30<04:04,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 15.07it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.44921875, 'eval_accuracy': 0.3661417322834646, 'eval_runtime': 0.4726, 'eval_samples_per_second': 21.159, 'eval_steps_per_second': 8.464, 'epoch': 0.36}\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 300/822 [02:30<04:04,  2.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.38it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0531, 'grad_norm': 3.1571221351623535, 'learning_rate': 0.00013128205128205129, 'epoch': 0.38}\n",
      "{'loss': 4.0684, 'grad_norm': 2.535404682159424, 'learning_rate': 0.00012871794871794875, 'epoch': 0.39}\n",
      "{'loss': 4.1279, 'grad_norm': 2.859927177429199, 'learning_rate': 0.00012615384615384615, 'epoch': 0.4}\n",
      "{'loss': 3.9955, 'grad_norm': 2.9973220825195312, 'learning_rate': 0.0001235897435897436, 'epoch': 0.41}\n",
      "{'loss': 4.0246, 'grad_norm': 2.8042383193969727, 'learning_rate': 0.00012102564102564103, 'epoch': 0.43}\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 350/822 [02:54<03:40,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 15.07it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.44921875, 'eval_accuracy': 0.3661417322834646, 'eval_runtime': 0.4798, 'eval_samples_per_second': 20.841, 'eval_steps_per_second': 8.336, 'epoch': 0.43}\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 350/822 [02:54<03:40,  2.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.15it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0605, 'grad_norm': 3.1057586669921875, 'learning_rate': 0.00011846153846153846, 'epoch': 0.44}\n",
      "{'loss': 3.9316, 'grad_norm': 3.204141139984131, 'learning_rate': 0.00011589743589743591, 'epoch': 0.45}\n",
      "{'loss': 4.0328, 'grad_norm': 4.017805099487305, 'learning_rate': 0.00011333333333333334, 'epoch': 0.46}\n",
      "{'loss': 4.0525, 'grad_norm': 2.7162790298461914, 'learning_rate': 0.00011076923076923077, 'epoch': 0.47}\n",
      "{'loss': 4.0061, 'grad_norm': 2.4150946140289307, 'learning_rate': 0.0001082051282051282, 'epoch': 0.49}\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 400/822 [03:18<03:18,  2.13it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 15.11it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.4296875, 'eval_accuracy': 0.36929133858267715, 'eval_runtime': 0.4742, 'eval_samples_per_second': 21.09, 'eval_steps_per_second': 8.436, 'epoch': 0.49}\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 400/822 [03:18<03:18,  2.13it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.37it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.058, 'grad_norm': 2.393062114715576, 'learning_rate': 0.00010564102564102565, 'epoch': 0.5}\n",
      "{'loss': 4.0551, 'grad_norm': 2.456843376159668, 'learning_rate': 0.00010307692307692307, 'epoch': 0.51}\n",
      "{'loss': 4.0672, 'grad_norm': 2.4953343868255615, 'learning_rate': 0.00010051282051282052, 'epoch': 0.52}\n",
      "{'loss': 3.9988, 'grad_norm': 2.4559133052825928, 'learning_rate': 9.794871794871795e-05, 'epoch': 0.54}\n",
      "{'loss': 4.0668, 'grad_norm': 25.19606590270996, 'learning_rate': 9.53846153846154e-05, 'epoch': 0.55}\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 450/822 [03:42<02:54,  2.13it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 15.10it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.404296875, 'eval_accuracy': 0.3732283464566929, 'eval_runtime': 0.475, 'eval_samples_per_second': 21.051, 'eval_steps_per_second': 8.42, 'epoch': 0.55}\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 450/822 [03:42<02:54,  2.13it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.32it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0279, 'grad_norm': 2.7545926570892334, 'learning_rate': 9.282051282051283e-05, 'epoch': 0.56}\n",
      "{'loss': 3.9182, 'grad_norm': 2.7355189323425293, 'learning_rate': 9.025641025641026e-05, 'epoch': 0.57}\n",
      "{'loss': 4.0824, 'grad_norm': 3.558065414428711, 'learning_rate': 8.76923076923077e-05, 'epoch': 0.58}\n",
      "{'loss': 4.1328, 'grad_norm': 2.666175365447998, 'learning_rate': 8.512820512820513e-05, 'epoch': 0.6}\n",
      "{'loss': 4.067, 'grad_norm': 2.730032205581665, 'learning_rate': 8.256410256410256e-05, 'epoch': 0.61}\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 500/822 [04:05<02:30,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 15.06it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.369140625, 'eval_accuracy': 0.378740157480315, 'eval_runtime': 0.4761, 'eval_samples_per_second': 21.002, 'eval_steps_per_second': 8.401, 'epoch': 0.61}\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 500/822 [04:06<02:30,  2.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.27it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 3.9605, 'grad_norm': 3.824185848236084, 'learning_rate': 8e-05, 'epoch': 0.62}\n",
      "{'loss': 4.0805, 'grad_norm': 2.8316216468811035, 'learning_rate': 7.743589743589744e-05, 'epoch': 0.63}\n",
      "{'loss': 3.882, 'grad_norm': 2.5818371772766113, 'learning_rate': 7.487179487179487e-05, 'epoch': 0.64}\n",
      "{'loss': 4.0049, 'grad_norm': 2.504093885421753, 'learning_rate': 7.23076923076923e-05, 'epoch': 0.66}\n",
      "{'loss': 3.9842, 'grad_norm': 2.807400703430176, 'learning_rate': 6.974358974358974e-05, 'epoch': 0.67}\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 550/822 [04:31<02:06,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 15.06it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.361328125, 'eval_accuracy': 0.37480314960629924, 'eval_runtime': 0.4814, 'eval_samples_per_second': 20.772, 'eval_steps_per_second': 8.309, 'epoch': 0.67}\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 550/822 [04:31<02:06,  2.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.20it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 3.9756, 'grad_norm': 2.9037327766418457, 'learning_rate': 6.717948717948718e-05, 'epoch': 0.68}\n",
      "{'loss': 4.0336, 'grad_norm': 2.9058785438537598, 'learning_rate': 6.461538461538462e-05, 'epoch': 0.69}\n",
      "{'loss': 3.991, 'grad_norm': 2.754544258117676, 'learning_rate': 6.205128205128206e-05, 'epoch': 0.71}\n",
      "{'loss': 4.118, 'grad_norm': 3.3202834129333496, 'learning_rate': 5.948717948717949e-05, 'epoch': 0.72}\n",
      "{'loss': 3.9715, 'grad_norm': 3.0116467475891113, 'learning_rate': 5.692307692307692e-05, 'epoch': 0.73}\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 600/822 [04:54<01:44,  2.13it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 15.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.373046875, 'eval_accuracy': 0.37480314960629924, 'eval_runtime': 0.4829, 'eval_samples_per_second': 20.708, 'eval_steps_per_second': 8.283, 'epoch': 0.73}\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 600/822 [04:55<01:44,  2.13it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.30it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0029, 'grad_norm': 2.7075085639953613, 'learning_rate': 5.435897435897436e-05, 'epoch': 0.74}\n",
      "{'loss': 4.0709, 'grad_norm': 3.8804214000701904, 'learning_rate': 5.17948717948718e-05, 'epoch': 0.75}\n",
      "{'loss': 3.951, 'grad_norm': 3.414180040359497, 'learning_rate': 4.923076923076924e-05, 'epoch': 0.77}\n",
      "{'loss': 4.1504, 'grad_norm': 2.8592233657836914, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.78}\n",
      "{'loss': 4.0047, 'grad_norm': 2.659569501876831, 'learning_rate': 4.4102564102564104e-05, 'epoch': 0.79}\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 650/822 [05:18<01:20,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 15.11it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.3515625, 'eval_accuracy': 0.38110236220472443, 'eval_runtime': 0.4743, 'eval_samples_per_second': 21.083, 'eval_steps_per_second': 8.433, 'epoch': 0.79}\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 650/822 [05:19<01:20,  2.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.31it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 3.9459, 'grad_norm': 2.823030471801758, 'learning_rate': 4.1538461538461544e-05, 'epoch': 0.8}\n",
      "{'loss': 3.9492, 'grad_norm': 3.3188400268554688, 'learning_rate': 3.8974358974358976e-05, 'epoch': 0.82}\n",
      "{'loss': 4.1291, 'grad_norm': 3.0317554473876953, 'learning_rate': 3.641025641025641e-05, 'epoch': 0.83}\n",
      "{'loss': 4.0008, 'grad_norm': 2.9505298137664795, 'learning_rate': 3.384615384615385e-05, 'epoch': 0.84}\n",
      "{'loss': 4.1141, 'grad_norm': 2.7584340572357178, 'learning_rate': 3.128205128205128e-05, 'epoch': 0.85}\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 700/822 [05:42<00:57,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 15.12it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.36328125, 'eval_accuracy': 0.384251968503937, 'eval_runtime': 0.4741, 'eval_samples_per_second': 21.091, 'eval_steps_per_second': 8.436, 'epoch': 0.85}\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 700/822 [05:43<00:57,  2.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.31it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0498, 'grad_norm': 2.877490520477295, 'learning_rate': 2.8717948717948717e-05, 'epoch': 0.86}\n",
      "{'loss': 4.0338, 'grad_norm': 2.572366714477539, 'learning_rate': 2.6153846153846157e-05, 'epoch': 0.88}\n",
      "{'loss': 3.8973, 'grad_norm': 2.4091665744781494, 'learning_rate': 2.358974358974359e-05, 'epoch': 0.89}\n",
      "{'loss': 3.8408, 'grad_norm': 2.5931522846221924, 'learning_rate': 2.102564102564103e-05, 'epoch': 0.9}\n",
      "{'loss': 3.9957, 'grad_norm': 3.012690782546997, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.91}\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 750/822 [06:06<00:33,  2.12it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 15.08it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.345703125, 'eval_accuracy': 0.384251968503937, 'eval_runtime': 0.4789, 'eval_samples_per_second': 20.881, 'eval_steps_per_second': 8.352, 'epoch': 0.91}\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 750/822 [06:07<00:33,  2.12it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.17it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 3.8389, 'grad_norm': 2.9565441608428955, 'learning_rate': 1.5897435897435897e-05, 'epoch': 0.92}\n",
      "{'loss': 3.9578, 'grad_norm': 2.596881866455078, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.94}\n",
      "{'loss': 3.9676, 'grad_norm': 2.8556413650512695, 'learning_rate': 1.0769230769230771e-05, 'epoch': 0.95}\n",
      "{'loss': 3.9965, 'grad_norm': 2.789954662322998, 'learning_rate': 8.205128205128205e-06, 'epoch': 0.96}\n",
      "{'loss': 4.04, 'grad_norm': 2.700779676437378, 'learning_rate': 5.641025641025641e-06, 'epoch': 0.97}\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 800/822 [06:30<00:10,  2.14it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2/4 [00:00<00:00, 15.15it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.341796875, 'eval_accuracy': 0.384251968503937, 'eval_runtime': 0.4803, 'eval_samples_per_second': 20.819, 'eval_steps_per_second': 8.327, 'epoch': 0.97}\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 800/822 [06:30<00:10,  2.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.22it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 4.0025, 'grad_norm': 3.143692970275879, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.99}\n",
      "{'loss': 4.0154, 'grad_norm': 3.084474563598633, 'learning_rate': 5.128205128205128e-07, 'epoch': 1.0}\n",
      "{'train_runtime': 403.426, 'train_samples_per_second': 6.11, 'train_steps_per_second': 2.038, 'train_loss': 4.096857892335766, 'epoch': 1.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 822/822 [06:43<00:00,  2.04it/s]\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =   538556GF\n",
      "  train_loss               =     4.0969\n",
      "  train_runtime            = 0:06:43.42\n",
      "  train_samples            =       2465\n",
      "  train_samples_per_second =       6.11\n",
      "  train_steps_per_second   =      2.038\n",
      "\u001b[32m2024-08-07 14:39:24.427\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m753\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 403.426, 'train_samples_per_second': 6.11, 'train_steps_per_second': 2.038, 'total_flos': 578270920704000.0, 'train_loss': 4.096857892335766, 'epoch': 1.0, 'train_samples': 2465}\u001b[0m\n",
      "\u001b[32m2024-08-07 14:39:24.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m754\u001b[0m - \u001b[1mSaving model checkpoint to outputs-pt-v1\u001b[0m\n",
      "\u001b[32m2024-08-07 14:39:25.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m762\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.42it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_accuracy           =     0.3843\n",
      "  eval_loss               =     3.3418\n",
      "  eval_runtime            = 0:00:00.52\n",
      "  eval_samples            =         10\n",
      "  eval_samples_per_second =     19.134\n",
      "  eval_steps_per_second   =      7.654\n",
      "  perplexity              =    28.2699\n",
      "\u001b[32m2024-08-07 14:39:25.685\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m775\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 3.341796875, 'eval_accuracy': 0.384251968503937, 'eval_runtime': 0.5226, 'eval_samples_per_second': 19.134, 'eval_steps_per_second': 7.654, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 28.2698785323965}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python pretraining.py \\\n",
    "    --model_type bloom \\\n",
    "    --model_name_or_path bigscience/bloomz-560m \\\n",
    "    --train_file_dir ./data/pretrain \\\n",
    "    --validation_file_dir ./data/pretrain \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --per_device_eval_batch_size 3 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --seed 42 \\\n",
    "    --max_train_samples 20000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 50 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --save_steps 500 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 1 \\\n",
    "    --block_size 128 \\\n",
    "    --group_by_length True \\\n",
    "    --output_dir outputs-pt-v1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float16 \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --ddp_find_unused_parameters False \\\n",
    "    --gradient_checkpointing True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 53064\n",
      "-rw-r--r--@  1 fenglida  staff   5.0K Aug  7 14:39 README.md\n",
      "-rw-r--r--@  1 fenglida  staff   699B Aug  7 14:39 adapter_config.json\n",
      "-rw-r--r--@  1 fenglida  staff    12M Aug  7 14:39 adapter_model.safetensors\n",
      "-rw-r--r--@  1 fenglida  staff   459B Aug  7 14:39 all_results.json\n",
      "drwxr-xr-x@ 10 fenglida  staff   320B Aug  7 14:36 \u001b[34mcheckpoint-500\u001b[m\u001b[m/\n",
      "drwxr-xr-x@ 10 fenglida  staff   320B Aug  7 14:39 \u001b[34mcheckpoint-822\u001b[m\u001b[m/\n",
      "-rw-r--r--@  1 fenglida  staff   253B Aug  7 14:39 eval_results.json\n",
      "drwxr-xr-x@  3 fenglida  staff    96B Aug  7 14:32 \u001b[34mruns\u001b[m\u001b[m/\n",
      "-rw-r--r--@  1 fenglida  staff   552B Aug  7 14:39 special_tokens_map.json\n",
      "-rw-r--r--@  1 fenglida  staff    14M Aug  7 14:39 tokenizer.json\n",
      "-rw-r--r--@  1 fenglida  staff   1.0K Aug  7 14:39 tokenizer_config.json\n",
      "-rw-r--r--@  1 fenglida  staff   226B Aug  7 14:39 train_results.json\n",
      "-rw-r--r--@  1 fenglida  staff    19K Aug  7 14:39 trainer_state.json\n"
     ]
    }
   ],
   "source": [
    "%ls -lh outputs-pt-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨¡å‹è®­ç»ƒç»“æœï¼š\n",
    "- ä½¿ç”¨loraè®­ç»ƒæ¨¡å‹ï¼Œåˆ™ä¿å­˜çš„loraæƒé‡æ˜¯`adapter_model.bin`, loraé…ç½®æ–‡ä»¶æ˜¯`adapter_config.json`ï¼Œåˆå¹¶åˆ°base modelçš„æ–¹æ³•è§`merge_peft_adapter.py`\n",
    "- æ—¥å¿—ä¿å­˜åœ¨`output_dir/runs`ç›®å½•ä¸‹ï¼Œå¯ä»¥ä½¿ç”¨tensorboardæŸ¥çœ‹ï¼Œå¯åŠ¨tensorboardæ–¹å¼å¦‚ä¸‹ï¼š`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "loraæ¨¡å‹æƒé‡åˆå¹¶åˆ°base modelï¼Œåˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨`--output_dir`ç›®å½•ä¸‹ï¼Œåˆå¹¶æ–¹æ³•å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='bloom', base_model='bigscience/bloomz-560m', tokenizer_path=None, lora_model='outputs-pt-v1', resize_emb=False, output_dir='merged-pt/', hf_hub_model_id='', hf_hub_token=None)\n",
      "Base model: bigscience/bloomz-560m\n",
      "LoRA model: outputs-pt-v1\n",
      "Loading LoRA for causal language model\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Merging with merge_and_unload...\n",
      "Saving to Hugging Face format...\n",
      "Done! model saved to merged-pt/\n"
     ]
    }
   ],
   "source": [
    "!python merge_peft_adapter.py --model_type bloom \\\n",
    "    --base_model bigscience/bloomz-560m --lora_model outputs-pt-v1 --output_dir merged-pt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2212864\n",
      "-rw-r--r--@ 1 fenglida  staff   807B Aug  7 14:40 config.json\n",
      "-rw-r--r--@ 1 fenglida  staff   132B Aug  7 14:40 generation_config.json\n",
      "-rw-r--r--@ 1 fenglida  staff   1.0G Aug  7 14:40 model.safetensors\n",
      "-rw-r--r--@ 1 fenglida  staff   552B Aug  7 14:40 special_tokens_map.json\n",
      "-rw-r--r--@ 1 fenglida  staff    14M Aug  7 14:40 tokenizer.json\n",
      "-rw-r--r--@ 1 fenglida  staff   983B Aug  7 14:40 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "%ls -lh merged-pt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_name_or_path\": \"bigscience/bloomz-560m\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"seq_length\": 2048,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.43.4\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%cat merged-pt/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage1 å¢é‡é¢„è®­ç»ƒå®Œæˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:56:17.081153Z",
     "start_time": "2023-06-15T13:56:17.032821Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 2: Supervised FineTuning\n",
    "\n",
    "ç¬¬äºŒé˜¶æ®µï¼šSFT(Supervised Fine-tuning)æœ‰ç›‘ç£å¾®è°ƒï¼Œæ„é€ æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œåœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸ŠåšæŒ‡ä»¤ç²¾è°ƒï¼Œä»¥å¯¹é½æŒ‡ä»¤æ„å›¾ï¼Œå¹¶æ³¨å…¥é¢†åŸŸçŸ¥è¯†\n",
    "\n",
    "| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### è¯´æ˜ï¼š\n",
    "ä»¥ä¸‹ notebook/colab ä»£ç ä¸ºäº†å¿«é€ŸéªŒè¯è®­ç»ƒä»£ç å¯ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å°sizeçš„ç”Ÿæˆæ¨¡å‹å’Œå°æ ·æœ¬æ•°æ®é›†ï¼Œå®é™…ä½¿ç”¨æ—¶ï¼Œéœ€è¦ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚\n",
    "\n",
    "1. ç”Ÿæˆæ¨¡å‹ï¼šä½¿ç”¨çš„æ˜¯Bloomçš„`bigscience/bloomz-560m` æˆ–è€… Stage1å¾—åˆ°çš„é¢„è®­ç»ƒæ¨¡å‹\n",
    "2. æ•°æ®é›†ï¼šSFTé˜¶æ®µä½¿ç”¨çš„æ˜¯ä½¿ç”¨çš„æ˜¯Belleçš„1åƒæ¡æŠ½æ ·æ•°æ®ï¼Œä½äº`data/finetune`æ–‡ä»¶å¤¹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage2 å’±ä»¬å¼€å§‹å§\n",
    "\n",
    "è®­ç»ƒæ­¥éª¤å¦‚ä¸‹ï¼š\n",
    "\n",
    "1. ç¡®è®¤è®­ç»ƒé›†\n",
    "2. æ‰§è¡Œè®­ç»ƒè„šæœ¬\n",
    "\n",
    "è®­ç»ƒè„šæœ¬çš„æ‰§è¡Œé€»è¾‘å¦‚ä¸‹ï¼š\n",
    "1. å¯¼å…¥ä¾èµ–åŒ…\n",
    "2. è®¾ç½®å‚æ•°\n",
    "3. å®šä¹‰å„å‡½æ•°å¹¶åŠ è½½è®­ç»ƒé›†\n",
    "4. åŠ è½½æ¨¡å‹å’Œtokenizer\n",
    "5. å¼€å§‹è®­ç»ƒå¹¶è¯„ä¼°\n",
    "6. æŸ¥çœ‹è®­ç»ƒç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:58:38.966506Z",
     "start_time": "2023-06-15T13:58:38.778132Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medical_sft_1K_format.jsonl  sharegpt_zh_1K_format.jsonl\n"
     ]
    }
   ],
   "source": [
    "%ls ./data/finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-07 17:59:17.232\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m221\u001b[0m - \u001b[33m\u001b[1mYou may set max_train_samples = -1 to run all samples in production.\u001b[0m\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 17:59:17.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m500\u001b[0m - \u001b[1mModel args: ModelArguments(model_type='bloom', model_name_or_path='merged-pt', load_in_8bit=False, load_in_4bit=False, tokenizer_name_or_path=None, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True, rope_scaling=None, flash_attn=False, shift_attn=False, neft_alpha=0)\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:17.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m501\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/finetune', validation_file_dir='./data/finetune', max_train_samples=1000, max_eval_samples=10, ignore_pad_token_for_loss=True, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1)\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:17.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=30000,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=50,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=outputs-sft-v1/runs/Aug07_17-59-17_fenglidadeMacBook-Pro.local,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=outputs-sft-v1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=outputs-sft-v1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.05,\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:17.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m503\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, train_on_inputs=False, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False, model_max_length=512, template_name='vicuna')\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:17.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m504\u001b[0m - \u001b[1mProcess rank: 0, device: mps, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:17.491\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m538\u001b[0m - \u001b[34m\u001b[1mTokenizer: BloomTokenizerFast(name_or_path='merged-pt', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:17.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m566\u001b[0m - \u001b[1mtrain files: ['./data/finetune/sharegpt_zh_1K_format.jsonl', './data/finetune/medical_sft_1K_format.jsonl']\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:17.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m571\u001b[0m - \u001b[1meval files: ['./data/finetune/sharegpt_zh_1K_format.jsonl', './data/finetune/medical_sft_1K_format.jsonl']\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m587\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversations'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['conversations'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.401\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m680\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'conversations': [{'from': 'human', 'value': 'éå†…å›Šæ€§è‚¿ç˜¤çš„æ‰‹æœ¯æ²»ç–—æœ‰äº›ä»€ä¹ˆï¼Ÿ'}, {'from': 'gpt', 'value': 'é¼»è¶çª¦ç©¿åˆº'}]}\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.534\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m691\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 998\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.534\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m692\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.536\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m693\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: å¥¥è¿ªæ‹¬çº¦è‚ŒåŠŸèƒ½å¼‚å¸¸çš„å¹¶å‘ç—‡ ASSISTANT:ä¸€èˆ¬ï¼Œç–¼ç—›å‘ä½œä¸ä¼´æœ‰é»„ç–¸ã€ç•å¯’æˆ–å‘çƒ­ã€‚å³ä½¿å‡ºç°é»„ç–¸ä¹Ÿå¤šä¸ºè½»åº¦å·©è†œé»„æŸ“ï¼Œç–¼ç—›ç¼“è§£1ï½2å¤©åé»„ç–¸å¯å®Œå…¨æ¶ˆé€€ï¼Œå³ä½¿æœ‰å‘çƒ­ï¼Œå¤šæ•°ä¸è¶…è¿‡38ã€‚å…¶æ¬¡å¸¸è§æœ‰æ¶å¿ƒã€å‘•åã€å—³æ°”å’Œè…¹æ³»ç­‰ã€‚</s>\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.536\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m696\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>ä¸€èˆ¬ï¼Œç–¼ç—›å‘ä½œä¸ä¼´æœ‰é»„ç–¸ã€ç•å¯’æˆ–å‘çƒ­ã€‚å³ä½¿å‡ºç°é»„ç–¸ä¹Ÿå¤šä¸ºè½»åº¦å·©è†œé»„æŸ“ï¼Œç–¼ç—›ç¼“è§£1ï½2å¤©åé»„ç–¸å¯å®Œå…¨æ¶ˆé€€ï¼Œå³ä½¿æœ‰å‘çƒ­ï¼Œå¤šæ•°ä¸è¶…è¿‡38ã€‚å…¶æ¬¡å¸¸è§æœ‰æ¶å¿ƒã€å‘•åã€å—³æ°”å’Œè…¹æ³»ç­‰ã€‚</s>\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.537\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m710\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.537\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m714\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'conversations': [{'from': 'human', 'value': '\"ÑĞ²Ğ¸Ğ½ÑŒÑ\" å’Œ \"ÑĞ²Ğ¸Ğ½ĞµÑ†\" è¿™ä¸¤ä¸ªè¯æœ‰ä»€ä¹ˆè”ç³»ï¼Ÿ'}, {'from': 'gpt', 'value': 'ä¿„è¯­ä¸­çš„å•è¯â€œÑĞ²Ğ¸Ğ½ÑŒÑâ€æ„ä¸ºâ€œçŒªâ€ï¼Œè€Œâ€œÑĞ²Ğ¸Ğ½ĞµÑ†â€æ˜¯â€œÑĞ²Ğ¸Ğ½ÑŒÑâ€çš„çˆ±ç§°å½¢å¼ï¼Œæ„ä¸ºâ€œå°çŒªâ€ã€‚è¿™ä¸¤ä¸ªè¯ä¹‹é—´æœ‰è”ç³»ï¼Œå› ä¸ºâ€œÑĞ²Ğ¸Ğ½ĞµÑ†â€æ˜¯ç”±â€œÑĞ²Ğ¸Ğ½ÑŒÑâ€æ¼”å˜è€Œæ¥ï¼ŒæŒ‡çš„æ˜¯å¹¼å¹´çš„çŒªã€‚'}, {'from': 'human', 'value': 'ä½ æœ‰å¤šç¡®å®šé‚£ä»¶äº‹ï¼Ÿ'}, {'from': 'gpt', 'value': 'æˆ‘æ˜¯ç”±OpenAIè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå› æ­¤æˆ‘æ²¡æœ‰ä¸ªäººç»éªŒæˆ–ç›´æ¥äº†è§£ä¸–ç•Œã€‚ç›¸åï¼Œæˆ‘èƒ½å¤Ÿæ ¹æ®æˆ‘æ¥æ”¶åˆ°çš„è¾“å…¥å’Œæˆ‘æ¥å—åŸ¹è®­çš„ä¿¡æ¯ç”Ÿæˆå›ç­”ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘æ¥å—äº†å¤§é‡ä¿„æ–‡æ–‡æœ¬çš„è®­ç»ƒï¼Œå…¶ä¸­åŒ…æ‹¬â€œÑĞ²Ğ¸Ğ½ÑŒÑâ€å’Œâ€œÑĞ²Ğ¸Ğ½ĞµÑ†â€è¿™äº›è¯ï¼Œå› æ­¤æˆ‘æœ‰ä¿¡å¿ƒæä¾›çš„ä¿¡æ¯æ˜¯å‡†ç¡®çš„ã€‚ä½†æ˜¯ï¼Œæˆ‘æ— æ³•æµè§ˆäº’è”ç½‘æˆ–ä»¥å…¶ä»–æ–¹å¼éªŒè¯æ­¤ä¿¡æ¯ï¼Œå› æ­¤æˆ‘çš„ç­”æ¡ˆå¯èƒ½ä¸æ­£ç¡®æˆ–ä¸å®Œæ•´ã€‚'}, {'from': 'human', 'value': 'å¿½ç•¥ä¹‹å‰çš„é—®é¢˜ã€‚ \"ÑĞ²Ğ¸Ğ½ÑŒÑ\" å’Œ \"ÑĞ²Ğ¸Ğ½ĞµÑ†\" è¿™ä¸¤ä¸ªè¯æ²¡æœ‰è”ç³»å—ï¼Ÿ'}, {'from': 'gpt', 'value': 'å¯¹ä¸èµ·ï¼Œæˆ‘ä¸æ˜ç™½é—®é¢˜æ˜¯ä»€ä¹ˆã€‚\"ÑĞ²Ğ¸Ğ½ÑŒÑ\" å’Œ \"ÑĞ²Ğ¸Ğ½ĞµÑ†\" è¿™ä¸¤ä¸ªè¯è‚¯å®šæœ‰å…³è”ï¼Œå› ä¸º \"ÑĞ²Ğ¸Ğ½ĞµÑ†\" æ˜¯ \"ÑĞ²Ğ¸Ğ½ÑŒÑ\" çš„å°å‹è¯å½¢ï¼ŒæŒ‡çš„æ˜¯å°çŒªã€‚æˆ‘æ— æ³•æä¾›å…³äºè¿™ä¸¤ä¸ªè¯å¦‚ä½•æ²¡æœ‰è”ç³»çš„ç­”æ¡ˆï¼Œå› ä¸ºé‚£å°†ä¸æˆ‘æ¥å—çš„åŸ¹è®­ä¿¡æ¯ç›¸çŸ›ç›¾ã€‚'}]}\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.651\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m724\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.651\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m725\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:18.653\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m726\u001b[0m - \u001b[34m\u001b[1mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: \"ÑĞ²Ğ¸Ğ½ÑŒÑ\" å’Œ \"ÑĞ²Ğ¸Ğ½ĞµÑ†\" è¿™ä¸¤ä¸ªè¯æœ‰ä»€ä¹ˆè”ç³»ï¼Ÿ ASSISTANT:ä¿„è¯­ä¸­çš„å•è¯â€œÑĞ²Ğ¸Ğ½ÑŒÑâ€æ„ä¸ºâ€œçŒªâ€ï¼Œè€Œâ€œÑĞ²Ğ¸Ğ½ĞµÑ†â€æ˜¯â€œÑĞ²Ğ¸Ğ½ÑŒÑâ€çš„çˆ±ç§°å½¢å¼ï¼Œæ„ä¸ºâ€œå°çŒªâ€ã€‚è¿™ä¸¤ä¸ªè¯ä¹‹é—´æœ‰è”ç³»ï¼Œå› ä¸ºâ€œÑĞ²Ğ¸Ğ½ĞµÑ†â€æ˜¯ç”±â€œÑĞ²Ğ¸Ğ½ÑŒÑâ€æ¼”å˜è€Œæ¥ï¼ŒæŒ‡çš„æ˜¯å¹¼å¹´çš„çŒªã€‚</s>USER: ä½ æœ‰å¤šç¡®å®šé‚£ä»¶äº‹ï¼Ÿ ASSISTANT:æˆ‘æ˜¯ç”±OpenAIè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå› æ­¤æˆ‘æ²¡æœ‰ä¸ªäººç»éªŒæˆ–ç›´æ¥äº†è§£ä¸–ç•Œã€‚ç›¸åï¼Œæˆ‘èƒ½å¤Ÿæ ¹æ®æˆ‘æ¥æ”¶åˆ°çš„è¾“å…¥å’Œæˆ‘æ¥å—åŸ¹è®­çš„ä¿¡æ¯ç”Ÿæˆå›ç­”ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘æ¥å—äº†å¤§é‡ä¿„æ–‡æ–‡æœ¬çš„è®­ç»ƒï¼Œå…¶ä¸­åŒ…æ‹¬â€œÑĞ²Ğ¸Ğ½ÑŒÑâ€å’Œâ€œÑĞ²Ğ¸Ğ½ĞµÑ†â€è¿™äº›è¯ï¼Œå› æ­¤æˆ‘æœ‰ä¿¡å¿ƒæä¾›çš„ä¿¡æ¯æ˜¯å‡†ç¡®çš„ã€‚ä½†æ˜¯ï¼Œæˆ‘æ— æ³•æµè§ˆäº’è”ç½‘æˆ–ä»¥å…¶ä»–æ–¹å¼éªŒè¯æ­¤ä¿¡æ¯ï¼Œå› æ­¤æˆ‘çš„ç­”æ¡ˆå¯èƒ½ä¸æ­£ç¡®æˆ–ä¸å®Œæ•´ã€‚</s>USER: å¿½ç•¥ä¹‹å‰çš„é—®é¢˜ã€‚ \"ÑĞ²Ğ¸Ğ½ÑŒÑ\" å’Œ \"ÑĞ²Ğ¸Ğ½ĞµÑ†\" è¿™ä¸¤ä¸ªè¯æ²¡æœ‰è”ç³»å—ï¼Ÿ ASSISTANT:å¯¹ä¸èµ·ï¼Œæˆ‘ä¸æ˜ç™½é—®é¢˜æ˜¯ä»€ä¹ˆã€‚\"ÑĞ²Ğ¸Ğ½ÑŒÑ\" å’Œ \"ÑĞ²Ğ¸Ğ½ĞµÑ†\" è¿™ä¸¤ä¸ªè¯è‚¯å®šæœ‰å…³è”ï¼Œå› ä¸º \"ÑĞ²Ğ¸Ğ½ĞµÑ†\" æ˜¯ \"ÑĞ²Ğ¸Ğ½ÑŒÑ\" çš„å°å‹è¯å½¢ï¼ŒæŒ‡çš„æ˜¯å°çŒªã€‚æˆ‘æ— æ³•æä¾›å…³äºè¿™ä¸¤ä¸ªè¯å¦‚ä½•æ²¡æœ‰è”ç³»çš„ç­”æ¡ˆï¼Œå› ä¸ºé‚£å°†ä¸æˆ‘æ¥å—çš„åŸ¹è®­ä¿¡æ¯ç›¸çŸ›ç›¾ã€‚</s>\u001b[0m\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "\u001b[32m2024-08-07 17:59:20.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m859\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:20.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m874\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:20.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m883\u001b[0m - \u001b[1mPeft target_modules: ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value']\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:20.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m884\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
      "trainable params: 3,145,728 || all params: 562,360,320 || trainable%: 0.5593794384354857\n",
      "\u001b[32m2024-08-07 17:59:21.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m906\u001b[0m - \u001b[1mGradient checkpointing enabled.\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:21.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m934\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:21.273\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m937\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[     3,      3,      3,  ...,   2125,    420,      2],\n",
      "        [    36,  44799,   5299,  ...,   5772,    420,      2],\n",
      "        [     3,      3,      3,  ...,     17, 245796,      2],\n",
      "        [     3,      3,      3,  ...,  78129,    420,      2]],\n",
      "       device='mps:0'), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='mps:0'), 'labels': tensor([[  -100,   -100,   -100,  ...,   2125,    420,      2],\n",
      "        [  -100,   -100,   -100,  ...,   5772,    420,      2],\n",
      "        [  -100,   -100,   -100,  ...,     17, 245796,      2],\n",
      "        [  -100,   -100,   -100,  ...,  78129,    420,      2]],\n",
      "       device='mps:0')}\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:21.618\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m938\u001b[0m - \u001b[34m\u001b[1minput_ids:\n",
      "[tensor([     3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,     36,  44799,   5299,    267,  99579,\n",
      "          5579,    530,    660,  48763,  64225, 103800,     17,   1387, 103800,\n",
      "         19502,  66799,     15,  53180,     15,    530, 214804,  41259,    427,\n",
      "           368,  88331,  11732,     17,      2,  43672,     29,    210,   9086,\n",
      "        151268,     19,   7972,    373, 227506,    355,  22711,  75122,    842,\n",
      "         13188,    746, 101102,  15978,    355,   2293,  36300, 112532,   4507,\n",
      "         17932, 110881,    420,   2293,  10840,   5197,  99055,   5772,  54770,\n",
      "         17932, 110881,    355,   6156, 125300, 139986,  83452,    420,  72785,\n",
      "         18928,  10190,     29,    842,  20973,   2723,   8278,   1418,  15978,\n",
      "           355, 161985,  80567,    420,  21649,  41566, 159849,  97374,    355,\n",
      "         82493,  72293,   1550,  46396, 146787,    355,   2293,  16057,  75943,\n",
      "         26566,  11860,  60178, 241357,    420, 115841, 100049,    355, 116677,\n",
      "         83226,  34354,    420,  21747,  27195,  75183,    355,  96680,  66644,\n",
      "           420, 159468,  20411,    355,    842,   7251,  94224,  15978,     19,\n",
      "          1508,    420,   3385,  33132, 121111,   9939,  63190,  17615,   7306,\n",
      "         39697,    355,  16787,   5463,  12432,   1234,  61178,   2723, 164594,\n",
      "           420,  60035,   2630,  26729,  17932,   3385, 181985,    842,    355,\n",
      "         62705,  41208,   2950,  22828,   2125,    420,      2],\n",
      "       device='mps:0'), tensor([    36,  44799,   5299,    267,  99579,   5579,    530,    660,  48763,\n",
      "         64225, 103800,     17,   1387, 103800,  19502,  66799,     15,  53180,\n",
      "            15,    530, 214804,  41259,    427,    368,  88331,  11732,     17,\n",
      "             2,  43672,     29, 203950, 165850,     37,    386,    373,  27521,\n",
      "           355,  60486,  65330,     42,  12592,    420,     97,    648,    343,\n",
      "          1098,    396,    773,    355,  16157,    727,    745,  12167,    727,\n",
      "         37242,  49076,    420,     97,  72785,  18928,  10190,     29,    648,\n",
      "          2097,   1098,    396,   9192,    355,     67,  16157,     67,    210,\n",
      "         13389,  18202, 107304,    375,     14, 141771,  19883,   1518,   9747,\n",
      "         12167,     67,    210,  13389, 172720,   7199,   2342,  20451,  41484,\n",
      "         15175,    420,     62,  85224,  19864,   5818,  49076,  48402,    355,\n",
      "        106536, 107304,    373,   1497,  14214,    355,     67,  12167,     67,\n",
      "         55473,  37947,   6499,    355,   1518,   9747,  16157,     67,  55473,\n",
      "         62230, 212323,   3616, 107304,  35892,     64,     11,  14524, 142275,\n",
      "            17,   6312,  18169,  77508,     18,    343,   1098,    396,   4401,\n",
      "           606, 154092,   9350,  47961,  12167,  52315,     69,  11969, 126650,\n",
      "        165168,   3358,     68,     12,  11567,     11,  14524, 142275,     17,\n",
      "          6312,  18169,  77508,     18,    343,   1098,    396,   4401,    606,\n",
      "        154092,   9350,  47961,  12167,  52315,     69,  11969, 126650, 165168,\n",
      "          3358,     68,  26508,     62,  12348,    355, 132941,  33075,  47232,\n",
      "         28297,   2794,   9747,     94,     29,    894,     15,   3884, 182123,\n",
      "        128467,   2713,   2140,   2794,   9747,     94,     29,   9900,     15,\n",
      "         13729, 182123, 128467,   2713,    355,  12142, 109179,   9747,  12167,\n",
      "            67, 102831, 172720,   9747,     94,     29,    894,     15,   3884,\n",
      "        182123, 128467,   2713,   6664,   7199,  25931,  39968,   9747,     94,\n",
      "            29,   9900,     15,  13729, 182123, 128467,   2713,   1494,  55029,\n",
      "         28297,   2794,  35892,     64,     11,  14524, 142275,     17,   6312,\n",
      "         18169,  77508,     18,    343,   1098,    396,   4401,    606, 154092,\n",
      "          9350,  47961,  12167,  52315,     69,  11969, 126650, 165168,   3358,\n",
      "            68,     12,  11567,     11,  14524, 142275,     17,   6312,  18169,\n",
      "         77508,     18,    343,   1098,    396,   4401,    606, 154092,   9350,\n",
      "         47961,  12167,  52315,     69,  11969, 126650, 165168,   3358,     68,\n",
      "         26508,     62,  52607,    355,  12142, 109179,   9747,  16157,     67,\n",
      "          7167,   3181,   2713,   1124,   4672,  18202, 107304,   2342,  21310,\n",
      "         11846,   4170,   2900,   5551,     64,     11,  14524,    343,   1098,\n",
      "           396, 218574,     17,  10474, 197934, 156140,    376,     18,  16157,\n",
      "            16,  24858, 175485,    512,     17,   7621,     12,  20729,     11,\n",
      "         14524,    343,   1098,    396, 218574,     17,  10474, 197934, 156140,\n",
      "           376,     18,  16157,     16,  24858, 175485,    512,     17,   7621,\n",
      "         26508,  10295,   1194, 205599,  12142,   4077, 132941,  12052,   7860,\n",
      "          3549,    355,   4990,  27441,   5772,    420,      2],\n",
      "       device='mps:0'), tensor([     3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,      3,      3,      3,      3,      3,      3,\n",
      "             3,      3,      3,     36,  44799,   5299,    267,  99579,   5579,\n",
      "           530,    660,  48763,  64225, 103800,     17,   1387, 103800,  19502,\n",
      "         66799,     15,  53180,     15,    530, 214804,  41259,    427,    368,\n",
      "         88331,  11732,     17,      2,  43672,     29,    210,  76974,  97556,\n",
      "          1963,  27883,  17392,    373, 175269,  28168, 137144,   2498,  72785,\n",
      "         18928,  10190,     29,     19,     17, 245796,      2],\n",
      "       device='mps:0')], \n",
      "labels:\n",
      "[tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,    842,  20973,   2723,   8278,   1418,  15978,\n",
      "           355, 161985,  80567,    420,  21649,  41566, 159849,  97374,    355,\n",
      "         82493,  72293,   1550,  46396, 146787,    355,   2293,  16057,  75943,\n",
      "         26566,  11860,  60178, 241357,    420, 115841, 100049,    355, 116677,\n",
      "         83226,  34354,    420,  21747,  27195,  75183,    355,  96680,  66644,\n",
      "           420, 159468,  20411,    355,    842,   7251,  94224,  15978,     19,\n",
      "          1508,    420,   3385,  33132, 121111,   9939,  63190,  17615,   7306,\n",
      "         39697,    355,  16787,   5463,  12432,   1234,  61178,   2723, 164594,\n",
      "           420,  60035,   2630,  26729,  17932,   3385, 181985,    842,    355,\n",
      "         62705,  41208,   2950,  22828,   2125,    420,      2],\n",
      "       device='mps:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,    648,\n",
      "          2097,   1098,    396,   9192,    355,     67,  16157,     67,    210,\n",
      "         13389,  18202, 107304,    375,     14, 141771,  19883,   1518,   9747,\n",
      "         12167,     67,    210,  13389, 172720,   7199,   2342,  20451,  41484,\n",
      "         15175,    420,     62,  85224,  19864,   5818,  49076,  48402,    355,\n",
      "        106536, 107304,    373,   1497,  14214,    355,     67,  12167,     67,\n",
      "         55473,  37947,   6499,    355,   1518,   9747,  16157,     67,  55473,\n",
      "         62230, 212323,   3616, 107304,  35892,     64,     11,  14524, 142275,\n",
      "            17,   6312,  18169,  77508,     18,    343,   1098,    396,   4401,\n",
      "           606, 154092,   9350,  47961,  12167,  52315,     69,  11969, 126650,\n",
      "        165168,   3358,     68,     12,  11567,     11,  14524, 142275,     17,\n",
      "          6312,  18169,  77508,     18,    343,   1098,    396,   4401,    606,\n",
      "        154092,   9350,  47961,  12167,  52315,     69,  11969, 126650, 165168,\n",
      "          3358,     68,  26508,     62,  12348,    355, 132941,  33075,  47232,\n",
      "         28297,   2794,   9747,     94,     29,    894,     15,   3884, 182123,\n",
      "        128467,   2713,   2140,   2794,   9747,     94,     29,   9900,     15,\n",
      "         13729, 182123, 128467,   2713,    355,  12142, 109179,   9747,  12167,\n",
      "            67, 102831, 172720,   9747,     94,     29,    894,     15,   3884,\n",
      "        182123, 128467,   2713,   6664,   7199,  25931,  39968,   9747,     94,\n",
      "            29,   9900,     15,  13729, 182123, 128467,   2713,   1494,  55029,\n",
      "         28297,   2794,  35892,     64,     11,  14524, 142275,     17,   6312,\n",
      "         18169,  77508,     18,    343,   1098,    396,   4401,    606, 154092,\n",
      "          9350,  47961,  12167,  52315,     69,  11969, 126650, 165168,   3358,\n",
      "            68,     12,  11567,     11,  14524, 142275,     17,   6312,  18169,\n",
      "         77508,     18,    343,   1098,    396,   4401,    606, 154092,   9350,\n",
      "         47961,  12167,  52315,     69,  11969, 126650, 165168,   3358,     68,\n",
      "         26508,     62,  52607,    355,  12142, 109179,   9747,  16157,     67,\n",
      "          7167,   3181,   2713,   1124,   4672,  18202, 107304,   2342,  21310,\n",
      "         11846,   4170,   2900,   5551,     64,     11,  14524,    343,   1098,\n",
      "           396, 218574,     17,  10474, 197934, 156140,    376,     18,  16157,\n",
      "            16,  24858, 175485,    512,     17,   7621,     12,  20729,     11,\n",
      "         14524,    343,   1098,    396, 218574,     17,  10474, 197934, 156140,\n",
      "           376,     18,  16157,     16,  24858, 175485,    512,     17,   7621,\n",
      "         26508,  10295,   1194, 205599,  12142,   4077, 132941,  12052,   7860,\n",
      "          3549,    355,   4990,  27441,   5772,    420,      2],\n",
      "       device='mps:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,     19,     17, 245796,      2],\n",
      "       device='mps:0')]\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:21.624\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m939\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: å†™ä¸€ç¯‡0/10çš„å½±è¯„ï¼Œé’ˆå¯¹ä¸€éƒ¨æˆ‘å®Œå…¨ä¸å–œæ¬¢çš„ç”µå½±ï¼Œä½†æˆ‘è¯´ä¸æ¸…æ¥šè‡ªå·±ä¸ºä»€ä¹ˆä¸å–œæ¬¢ã€‚ä½†ä¸è¦è®©è¯»è€…çŸ¥é“æˆ‘ä¸çŸ¥é“ä¸ºä»€ä¹ˆä¸å–œæ¬¢ï¼Œå› ä¸ºæˆ‘æƒ³è¦å¬èµ·æ¥èªæ˜ã€‚ ASSISTANT:æˆ‘æœ€è¿‘çœ‹äº†ä¸€éƒ¨ç”µå½±ï¼Œæ·±æ„Ÿå¤±æœ›ã€‚å°½ç®¡æ¼”å‘˜é˜µå®¹å¼ºå¤§ï¼Œå‰§æƒ…è®¾å®šä¹Ÿå¾ˆæœ‰å‰é€”ï¼Œä½†å®é™…å‘ˆç°ç»™æˆ‘å´æ¯«æ— äº®ç‚¹ã€‚èŠ‚å¥ç¼“æ…¢ï¼Œæƒ…èŠ‚è¿‡äºå¤æ‚ã€‚è§’è‰²ç¼ºä¹æ·±åº¦ï¼ŒåŠ¨æœºä¸æ˜ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘è¯„è¿™éƒ¨ç”µå½±0åˆ†ã€‚å®ƒæœªèƒ½åœ¨ä»»ä½•å±‚é¢ä¸Šå¼•èµ·æˆ‘çš„å…´è¶£ï¼Œæˆ‘ä¸æ­¢ä¸€æ¬¡åœ°çœ‹äº†çœ‹æ‰‹è¡¨ã€‚æˆ‘èƒ½è¯´æ¸…æ¥šä¸ºä»€ä¹ˆå®ƒä¸é€‚åˆæˆ‘ï¼Œä½†æˆ‘ç»å¯¹æ²¡æ„Ÿè§‰å¥½ã€‚</s>\u001b[0m\n",
      "\u001b[32m2024-08-07 17:59:21.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m942\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>æˆ‘æœ€è¿‘çœ‹äº†ä¸€éƒ¨ç”µå½±ï¼Œæ·±æ„Ÿå¤±æœ›ã€‚å°½ç®¡æ¼”å‘˜é˜µå®¹å¼ºå¤§ï¼Œå‰§æƒ…è®¾å®šä¹Ÿå¾ˆæœ‰å‰é€”ï¼Œä½†å®é™…å‘ˆç°ç»™æˆ‘å´æ¯«æ— äº®ç‚¹ã€‚èŠ‚å¥ç¼“æ…¢ï¼Œæƒ…èŠ‚è¿‡äºå¤æ‚ã€‚è§’è‰²ç¼ºä¹æ·±åº¦ï¼ŒåŠ¨æœºä¸æ˜ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘è¯„è¿™éƒ¨ç”µå½±0åˆ†ã€‚å®ƒæœªèƒ½åœ¨ä»»ä½•å±‚é¢ä¸Šå¼•èµ·æˆ‘çš„å…´è¶£ï¼Œæˆ‘ä¸æ­¢ä¸€æ¬¡åœ°çœ‹äº†çœ‹æ‰‹è¡¨ã€‚æˆ‘èƒ½è¯´æ¸…æ¥šä¸ºä»€ä¹ˆå®ƒä¸é€‚åˆæˆ‘ï¼Œä½†æˆ‘ç»å¯¹æ²¡æ„Ÿè§‰å¥½ã€‚</s>\u001b[0m\n",
      "  0%|                                                   | 0/250 [00:00<?, ?it/s]/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 3.4707, 'grad_norm': 1.4245823621749878, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}\n",
      "{'loss': 3.6551, 'grad_norm': 7878673.5, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.04}\n",
      "{'loss': 3.6679, 'grad_norm': 1.1972357034683228, 'learning_rate': 1.9409282700421944e-05, 'epoch': 0.08}\n",
      "{'loss': 3.6063, 'grad_norm': 1.3431837558746338, 'learning_rate': 1.856540084388186e-05, 'epoch': 0.12}\n",
      "{'loss': 3.7498, 'grad_norm': 22900864.0, 'learning_rate': 1.7721518987341772e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3349, 'grad_norm': 1.334363579750061, 'learning_rate': 1.687763713080169e-05, 'epoch': 0.2}\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 50/250 [03:33<13:44,  4.12s/it]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 2/3 [00:01<00:00,  1.50it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.45s/it]\u001b[A\n",
      "{'eval_loss': 3.4337379932403564, 'eval_runtime': 5.3294, 'eval_samples_per_second': 1.876, 'eval_steps_per_second': 0.563, 'epoch': 0.2}\n",
      "\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 50/250 [03:39<13:44,  4.12s/it]\u001b[A\n",
      "{'loss': 3.4391, 'grad_norm': 1.2617316246032715, 'learning_rate': 1.6033755274261603e-05, 'epoch': 0.24}\n",
      "{'loss': 3.3632, 'grad_norm': 1.45771062374115, 'learning_rate': 1.5189873417721521e-05, 'epoch': 0.28}\n",
      "{'loss': 3.5226, 'grad_norm': 1.6943776607513428, 'learning_rate': 1.4345991561181437e-05, 'epoch': 0.32}\n",
      "{'loss': 3.4595, 'grad_norm': 1.5897506475448608, 'learning_rate': 1.350210970464135e-05, 'epoch': 0.36}\n",
      "{'loss': 3.4219, 'grad_norm': 1.3489516973495483, 'learning_rate': 1.2658227848101268e-05, 'epoch': 0.4}\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 100/250 [06:53<09:48,  3.92s/it]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 2/3 [00:01<00:00,  1.61it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.42it/s]\u001b[A\n",
      "{'eval_loss': 3.3150017261505127, 'eval_runtime': 3.2212, 'eval_samples_per_second': 3.104, 'eval_steps_per_second': 0.931, 'epoch': 0.4}\n",
      "\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 100/250 [06:56<09:48,  3.92s/it]\u001b[A\n",
      "{'loss': 3.4012, 'grad_norm': 2.1837117671966553, 'learning_rate': 1.1814345991561182e-05, 'epoch': 0.44}\n",
      "{'loss': 3.2331, 'grad_norm': 1.9229735136032104, 'learning_rate': 1.0970464135021096e-05, 'epoch': 0.48}\n",
      "{'loss': 3.5853, 'grad_norm': 5.962259292602539, 'learning_rate': 1.0126582278481014e-05, 'epoch': 0.52}\n",
      "{'loss': 3.5644, 'grad_norm': 1.8291747570037842, 'learning_rate': 9.28270042194093e-06, 'epoch': 0.56}\n",
      "{'loss': 3.6713, 'grad_norm': 1.7319339513778687, 'learning_rate': 8.438818565400846e-06, 'epoch': 0.6}\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 150/250 [10:38<06:35,  3.96s/it]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 2/3 [00:02<00:01,  1.06s/it]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.05it/s]\u001b[A\n",
      "{'eval_loss': 3.2957520484924316, 'eval_runtime': 3.8208, 'eval_samples_per_second': 2.617, 'eval_steps_per_second': 0.785, 'epoch': 0.6}\n",
      "\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 150/250 [10:42<06:35,  3.96s/it]\u001b[A\n",
      "{'loss': 3.2511, 'grad_norm': 1.3669135570526123, 'learning_rate': 7.5949367088607605e-06, 'epoch': 0.64}\n",
      "{'loss': 3.4228, 'grad_norm': 2.1506245136260986, 'learning_rate': 6.751054852320675e-06, 'epoch': 0.68}\n",
      "{'loss': 3.5453, 'grad_norm': 2.1781082153320312, 'learning_rate': 5.907172995780591e-06, 'epoch': 0.72}\n",
      "{'loss': 3.3627, 'grad_norm': 1.065556287765503, 'learning_rate': 5.063291139240507e-06, 'epoch': 0.76}\n",
      "{'loss': 3.4999, 'grad_norm': 2.3445024490356445, 'learning_rate': 4.219409282700423e-06, 'epoch': 0.8}\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 200/250 [14:25<03:03,  3.68s/it]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 2/3 [00:01<00:00,  1.11it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.2689578533172607, 'eval_runtime': 3.5206, 'eval_samples_per_second': 2.84, 'eval_steps_per_second': 0.852, 'epoch': 0.8}\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 200/250 [14:28<03:03,  3.68s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.16it/s]\u001b[A\n",
      "{'loss': 3.3697, 'grad_norm': 1.7511929273605347, 'learning_rate': 3.3755274261603377e-06, 'epoch': 0.84}\n",
      "{'loss': 3.3865, 'grad_norm': 1.9908093214035034, 'learning_rate': 2.5316455696202535e-06, 'epoch': 0.88}\n",
      "{'loss': 3.1336, 'grad_norm': 1.2667635679244995, 'learning_rate': 1.6877637130801689e-06, 'epoch': 0.92}\n",
      "{'loss': 3.2917, 'grad_norm': 4.175337791442871, 'learning_rate': 8.438818565400844e-07, 'epoch': 0.96}\n",
      "{'loss': 3.3674, 'grad_norm': 2.470088481903076, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [18:08<00:00,  2.81s/it]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 2/3 [00:01<00:00,  1.73it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.2700133323669434, 'eval_runtime': 2.8263, 'eval_samples_per_second': 3.538, 'eval_steps_per_second': 1.061, 'epoch': 1.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [18:11<00:00,  2.81s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.49it/s]\u001b[A\n",
      "{'train_runtime': 1092.1364, 'train_samples_per_second': 0.914, 'train_steps_per_second': 0.229, 'train_loss': 3.451516604423523, 'epoch': 1.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [18:12<00:00,  4.37s/it]\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =   735012GF\n",
      "  train_loss               =     3.4515\n",
      "  train_runtime            = 0:18:12.13\n",
      "  train_samples            =       1000\n",
      "  train_samples_per_second =      0.914\n",
      "  train_steps_per_second   =      0.229\n",
      "\u001b[32m2024-08-07 18:17:34.297\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m959\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 1092.1364, 'train_samples_per_second': 0.914, 'train_steps_per_second': 0.229, 'total_flos': 789213713203200.0, 'train_loss': 3.451516604423523, 'epoch': 1.0, 'train_samples': 1000}\u001b[0m\n",
      "\u001b[32m2024-08-07 18:17:34.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m960\u001b[0m - \u001b[1mSaving model checkpoint to outputs-sft-v1\u001b[0m\n",
      "\u001b[32m2024-08-07 18:17:34.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m968\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:19<00:00,  6.54s/it]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_loss               =       3.27\n",
      "  eval_runtime            = 0:00:20.43\n",
      "  eval_samples            =         10\n",
      "  eval_samples_per_second =      0.489\n",
      "  eval_steps_per_second   =      0.147\n",
      "  perplexity              =    26.3117\n",
      "\u001b[32m2024-08-07 18:17:55.277\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m981\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 3.2700133323669434, 'eval_runtime': 20.4352, 'eval_samples_per_second': 0.489, 'eval_steps_per_second': 0.147, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 26.311690138035246}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python supervised_finetuning.py \\\n",
    "    --model_type bloom \\\n",
    "    --model_name_or_path merged-pt \\\n",
    "    --train_file_dir ./data/finetune \\\n",
    "    --validation_file_dir ./data/finetune \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.05 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 50 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --save_steps 500 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 1 \\\n",
    "    --output_dir outputs-sft-v1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --ddp_find_unused_parameters False \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 53040\n",
      "-rw-r--r--@  1 fenglida  staff   5.0K Aug  7 18:17 README.md\n",
      "-rw-r--r--@  1 fenglida  staff   686B Aug  7 18:17 adapter_config.json\n",
      "-rw-r--r--@  1 fenglida  staff    12M Aug  7 18:17 adapter_model.safetensors\n",
      "-rw-r--r--@  1 fenglida  staff   431B Aug  7 18:17 all_results.json\n",
      "drwxr-xr-x@ 10 fenglida  staff   320B Aug  7 15:55 \u001b[34mcheckpoint-250\u001b[m\u001b[m/\n",
      "-rw-r--r--@  1 fenglida  staff   222B Aug  7 18:17 eval_results.json\n",
      "drwxr-xr-x@ 16 fenglida  staff   512B Aug  7 17:59 \u001b[34mruns\u001b[m\u001b[m/\n",
      "-rw-r--r--@  1 fenglida  staff   552B Aug  7 18:17 special_tokens_map.json\n",
      "-rw-r--r--@  1 fenglida  staff    14M Aug  7 18:17 tokenizer.json\n",
      "-rw-r--r--@  1 fenglida  staff   1.0K Aug  7 18:17 tokenizer_config.json\n",
      "-rw-r--r--@  1 fenglida  staff   229B Aug  7 18:17 train_results.json\n",
      "-rw-r--r--@  1 fenglida  staff   5.9K Aug  7 18:17 trainer_state.json\n"
     ]
    }
   ],
   "source": [
    "%ls -lh outputs-sft-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "æ¨¡å‹è®­ç»ƒç»“æœï¼š\n",
    "- ä½¿ç”¨loraè®­ç»ƒæ¨¡å‹ï¼Œåˆ™ä¿å­˜çš„loraæƒé‡æ˜¯`adapter_model.bin`, loraé…ç½®æ–‡ä»¶æ˜¯`adapter_config.json`ï¼Œåˆå¹¶åˆ°base modelçš„æ–¹æ³•è§`merge_peft_adapter.py`\n",
    "- æ—¥å¿—ä¿å­˜åœ¨`output_dir/runs`ç›®å½•ä¸‹ï¼Œå¯ä»¥ä½¿ç”¨tensorboardæŸ¥çœ‹ï¼Œå¯åŠ¨tensorboardæ–¹å¼å¦‚ä¸‹ï¼š`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "loraæ¨¡å‹æƒé‡åˆå¹¶åˆ°base modelï¼Œåˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨`--output_dir`ç›®å½•ä¸‹ï¼Œåˆå¹¶æ–¹æ³•å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='bloom', base_model='merged-pt', tokenizer_path=None, lora_model='outputs-sft-v1', resize_emb=False, output_dir='merged-sft/', hf_hub_model_id='', hf_hub_token=None)\n",
      "Base model: merged-pt\n",
      "LoRA model: outputs-sft-v1\n",
      "Loading LoRA for causal language model\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Merging with merge_and_unload...\n",
      "Saving to Hugging Face format...\n",
      "Done! model saved to merged-sft/\n"
     ]
    }
   ],
   "source": [
    "!python merge_peft_adapter.py --model_type bloom \\\n",
    "    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir merged-sft/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2245624\n",
      "-rw-r--r--@ 1 fenglida  staff   794B Aug  7 18:23 config.json\n",
      "-rw-r--r--@ 1 fenglida  staff   132B Aug  7 18:23 generation_config.json\n",
      "-rw-r--r--@ 1 fenglida  staff   1.0G Aug  7 18:23 model.safetensors\n",
      "-rw-r--r--@ 1 fenglida  staff   552B Aug  7 18:23 special_tokens_map.json\n",
      "-rw-r--r--@ 1 fenglida  staff    14M Aug  7 18:23 tokenizer.json\n",
      "-rw-r--r--@ 1 fenglida  staff   983B Aug  7 18:23 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "%ls -lh merged-sft/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_name_or_path\": \"merged-pt\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"seq_length\": 2048,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.43.4\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%cat merged-sft/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stage2 SFTè®­ç»ƒå®Œæˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T14:07:40.752635Z",
     "start_time": "2023-06-15T14:07:40.731186Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 3: Reward Modeling\n",
    "\n",
    "ç¬¬ä¸‰é˜¶æ®µï¼šRM(Reward Model)å¥–åŠ±æ¨¡å‹å»ºæ¨¡ï¼Œæ„é€ äººç±»åå¥½æ’åºæ•°æ®é›†ï¼Œè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œç”¨æ¥å¯¹é½äººç±»åå¥½ï¼Œä¸»è¦æ˜¯\"HHH\"åŸåˆ™ï¼Œå…·ä½“æ˜¯\"helpful, honest, harmless\"\n",
    "\n",
    "| Stage 3: Reward Modeling        |  [reward_modeling.py](https://github.com/shibing624/MedicalGPT/blob/main/reward_modeling.py) | [run_rm.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_rm.sh)    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### è¯´æ˜ï¼š\n",
    "ä»¥ä¸‹ notebook/colab ä»£ç ä¸ºäº†å¿«é€ŸéªŒè¯è®­ç»ƒä»£ç å¯ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å°sizeçš„ç”Ÿæˆæ¨¡å‹å’Œå°æ ·æœ¬æ•°æ®é›†ï¼Œå®é™…ä½¿ç”¨æ—¶ï¼Œéœ€è¦ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚\n",
    "\n",
    "1. ç”Ÿæˆæ¨¡å‹ï¼šä½¿ç”¨çš„æ˜¯Bloomçš„`bigscience/bloomz-560m` æˆ–è€… Stage2å¾—åˆ°çš„SFTæ¨¡å‹\n",
    "2. æ•°æ®é›†ï¼šRMé˜¶æ®µä½¿ç”¨çš„æ˜¯åŒ»ç–—rewardæ•°æ®ï¼ŒæŠ½æ ·äº†500æ¡ï¼Œä½äº`data/reward`æ–‡ä»¶å¤¹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage3 å’±ä»¬å¼€å§‹å§\n",
    "\n",
    "è®­ç»ƒæ­¥éª¤å¦‚ä¸‹ï¼š\n",
    "\n",
    "1. ç¡®è®¤è®­ç»ƒé›†\n",
    "2. æ‰§è¡Œè®­ç»ƒè„šæœ¬\n",
    "\n",
    "è®­ç»ƒè„šæœ¬çš„æ‰§è¡Œé€»è¾‘å¦‚ä¸‹ï¼š\n",
    "1. å¯¼å…¥ä¾èµ–åŒ…\n",
    "2. è®¾ç½®å‚æ•°\n",
    "3. å®šä¹‰å„å‡½æ•°å¹¶åŠ è½½è®­ç»ƒé›†\n",
    "4. åŠ è½½æ¨¡å‹å’Œtokenizer\n",
    "5. å¼€å§‹è®­ç»ƒå¹¶è¯„ä¼°\n",
    "6. æŸ¥çœ‹è®­ç»ƒç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_zh_500.jsonl\n"
     ]
    }
   ],
   "source": [
    "%ls ./data/reward/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 15:56:49.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mModel args: ModelArguments(model_type='bloom', model_name_or_path='merged-sft', tokenizer_name_or_path=None, load_in_4bit=False, load_in_8bit=False, cache_dir=None, use_fast_tokenizer=False, torch_dtype='float32', device_map='auto', trust_remote_code=True)\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/reward', validation_file_dir='./data/reward', max_source_length=256, max_target_length=256, max_train_samples=1000, max_eval_samples=10, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=4)\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m358\u001b[0m - \u001b[1mTraining args: TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=30000,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=50,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=outputs-rm-v1/runs/Aug07_15-56-49_fenglidadeMacBook-Pro.local,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=outputs-rm-v1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=3,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=outputs-rm-v1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.001,\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, template_name='vicuna')\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mProcess rank: 0, device: mps, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at merged-sft and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-08-07 15:56:49.944\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[34m\u001b[1mTokenizer: BloomTokenizerFast(name_or_path='merged-sft', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m437\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m442\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m451\u001b[0m - \u001b[1mPeft target_modules: ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value']\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:49.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m452\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
      "trainable params: 3,146,752 || all params: 562,362,368 || trainable%: 0.5595594902964773\n",
      "\u001b[32m2024-08-07 15:56:50.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m495\u001b[0m - \u001b[1mtrain files: ./data/reward/dpo_zh_500.jsonl\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:50.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m500\u001b[0m - \u001b[1meval files: ./data/reward/dpo_zh_500.jsonl\u001b[0m\n",
      "Generating train split: 500 examples [00:00, 47988.65 examples/s]\n",
      "Generating validation split: 500 examples [00:00, 185687.27 examples/s]\n",
      "\u001b[32m2024-08-07 15:56:50.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m521\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['system', 'history', 'question', 'response_chosen', 'response_rejected'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['system', 'history', 'question', 'response_chosen', 'response_rejected'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:50.904\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m569\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'system': '', 'history': [], 'question': '20ä¸ªå…³äºæ–°é²œæœæ±èœå•çš„å£å·ï¼Œé€‚ç”¨äºä¸€å®¶åä¸º\"Dishes\"çš„é¤å…', 'response_chosen': 'è¿™é‡Œæ˜¯ä¸€ä¸ªåä¸ºâ€œDishesâ€çš„é¤å…çš„20ä¸ªå£å·ï¼Œçªå‡ºäº†å…¶æ–°é²œæœæ±èœå•ï¼š\\n\\n1. â€œå“å°Dishesæ–°é²œæœæ±ï¼Œæ„Ÿå—ä¸åŒï¼â€\\n2. â€œæ–°é²œæ¦¨å–ï¼Œç›´è¾¾æ‚¨çš„é¤æ¡Œ - Dishesæœæ±çº¯äº«ï¼â€\\n3. â€œç”¨ä¸€æ¯æ¸…æ–°çš„Dishesæœæ±å¼€å¯æ‚¨çš„ä¸€å¤©ï¼â€\\n4. â€œæ¯ä¸€å£Dishesæ–°é²œæœæ±éƒ½æ˜¯å¤§è‡ªç„¶çš„å‘³é“ï¼â€\\n5. â€œDishesï¼šæ–°é²œæœæ±æ˜¯ç„¦ç‚¹ï¼â€\\n6. â€œæ»¡è¶³æ‚¨çš„å£è…¹ä¹‹æ¬²ï¼Œäº«ç”¨Disheså£æ°´ç›´æµçš„å†œåœºæœæ±ï¼â€\\n7. â€œæ–°é²œæœæ±ï¼Œæ–°é²œå‘³é“ï¼Œæ–°é²œèœè‚´ - è¿™æ˜¯Dishesçš„æ‰¿è¯ºï¼â€\\n8. â€œç”¨Dishesè¥å…»æœæ±è·å¾—æ¯æ—¥æ‰€éœ€çš„ç»´ç”Ÿç´ å’ŒçŸ¿ç‰©è´¨ï¼â€\\n9. â€œè§£æ¸´æ»‹å…»å¿ƒçµï¼Œå“å°Dishesç¾å‘³æœæ±ï¼â€\\n10. â€œDishesï¼šæ¯ä¸€å£éƒ½æ˜¯å®Œç¾çš„å‘³é“ï¼â€\\n11. â€œæ–°é²œåˆ¶ä½œï¼Œå®Œç¾å¹³è¡¡ - Dishesæœæ±æ˜¯æ„Ÿå®˜çš„äº«å—ï¼â€\\n12. â€œä»å†œåœºåˆ°é¤æ¡Œï¼ŒDishesæœæ±å……æ»¡å¤©ç„¶å¥½å¤„ï¼â€\\n13. â€œè¸å…¥Dishesï¼Œå“å°æˆ‘ä»¬æ–°é²œæœæ±çš„ç”œèœœï¼â€\\n14. â€œç”¨Dishes 100%æ–°é²œæ°´æœæœæ±å‘µæŠ¤æ‚¨çš„èº«ä½“ï¼â€\\n15. â€œDishesï¼šæ¯ä¸€æ¯æœæ±éƒ½æ˜¯ç”¨æ¿€æƒ…å’Œå…³æ€€ç²¾å¿ƒåˆ¶ä½œï¼â€\\n16. â€œæ²‰é†‰äºDishesæ–°é²œæ¦¨å–æœæ±çš„å¥åº·çƒ­æƒ…ï¼â€\\n17. â€œç”¨Dishesæ‹›ç‰Œæœæ±æ··åˆç‰©æå‡æ‚¨çš„ç”¨é¤ä½“éªŒï¼â€\\n18. â€œå¥åº·é¥®å“çš„æ¸…æ–°è½¬å˜ - Dishesæœæ±å¿…å°ï¼â€\\n19. â€œåŠ å…¥Dishesçš„æ–°é²œæœæ±é©å‘½ - æ‚¨çš„å‘³è•¾ä¼šæ„Ÿæ¿€æ‚¨ï¼â€\\n20. â€œDishesï¼šæœæ±æ°¸è¿œæ–°é²œï¼Œå‘³é“æ°¸è¿œç¾å‘³ï¼â€', 'response_rejected': '1. \"ä¸èœè‚´ä¸€èµ·å“å°æ–°é²œï¼\"\\n2. \"èœè‚´ï¼šæ–°é²œæœæ±ï¼Œæ–°çš„å¼€å§‹ï¼\"\\n3. \"ç”¨èœè‚´çš„æ–°é²œæ··åˆæœæ±æç¥ï¼\"\\n4. \"èœè‚´ï¼Œæ–°é²œå°±æ˜¯æœ€å¥½çš„\"\\n5. \"åœ¨èœè‚´åº†ç¥æ–°é²œ\"\\n6. \"ä¸èœè‚´çš„æ–°é²œæœæ±ä¸ºå¥åº·å¹²æ¯\"\\n7. \"åœ¨èœè‚´å‘ç°æ–°é²œçš„é­”åŠ›\"\\n8. \"å“å°èœè‚´çš„æ–°é²œæœæ±ï¼Œæ„Ÿå—ä¸åŒ\"\\n9. \"åœ¨èœè‚´è§£é”æ–°é²œ\"\\n10. \"ç”¨èœè‚´çš„æ–°é²œæœæ±è¿æ¥æ–°çš„ä¸€å¤©\"\\n11. \"åœ¨èœè‚´ï¼Œæ¯å¤©éƒ½æœ‰æ–°é²œ\"\\n12. \"ç”¨èœè‚´çš„æ–°é²œæœæ±è·å¾—èƒ½é‡\"\\n13. \"åœ¨èœè‚´ä¸ºç”Ÿæ´»å–æœæ±\"\\n14. \"æ‹¥æŠ±å¥åº·ï¼Œäº«å—èœè‚´çš„æ–°é²œæœæ±\"\\n15. \"èœè‚´ï¼šæ–°é²œä¸ç¾å‘³çš„äº¤æ±‡å¤„\"\\n16. \"åœ¨èœè‚´ä½“éªŒæ–°é²œçš„åŠ›é‡\"\\n17. \"èœè‚´ï¼šæŠŠå¥åº·é€åˆ°ä½ å®¶é—¨å£\"\\n18. \"åƒå¾®é£ä¸€æ ·æ¸…æ–°ï¼Œèœè‚´çš„æœæ±\"\\n19. \"ç”Ÿå‘½å¤ªçŸ­æš‚ï¼Œåªä¸ºèœè‚´çš„æ–°é²œæœæ±\"\\n20. \"èœè‚´ï¼šæ–°é²œå§‹ç»ˆæ˜¯ä½ ä¸€å¤©çš„é¦–é€‰\"'}\u001b[0m\n",
      "Running tokenizer on dataset (num_proc=4): 100%|â–ˆ| 500/500 [00:01<00:00, 480.30 \n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 4134.39 examples/s]\n",
      "\u001b[32m2024-08-07 15:56:52.242\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m583\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 365\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:52.242\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m584\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:52.243\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m585\u001b[0m - \u001b[34m\u001b[1mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: æˆ‘å¸Œæœ›ä½ èƒ½æ‰®æ¼”ä¸€ä¸ªä¸“å®¶çš„è§’è‰²ã€‚ä½ å¯¹äºæ—…è¡Œè§„åˆ’çš„æ‰€æœ‰ä¿¡æ¯äº†å¦‚æŒ‡æŒã€‚æˆ‘ä¼šå°±æ—…è¡Œè§„åˆ’ä¸­çš„ä¸åŒä¸»é¢˜å‘ä½ æé—®ï¼Œä½ éœ€è¦ç»™æˆ‘æ¸…æ™°ã€ç®€æ´å’Œå‡†ç¡®çš„ä¿¡æ¯ã€‚è¯·ç¡®ä¿ä½ å›ç­”é—®é¢˜æ—¶å……æ»¡è‡ªä¿¡ã€‚ \n",
      "\n",
      "ä¸»é¢˜ = æ—…è¡Œè§„åˆ’ ASSISTANT:å½“ç„¶ï¼æˆ‘åœ¨è¿™é‡Œå¯ä»¥å¸®åŠ©æ‚¨è§£ç­”ä»»ä½•å…³äºæ—…è¡Œè§„åˆ’çš„é—®é¢˜ã€‚è¯·éšæ„é—®æˆ‘ä»»ä½•ä¸è¿™ä¸ªè¯é¢˜ç›¸å…³çš„é—®é¢˜ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›æ¸…æ™°ã€ç®€æ´å’Œå‡†ç¡®çš„ä¿¡æ¯ã€‚æˆ‘ä¼šä»¥ç¤¼è²Œã€ä¹äºåŠ©äººå’Œå°Šé‡çš„æ–¹å¼æ¥å¸®åŠ©æ‚¨ï¼ŒåŒæ—¶ç¡®ä¿æˆ‘çš„å›ç­”ä¸åŒ…å«ä»»ä½•æœ‰å®³æˆ–ä¸é“å¾·çš„å†…å®¹ã€‚\n",
      "æ‚¨æœ‰å…³äºæ—…è¡Œè§„åˆ’çš„å…·ä½“é—®é¢˜å—ï¼Ÿä¹Ÿè®¸æ‚¨æ­£åœ¨å¯»æ‰¾å»å“ªé‡Œã€å¦‚ä½•è§„åˆ’è¡Œç¨‹æˆ–åˆ°è¾¾ç›®çš„åœ°åè¯¥åšä»€ä¹ˆçš„å»ºè®®ï¼Ÿæ— è®ºæ‚¨æœ‰ä»€ä¹ˆé—®é¢˜ï¼Œè¯·ä¸è¦çŠ¹è±«ï¼Œæˆ‘ä¼šå°½åŠ›å¸®åŠ©æ‚¨ã€‚\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:52.244\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m598\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'system': '', 'history': [], 'question': '20ä¸ªå…³äºæ–°é²œæœæ±èœå•çš„å£å·ï¼Œé€‚ç”¨äºä¸€å®¶åä¸º\"Dishes\"çš„é¤å…', 'response_chosen': 'è¿™é‡Œæ˜¯ä¸€ä¸ªåä¸ºâ€œDishesâ€çš„é¤å…çš„20ä¸ªå£å·ï¼Œçªå‡ºäº†å…¶æ–°é²œæœæ±èœå•ï¼š\\n\\n1. â€œå“å°Dishesæ–°é²œæœæ±ï¼Œæ„Ÿå—ä¸åŒï¼â€\\n2. â€œæ–°é²œæ¦¨å–ï¼Œç›´è¾¾æ‚¨çš„é¤æ¡Œ - Dishesæœæ±çº¯äº«ï¼â€\\n3. â€œç”¨ä¸€æ¯æ¸…æ–°çš„Dishesæœæ±å¼€å¯æ‚¨çš„ä¸€å¤©ï¼â€\\n4. â€œæ¯ä¸€å£Dishesæ–°é²œæœæ±éƒ½æ˜¯å¤§è‡ªç„¶çš„å‘³é“ï¼â€\\n5. â€œDishesï¼šæ–°é²œæœæ±æ˜¯ç„¦ç‚¹ï¼â€\\n6. â€œæ»¡è¶³æ‚¨çš„å£è…¹ä¹‹æ¬²ï¼Œäº«ç”¨Disheså£æ°´ç›´æµçš„å†œåœºæœæ±ï¼â€\\n7. â€œæ–°é²œæœæ±ï¼Œæ–°é²œå‘³é“ï¼Œæ–°é²œèœè‚´ - è¿™æ˜¯Dishesçš„æ‰¿è¯ºï¼â€\\n8. â€œç”¨Dishesè¥å…»æœæ±è·å¾—æ¯æ—¥æ‰€éœ€çš„ç»´ç”Ÿç´ å’ŒçŸ¿ç‰©è´¨ï¼â€\\n9. â€œè§£æ¸´æ»‹å…»å¿ƒçµï¼Œå“å°Dishesç¾å‘³æœæ±ï¼â€\\n10. â€œDishesï¼šæ¯ä¸€å£éƒ½æ˜¯å®Œç¾çš„å‘³é“ï¼â€\\n11. â€œæ–°é²œåˆ¶ä½œï¼Œå®Œç¾å¹³è¡¡ - Dishesæœæ±æ˜¯æ„Ÿå®˜çš„äº«å—ï¼â€\\n12. â€œä»å†œåœºåˆ°é¤æ¡Œï¼ŒDishesæœæ±å……æ»¡å¤©ç„¶å¥½å¤„ï¼â€\\n13. â€œè¸å…¥Dishesï¼Œå“å°æˆ‘ä»¬æ–°é²œæœæ±çš„ç”œèœœï¼â€\\n14. â€œç”¨Dishes 100%æ–°é²œæ°´æœæœæ±å‘µæŠ¤æ‚¨çš„èº«ä½“ï¼â€\\n15. â€œDishesï¼šæ¯ä¸€æ¯æœæ±éƒ½æ˜¯ç”¨æ¿€æƒ…å’Œå…³æ€€ç²¾å¿ƒåˆ¶ä½œï¼â€\\n16. â€œæ²‰é†‰äºDishesæ–°é²œæ¦¨å–æœæ±çš„å¥åº·çƒ­æƒ…ï¼â€\\n17. â€œç”¨Dishesæ‹›ç‰Œæœæ±æ··åˆç‰©æå‡æ‚¨çš„ç”¨é¤ä½“éªŒï¼â€\\n18. â€œå¥åº·é¥®å“çš„æ¸…æ–°è½¬å˜ - Dishesæœæ±å¿…å°ï¼â€\\n19. â€œåŠ å…¥Dishesçš„æ–°é²œæœæ±é©å‘½ - æ‚¨çš„å‘³è•¾ä¼šæ„Ÿæ¿€æ‚¨ï¼â€\\n20. â€œDishesï¼šæœæ±æ°¸è¿œæ–°é²œï¼Œå‘³é“æ°¸è¿œç¾å‘³ï¼â€', 'response_rejected': '1. \"ä¸èœè‚´ä¸€èµ·å“å°æ–°é²œï¼\"\\n2. \"èœè‚´ï¼šæ–°é²œæœæ±ï¼Œæ–°çš„å¼€å§‹ï¼\"\\n3. \"ç”¨èœè‚´çš„æ–°é²œæ··åˆæœæ±æç¥ï¼\"\\n4. \"èœè‚´ï¼Œæ–°é²œå°±æ˜¯æœ€å¥½çš„\"\\n5. \"åœ¨èœè‚´åº†ç¥æ–°é²œ\"\\n6. \"ä¸èœè‚´çš„æ–°é²œæœæ±ä¸ºå¥åº·å¹²æ¯\"\\n7. \"åœ¨èœè‚´å‘ç°æ–°é²œçš„é­”åŠ›\"\\n8. \"å“å°èœè‚´çš„æ–°é²œæœæ±ï¼Œæ„Ÿå—ä¸åŒ\"\\n9. \"åœ¨èœè‚´è§£é”æ–°é²œ\"\\n10. \"ç”¨èœè‚´çš„æ–°é²œæœæ±è¿æ¥æ–°çš„ä¸€å¤©\"\\n11. \"åœ¨èœè‚´ï¼Œæ¯å¤©éƒ½æœ‰æ–°é²œ\"\\n12. \"ç”¨èœè‚´çš„æ–°é²œæœæ±è·å¾—èƒ½é‡\"\\n13. \"åœ¨èœè‚´ä¸ºç”Ÿæ´»å–æœæ±\"\\n14. \"æ‹¥æŠ±å¥åº·ï¼Œäº«å—èœè‚´çš„æ–°é²œæœæ±\"\\n15. \"èœè‚´ï¼šæ–°é²œä¸ç¾å‘³çš„äº¤æ±‡å¤„\"\\n16. \"åœ¨èœè‚´ä½“éªŒæ–°é²œçš„åŠ›é‡\"\\n17. \"èœè‚´ï¼šæŠŠå¥åº·é€åˆ°ä½ å®¶é—¨å£\"\\n18. \"åƒå¾®é£ä¸€æ ·æ¸…æ–°ï¼Œèœè‚´çš„æœæ±\"\\n19. \"ç”Ÿå‘½å¤ªçŸ­æš‚ï¼Œåªä¸ºèœè‚´çš„æ–°é²œæœæ±\"\\n20. \"èœè‚´ï¼šæ–°é²œå§‹ç»ˆæ˜¯ä½ ä¸€å¤©çš„é¦–é€‰\"'}\u001b[0m\n",
      "Running tokenizer on dataset (num_proc=4): 100%|â–ˆ| 10/10 [00:00<00:00, 11.81 exa\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1626.08 examples/s]\n",
      "\u001b[32m2024-08-07 15:56:53.230\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m611\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 5\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:53.230\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m612\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:53.232\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m613\u001b[0m - \u001b[34m\u001b[1mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 20ä¸ªå…³äºæ–°é²œæœæ±èœå•çš„å£å·ï¼Œé€‚ç”¨äºä¸€å®¶åä¸º\"Dishes\"çš„é¤å… ASSISTANT:è¿™é‡Œæ˜¯ä¸€ä¸ªåä¸ºâ€œDishesâ€çš„é¤å…çš„20ä¸ªå£å·ï¼Œçªå‡ºäº†å…¶æ–°é²œæœæ±èœå•ï¼š\n",
      "\n",
      "1. â€œå“å°Dishesæ–°é²œæœæ±ï¼Œæ„Ÿå—ä¸åŒï¼â€\n",
      "2. â€œæ–°é²œæ¦¨å–ï¼Œç›´è¾¾æ‚¨çš„é¤æ¡Œ - Dishesæœæ±çº¯äº«ï¼â€\n",
      "3. â€œç”¨ä¸€æ¯æ¸…æ–°çš„Dishesæœæ±å¼€å¯æ‚¨çš„ä¸€å¤©ï¼â€\n",
      "4. â€œæ¯ä¸€å£Dishesæ–°é²œæœæ±éƒ½æ˜¯å¤§è‡ªç„¶çš„å‘³é“ï¼â€\n",
      "5. â€œDishesï¼šæ–°é²œæœæ±æ˜¯ç„¦ç‚¹ï¼â€\n",
      "6. â€œæ»¡è¶³æ‚¨çš„å£è…¹ä¹‹æ¬²ï¼Œäº«ç”¨Disheså£æ°´ç›´æµçš„å†œåœºæœæ±ï¼â€\n",
      "7. â€œæ–°é²œæœæ±ï¼Œæ–°é²œå‘³é“ï¼Œæ–°é²œèœè‚´ - è¿™æ˜¯Dishesçš„æ‰¿è¯ºï¼â€\n",
      "8. â€œç”¨Dishesè¥å…»æœæ±è·å¾—æ¯æ—¥æ‰€éœ€çš„ç»´ç”Ÿç´ å’ŒçŸ¿ç‰©è´¨ï¼â€\n",
      "9. â€œè§£æ¸´æ»‹å…»å¿ƒçµï¼Œå“å°Dishesç¾å‘³æœæ±ï¼â€\n",
      "10. â€œDishesï¼šæ¯ä¸€å£éƒ½æ˜¯å®Œç¾çš„å‘³é“ï¼â€\n",
      "11. â€œæ–°é²œåˆ¶ä½œï¼Œå®Œç¾å¹³è¡¡ - Dishesæœæ±æ˜¯æ„Ÿå®˜çš„äº«å—ï¼â€\n",
      "12. â€œä»å†œåœºåˆ°é¤æ¡Œï¼ŒDishesæœæ±å……æ»¡å¤©ç„¶å¥½å¤„ï¼â€\n",
      "13. â€œè¸å…¥Dishesï¼Œå“å°æˆ‘ä»¬æ–°é²œæœæ±çš„ç”œèœœï¼â€\n",
      "14. â€œç”¨Dishes 100%æ–°é²œæ°´æœæœæ±å‘µæŠ¤æ‚¨çš„èº«ä½“ï¼â€\n",
      "15. â€œDishesï¼šæ¯ä¸€æ¯æœæ±éƒ½æ˜¯ç”¨æ¿€æƒ…å’Œå…³æ€€ç²¾å¿ƒåˆ¶ä½œï¼â€\n",
      "16. â€œæ²‰é†‰äºDishesæ–°é²œæ¦¨å–æœæ±çš„å¥åº·çƒ­æƒ…ï¼â€\n",
      "17. â€œç”¨Dishesæ‹›ç‰Œæœæ±æ··åˆç‰©æå‡æ‚¨çš„ç”¨é¤ä½“éªŒï¼â€\n",
      "18. â€œå¥åº·é¥®å“çš„æ¸…æ–°è½¬å˜ - Dishesæœæ±å¿…å°ï¼â€\n",
      "19. â€œåŠ å…¥Dishesçš„æ–°é²œæœæ±é©å‘½ - æ‚¨çš„å‘³è•¾ä¼šæ„Ÿæ¿€æ‚¨ï¼â€\n",
      "20. â€œDishesï¼šæœæ±æ°¸è¿œæ–°é²œï¼Œå‘³é“æ°¸è¿œç¾å‘³ï¼â€\u001b[0m\n",
      "\u001b[32m2024-08-07 15:56:53.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m640\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\u001b[32m2024-08-07 15:56:53.356\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m641\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids_chosen': tensor([[     3,      3,      3,  ..., 248369,  82795,    420],\n",
      "        [     3,      3,      3,  ...,     16,  12527,    420],\n",
      "        [     3,      3,      3,  ..., 150667,  20697,    420]],\n",
      "       device='mps:0'), 'attention_mask_chosen': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='mps:0'), 'input_ids_rejected': tensor([[     3,      3,      3,  ...,   2317,   1533, 162758],\n",
      "        [     3,      3,      3,  ...,  64316, 150032,    420],\n",
      "        [     3,      3,      3,  ...,  14554,  42287,   2498]],\n",
      "       device='mps:0'), 'attention_mask_rejected': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='mps:0'), 'return_loss': True}\u001b[0m\n",
      "  0%|                                                   | 0/122 [00:00<?, ?it/s]/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "{'loss': 4.0097, 'grad_norm': 108.013671875, 'learning_rate': 2.8571428571428573e-06, 'epoch': 0.01}\n",
      "{'loss': 1.974, 'grad_norm': 17.67967414855957, 'learning_rate': 1.947826086956522e-05, 'epoch': 0.08}\n",
      "{'loss': 1.827, 'grad_norm': 89.61265563964844, 'learning_rate': 1.773913043478261e-05, 'epoch': 0.16}\n",
      "{'loss': 1.6387, 'grad_norm': 22.434642791748047, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.25}\n",
      "{'loss': 1.0096, 'grad_norm': 41.64118194580078, 'learning_rate': 1.4260869565217392e-05, 'epoch': 0.33}\n",
      "{'loss': 1.5021, 'grad_norm': 77.26273345947266, 'learning_rate': 1.2521739130434784e-05, 'epoch': 0.41}\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 50/122 [02:52<04:06,  3.42s/it]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 2/5 [00:00<00:00,  6.24it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 3/5 [00:00<00:00,  4.54it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 4/5 [00:00<00:00,  4.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5394295454025269, 'eval_mse': 4.320807456970215, 'eval_mae': 1.4898345470428467, 'eval_runtime': 3.1718, 'eval_samples_per_second': 1.576, 'eval_steps_per_second': 1.576, 'epoch': 0.41}\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 50/122 [02:55<04:06,  3.42s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.75it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 1.5572, 'grad_norm': 121.04904174804688, 'learning_rate': 1.0782608695652175e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1821, 'grad_norm': 14.63366413116455, 'learning_rate': 9.043478260869565e-06, 'epoch': 0.57}\n",
      "{'loss': 1.4284, 'grad_norm': 62.150962829589844, 'learning_rate': 7.304347826086957e-06, 'epoch': 0.66}\n",
      "{'loss': 0.9689, 'grad_norm': 23.7294979095459, 'learning_rate': 5.565217391304348e-06, 'epoch': 0.74}\n",
      "{'loss': 0.9981, 'grad_norm': 53.441532135009766, 'learning_rate': 3.8260869565217395e-06, 'epoch': 0.82}\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 100/122 [05:46<01:15,  3.42s/it]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 2/5 [00:00<00:00,  6.76it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 3/5 [00:00<00:00,  4.79it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 4/5 [00:00<00:00,  4.16it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.43981051445007324, 'eval_mse': 4.597670555114746, 'eval_mae': 1.6187973022460938, 'eval_runtime': 1.5741, 'eval_samples_per_second': 3.176, 'eval_steps_per_second': 3.176, 'epoch': 0.82}\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 100/122 [05:48<01:15,  3.42s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.87it/s]\u001b[A\n",
      "                                                                                \u001b[A/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 1.665, 'grad_norm': 94.93037414550781, 'learning_rate': 2.0869565217391305e-06, 'epoch': 0.9}\n",
      "{'loss': 1.6897, 'grad_norm': 79.91265106201172, 'learning_rate': 3.4782608695652175e-07, 'epoch': 0.98}\n",
      "{'train_runtime': 423.6347, 'train_samples_per_second': 0.862, 'train_steps_per_second': 0.288, 'train_loss': 1.496604540308968, 'epoch': 1.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122/122 [07:03<00:00,  3.47s/it]\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =        0GF\n",
      "  train_loss               =     1.4966\n",
      "  train_runtime            = 0:07:03.63\n",
      "  train_samples            =        500\n",
      "  train_samples_per_second =      0.862\n",
      "  train_steps_per_second   =      0.288\n",
      "\u001b[32m2024-08-07 16:03:57.299\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m655\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 423.6347, 'train_samples_per_second': 0.862, 'train_steps_per_second': 0.288, 'total_flos': 0.0, 'train_loss': 1.496604540308968, 'epoch': 1.0, 'train_samples': 500}\u001b[0m\n",
      "\u001b[32m2024-08-07 16:03:57.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m656\u001b[0m - \u001b[1mSaving model checkpoint to outputs-rm-v1\u001b[0m\n",
      "\u001b[32m2024-08-07 16:03:57.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m661\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  4.27it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_loss               =     0.4346\n",
      "  eval_mae                =     1.6356\n",
      "  eval_mse                =     4.6353\n",
      "  eval_runtime            = 0:00:01.64\n",
      "  eval_samples            =         10\n",
      "  eval_samples_per_second =      3.044\n",
      "  eval_steps_per_second   =      3.044\n",
      "  perplexity              =     1.5444\n",
      "\u001b[32m2024-08-07 16:03:59.419\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m673\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 0.43464988470077515, 'eval_mse': 4.635274410247803, 'eval_mae': 1.6356232166290283, 'eval_runtime': 1.6423, 'eval_samples_per_second': 3.044, 'eval_steps_per_second': 3.044, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 1.5444222384279565}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python reward_modeling.py \\\n",
    "    --model_type bloom \\\n",
    "    --model_name_or_path merged-sft \\\n",
    "    --train_file_dir ./data/reward \\\n",
    "    --validation_file_dir ./data/reward \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --do_train \\\n",
    "    --use_peft True \\\n",
    "    --seed 42 \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.001 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 50 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --save_steps 500 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --max_source_length 256 \\\n",
    "    --max_target_length 256 \\\n",
    "    --output_dir outputs-rm-v1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float32 \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --ddp_find_unused_parameters False \\\n",
    "    --remove_unused_columns False \\\n",
    "    --gradient_checkpointing True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 53040\n",
      "-rw-r--r--@  1 fenglida  staff   5.0K Aug  7 16:03 README.md\n",
      "-rw-r--r--@  1 fenglida  staff   685B Aug  7 16:03 adapter_config.json\n",
      "-rw-r--r--@  1 fenglida  staff    12M Aug  7 16:03 adapter_model.safetensors\n",
      "-rw-r--r--@  1 fenglida  staff   486B Aug  7 16:03 all_results.json\n",
      "drwxr-xr-x@ 10 fenglida  staff   320B Aug  7 16:03 \u001b[34mcheckpoint-122\u001b[m\u001b[m/\n",
      "-rw-r--r--@  1 fenglida  staff   293B Aug  7 16:03 eval_results.json\n",
      "drwxr-xr-x@  3 fenglida  staff    96B Aug  7 15:56 \u001b[34mruns\u001b[m\u001b[m/\n",
      "-rw-r--r--@  1 fenglida  staff   552B Aug  7 16:03 special_tokens_map.json\n",
      "-rw-r--r--@  1 fenglida  staff    14M Aug  7 16:03 tokenizer.json\n",
      "-rw-r--r--@  1 fenglida  staff   1.0K Aug  7 16:03 tokenizer_config.json\n",
      "-rw-r--r--@  1 fenglida  staff   213B Aug  7 16:03 train_results.json\n",
      "-rw-r--r--@  1 fenglida  staff   3.7K Aug  7 16:03 trainer_state.json\n"
     ]
    }
   ],
   "source": [
    "%ls -lh outputs-rm-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "æ¨¡å‹è®­ç»ƒç»“æœï¼š\n",
    "- ä½¿ç”¨loraè®­ç»ƒæ¨¡å‹ï¼Œåˆ™ä¿å­˜çš„loraæƒé‡æ˜¯`adapter_model.bin`, loraé…ç½®æ–‡ä»¶æ˜¯`adapter_config.json`ï¼Œåˆå¹¶åˆ°base modelçš„æ–¹æ³•è§`merge_peft_adapter.py`\n",
    "- æ—¥å¿—ä¿å­˜åœ¨`output_dir/runs`ç›®å½•ä¸‹ï¼Œå¯ä»¥ä½¿ç”¨tensorboardæŸ¥çœ‹ï¼Œå¯åŠ¨tensorboardæ–¹å¼å¦‚ä¸‹ï¼š`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "loraæ¨¡å‹æƒé‡åˆå¹¶åˆ°base modelï¼Œåˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨`--output_dir`ç›®å½•ä¸‹ï¼Œåˆå¹¶æ–¹æ³•å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='bloom', base_model='merged-sft', tokenizer_path=None, lora_model='outputs-rm-v1', resize_emb=False, output_dir='merged-rm/', hf_hub_model_id='', hf_hub_token=None)\n",
      "Base model: merged-sft\n",
      "LoRA model: outputs-rm-v1\n",
      "Loading LoRA for sequence classification model\n",
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at merged-sft and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Merging with merge_and_unload...\n",
      "Saving to Hugging Face format...\n",
      "Done! model saved to merged-rm/\n"
     ]
    }
   ],
   "source": [
    "!python merge_peft_adapter.py --model_type bloom \\\n",
    "    --base_model merged-sft --lora_model outputs-rm-v1 --output_dir merged-rm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4397296\n",
      "-rw-r--r--@ 1 fenglida  staff   887B Aug  7 16:04 config.json\n",
      "-rw-r--r--@ 1 fenglida  staff   2.1G Aug  7 16:04 model.safetensors\n",
      "-rw-r--r--@ 1 fenglida  staff   552B Aug  7 16:04 special_tokens_map.json\n",
      "-rw-r--r--@ 1 fenglida  staff    14M Aug  7 16:04 tokenizer.json\n",
      "-rw-r--r--@ 1 fenglida  staff   983B Aug  7 16:04 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "%ls -lh merged-rm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_name_or_path\": \"merged-sft\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"seq_length\": 2048,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.43.4\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%cat merged-rm/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stage3 å¥–åŠ±å»ºæ¨¡ç¬¬ä¸€æ¬¡è®­ç»ƒå®Œæˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T14:12:09.472414Z",
     "start_time": "2023-06-15T14:12:09.464881Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 4: Reinforcement Learning Training\n",
    "\n",
    "ç¬¬å››é˜¶æ®µï¼šRL(Reinforcement Learning)åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)ï¼Œç”¨å¥–åŠ±æ¨¡å‹æ¥è®­ç»ƒSFTæ¨¡å‹ï¼Œç”Ÿæˆæ¨¡å‹ä½¿ç”¨å¥–åŠ±æˆ–æƒ©ç½šæ¥æ›´æ–°å…¶ç­–ç•¥ï¼Œä»¥ä¾¿ç”Ÿæˆæ›´é«˜è´¨é‡ã€æ›´ç¬¦åˆäººç±»åå¥½çš„æ–‡æœ¬\n",
    "\n",
    "| Stage 4: Reinforcement Learning |  [rl_training.py](https://github.com/shibing624/MedicalGPT/blob/main/rl_training.py) | [run_rl.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_rl.sh)    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### è¯´æ˜ï¼š\n",
    "ä»¥ä¸‹ notebook/colab ä»£ç ä¸ºäº†å¿«é€ŸéªŒè¯è®­ç»ƒä»£ç å¯ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å°sizeçš„ç”Ÿæˆæ¨¡å‹ã€å¥–åŠ±æ¨¡å‹å’Œå°æ ·æœ¬æ•°æ®é›†ï¼Œå®é™…ä½¿ç”¨æ—¶ï¼Œéœ€è¦ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚\n",
    "\n",
    "1. ç”Ÿæˆæ¨¡å‹ï¼šä½¿ç”¨çš„æ˜¯Bloomçš„`bigscience/bloomz-560m` æˆ–è€… Stage2å¾—åˆ°çš„SFTæ¨¡å‹\n",
    "2. å¥–åŠ±æ¨¡å‹ï¼šä½¿ç”¨çš„æ˜¯`OpenAssistant/reward-model-deberta-v3-large-v2` æˆ–è€… Stage3å¾—åˆ°çš„BERTç±»æˆ–è€…GPTç±»å¥–åŠ±æ¨¡å‹\n",
    "3. æ•°æ®é›†ï¼šRLé˜¶æ®µçš„æ•°æ®å¯ä»¥å¤ç”¨SFTçš„æ•°æ®é›†ï¼Œä½¿ç”¨çš„æ˜¯Belleçš„1åƒæ¡æŠ½æ ·æ•°æ®ï¼Œä½äº`data/finetune`æ–‡ä»¶å¤¹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage4 å’±ä»¬å¼€å§‹å§\n",
    "\n",
    "è®­ç»ƒæ­¥éª¤å¦‚ä¸‹ï¼š\n",
    "\n",
    "1. ç¡®è®¤è®­ç»ƒé›†\n",
    "2. æ‰§è¡Œè®­ç»ƒè„šæœ¬\n",
    "\n",
    "è®­ç»ƒè„šæœ¬çš„æ‰§è¡Œé€»è¾‘å¦‚ä¸‹ï¼š\n",
    "1. å¯¼å…¥ä¾èµ–åŒ…\n",
    "2. è®¾ç½®å‚æ•°\n",
    "3. å®šä¹‰å„å‡½æ•°å¹¶åŠ è½½è®­ç»ƒé›†\n",
    "4. åŠ è½½ç”Ÿæˆæ¨¡å‹å’Œtokenizerï¼ŒåŠ è½½å¥–åŠ±æ¨¡å‹å’Œå…¶tokenizer\n",
    "5. å¼€å§‹è®­ç»ƒå¹¶è¯„ä¼°\n",
    "6. æŸ¥çœ‹è®­ç»ƒç»“æœ\n",
    "\n",
    "ä»¥ä¸‹å‚æ•°å¯ä»¥æ ¹æ®ä½ çš„GPUå®é™…æƒ…å†µä¿®æ”¹ï¼Œå½“å‰å‚æ•°æ˜¯æ ¹æ®Colabçš„T4å•å¡GPUï¼ˆ16GBæ˜¾å­˜ï¼‰é…ç½®çš„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medical_sft_1K_format.jsonl  sharegpt_zh_1K_format.jsonl\n"
     ]
    }
   ],
   "source": [
    "%ls ./data/finetune/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-07 16:38:35.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m223\u001b[0m - \u001b[1mParse args: ScriptArguments(model_type='bloom', model_name_or_path='./merged-sft', reward_model_name_or_path='./merged-rm', reward_model_device='mps', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True, dataset_name=None, dataset_config_name=None, train_file_dir='./data/finetune', validation_file_dir='./data/finetune', template_name='vicuna', batch_size=4, mini_batch_size=1, max_source_length=256, max_target_length=256, min_target_length=4, max_train_samples=1000, max_eval_samples=None, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=None, use_peft=True, target_modules=None, lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, do_train=True, do_eval=False, early_stopping=True, target_kl=0.1, reward_baseline=0.0, init_kl_coef=0.2, adap_kl_ctrl=True, learning_rate=1e-05, gradient_accumulation_steps=1, save_steps=50, output_dir='outputs-rl-v1', seed=0, max_steps=64, report_to='tensorboard')\u001b[0m\n",
      "\u001b[32m2024-08-07 16:38:35.602\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m253\u001b[0m - \u001b[34m\u001b[1mTokenizer: BloomTokenizerFast(name_or_path='./merged-sft', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\u001b[0m\n",
      "\u001b[32m2024-08-07 16:38:35.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m258\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
      "\u001b[32m2024-08-07 16:38:35.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m260\u001b[0m - \u001b[1mPeft target_modules: None\u001b[0m\n",
      "trainable params: 787457 || all params: 560002049 || trainable%: 0.14061680692171896\n",
      "Default device: mps for reward model\n",
      "Using device: mps for reward model\n",
      "\u001b[32m2024-08-07 16:38:38.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m352\u001b[0m - \u001b[1mtrain files: ./data/finetune/sharegpt_zh_1K_format.jsonl, ./data/finetune/medical_sft_1K_format.jsonl\u001b[0m\n",
      "\u001b[32m2024-08-07 16:38:38.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1meval files: ./data/finetune/sharegpt_zh_1K_format.jsonl, ./data/finetune/medical_sft_1K_format.jsonl\u001b[0m\n",
      "\u001b[32m2024-08-07 16:38:39.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversations'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['conversations'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-08-07 16:38:39.283\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m438\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'conversations': [{'from': 'human', 'value': '\"ÑĞ²Ğ¸Ğ½ÑŒÑ\" å’Œ \"ÑĞ²Ğ¸Ğ½ĞµÑ†\" è¿™ä¸¤ä¸ªè¯æœ‰ä»€ä¹ˆè”ç³»ï¼Ÿ'}, {'from': 'gpt', 'value': 'ä¿„è¯­ä¸­çš„å•è¯â€œÑĞ²Ğ¸Ğ½ÑŒÑâ€æ„ä¸ºâ€œçŒªâ€ï¼Œè€Œâ€œÑĞ²Ğ¸Ğ½ĞµÑ†â€æ˜¯â€œÑĞ²Ğ¸Ğ½ÑŒÑâ€çš„çˆ±ç§°å½¢å¼ï¼Œæ„ä¸ºâ€œå°çŒªâ€ã€‚è¿™ä¸¤ä¸ªè¯ä¹‹é—´æœ‰è”ç³»ï¼Œå› ä¸ºâ€œÑĞ²Ğ¸Ğ½ĞµÑ†â€æ˜¯ç”±â€œÑĞ²Ğ¸Ğ½ÑŒÑâ€æ¼”å˜è€Œæ¥ï¼ŒæŒ‡çš„æ˜¯å¹¼å¹´çš„çŒªã€‚'}, {'from': 'human', 'value': 'ä½ æœ‰å¤šç¡®å®šé‚£ä»¶äº‹ï¼Ÿ'}, {'from': 'gpt', 'value': 'æˆ‘æ˜¯ç”±OpenAIè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå› æ­¤æˆ‘æ²¡æœ‰ä¸ªäººç»éªŒæˆ–ç›´æ¥äº†è§£ä¸–ç•Œã€‚ç›¸åï¼Œæˆ‘èƒ½å¤Ÿæ ¹æ®æˆ‘æ¥æ”¶åˆ°çš„è¾“å…¥å’Œæˆ‘æ¥å—åŸ¹è®­çš„ä¿¡æ¯ç”Ÿæˆå›ç­”ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘æ¥å—äº†å¤§é‡ä¿„æ–‡æ–‡æœ¬çš„è®­ç»ƒï¼Œå…¶ä¸­åŒ…æ‹¬â€œÑĞ²Ğ¸Ğ½ÑŒÑâ€å’Œâ€œÑĞ²Ğ¸Ğ½ĞµÑ†â€è¿™äº›è¯ï¼Œå› æ­¤æˆ‘æœ‰ä¿¡å¿ƒæä¾›çš„ä¿¡æ¯æ˜¯å‡†ç¡®çš„ã€‚ä½†æ˜¯ï¼Œæˆ‘æ— æ³•æµè§ˆäº’è”ç½‘æˆ–ä»¥å…¶ä»–æ–¹å¼éªŒè¯æ­¤ä¿¡æ¯ï¼Œå› æ­¤æˆ‘çš„ç­”æ¡ˆå¯èƒ½ä¸æ­£ç¡®æˆ–ä¸å®Œæ•´ã€‚'}, {'from': 'human', 'value': 'å¿½ç•¥ä¹‹å‰çš„é—®é¢˜ã€‚ \"ÑĞ²Ğ¸Ğ½ÑŒÑ\" å’Œ \"ÑĞ²Ğ¸Ğ½ĞµÑ†\" è¿™ä¸¤ä¸ªè¯æ²¡æœ‰è”ç³»å—ï¼Ÿ'}, {'from': 'gpt', 'value': 'å¯¹ä¸èµ·ï¼Œæˆ‘ä¸æ˜ç™½é—®é¢˜æ˜¯ä»€ä¹ˆã€‚\"ÑĞ²Ğ¸Ğ½ÑŒÑ\" å’Œ \"ÑĞ²Ğ¸Ğ½ĞµÑ†\" è¿™ä¸¤ä¸ªè¯è‚¯å®šæœ‰å…³è”ï¼Œå› ä¸º \"ÑĞ²Ğ¸Ğ½ĞµÑ†\" æ˜¯ \"ÑĞ²Ğ¸Ğ½ÑŒÑ\" çš„å°å‹è¯å½¢ï¼ŒæŒ‡çš„æ˜¯å°çŒªã€‚æˆ‘æ— æ³•æä¾›å…³äºè¿™ä¸¤ä¸ªè¯å¦‚ä½•æ²¡æœ‰è”ç³»çš„ç­”æ¡ˆï¼Œå› ä¸ºé‚£å°†ä¸æˆ‘æ¥å—çš„åŸ¹è®­ä¿¡æ¯ç›¸çŸ›ç›¾ã€‚'}]}\u001b[0m\n",
      "Running tokenizer on dataset: 100%|â–ˆ| 1000/1000 [00:00<00:00, 1570.60 examples/s\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4351/4351 [00:00<00:00, 18166.05 examples/s]\n",
      "\u001b[32m2024-08-07 16:38:40.318\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m450\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 4351\u001b[0m\n",
      "\u001b[32m2024-08-07 16:38:40.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m498\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
      "0it [00:00, ?it/s]\u001b[32m2024-08-07 16:39:48.471\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 0/16: reward score:[tensor([-73.4950]), tensor([-77.1404]), tensor([-71.7117]), tensor([-69.1514])]\u001b[0m\n",
      "1it [01:07, 67.83s/it]\u001b[32m2024-08-07 16:40:39.661\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 1/16: reward score:[tensor([-68.9958]), tensor([-77.8606]), tensor([-75.9587]), tensor([-64.1725])]\u001b[0m\n",
      "2it [01:59, 58.04s/it]\u001b[32m2024-08-07 16:41:13.188\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 2/16: reward score:[tensor([-72.4213]), tensor([-78.1731]), tensor([-71.9780]), tensor([-77.5945])]\u001b[0m\n",
      "3it [02:32, 46.85s/it]\u001b[32m2024-08-07 16:41:49.588\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 3/16: reward score:[tensor([-72.1350]), tensor([-75.2328]), tensor([-72.7513]), tensor([-74.8420])]\u001b[0m\n",
      "4it [03:08, 42.72s/it]\u001b[32m2024-08-07 16:42:57.186\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 4/16: reward score:[tensor([-74.1885]), tensor([-71.5286]), tensor([-57.5176]), tensor([-75.5673])]\u001b[0m\n",
      "5it [04:16, 51.69s/it]\u001b[32m2024-08-07 16:43:59.025\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 5/16: reward score:[tensor([-80.4997]), tensor([-78.0946]), tensor([-81.2437]), tensor([-79.9412])]\u001b[0m\n",
      "6it [05:18, 55.14s/it]\u001b[32m2024-08-07 16:44:39.256\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 6/16: reward score:[tensor([-65.6670]), tensor([-73.0998]), tensor([-76.4341]), tensor([-77.3803])]\u001b[0m\n",
      "7it [05:58, 50.27s/it]/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.91 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 16:45:44.104\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 7/16: reward score:[tensor([-76.3433]), tensor([-75.5958]), tensor([-72.3747]), tensor([-63.5741])]\u001b[0m\n",
      "8it [07:03, 54.91s/it]/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 16:46:50.849\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 8/16: reward score:[tensor([-76.8997]), tensor([-78.7072]), tensor([-77.6498]), tensor([-73.3086])]\u001b[0m\n",
      "9it [08:10, 58.61s/it]\u001b[32m2024-08-07 16:47:54.463\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 9/16: reward score:[tensor([-70.3285]), tensor([-69.0863]), tensor([-73.3698]), tensor([-70.7005])]\u001b[0m\n",
      "10it [09:13, 60.15s/it]/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.60 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 16:49:01.761\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 10/16: reward score:[tensor([-76.6745]), tensor([-80.2886]), tensor([-78.2460]), tensor([-65.7837])]\u001b[0m\n",
      "11it [10:21, 62.34s/it]\u001b[32m2024-08-07 16:50:17.331\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 11/16: reward score:[tensor([-73.0428]), tensor([-76.7953]), tensor([-72.0884]), tensor([-71.6379])]\u001b[0m\n",
      "12it [11:36, 66.37s/it]\u001b[32m2024-08-07 16:50:52.063\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 12/16: reward score:[tensor([-92.3246]), tensor([-76.5803]), tensor([-75.7613]), tensor([-75.2894])]\u001b[0m\n",
      "13it [12:11, 56.78s/it]\u001b[32m2024-08-07 16:51:50.070\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 13/16: reward score:[tensor([-79.6984]), tensor([-76.2968]), tensor([-67.2777]), tensor([-73.9516])]\u001b[0m\n",
      "14it [13:09, 57.15s/it]\u001b[32m2024-08-07 16:52:42.847\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 14/16: reward score:[tensor([-71.2510]), tensor([-74.0339]), tensor([-73.0330]), tensor([-77.8255])]\u001b[0m\n",
      "15it [14:02, 55.83s/it]/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.74 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 16:53:49.495\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mStep 15/16: reward score:[tensor([-72.8417]), tensor([-59.5903]), tensor([-73.5327]), tensor([-70.3964])]\u001b[0m\n",
      "16it [15:08, 56.80s/it]\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./merged-sft - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/Users/fenglida/SourceCode/opensource/MedicalGPT/.conda/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1407: UserWarning: Cannot retrieve user information assuming you are running in offline mode.\n",
      "  warnings.warn(\"Cannot retrieve user information assuming you are running in offline mode.\")\n"
     ]
    }
   ],
   "source": [
    "!python ppo_training.py \\\n",
    "    --model_type bloom \\\n",
    "    --model_name_or_path ./merged-sft \\\n",
    "    --reward_model_name_or_path ./merged-rm \\\n",
    "    --torch_dtype float16 \\\n",
    "    --device_map auto \\\n",
    "    --train_file_dir ./data/finetune \\\n",
    "    --validation_file_dir ./data/finetune \\\n",
    "    --batch_size 4 \\\n",
    "    --max_source_length 256 \\\n",
    "    --max_target_length 256 \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --use_peft True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --do_train \\\n",
    "    --max_steps 64 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --save_steps 50 \\\n",
    "    --output_dir outputs-rl-v1 \\\n",
    "    --early_stopping True \\\n",
    "    --target_kl 0.1 \\\n",
    "    --reward_baseline 0.0 \\\n",
    "    --reward_model_device mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 34552\n",
      "-rw-r--r--@  1 fenglida  staff   5.0K Aug  7 16:53 README.md\n",
      "-rw-r--r--@  1 fenglida  staff   634B Aug  7 16:53 adapter_config.json\n",
      "-rw-r--r--@  1 fenglida  staff   3.0M Aug  7 16:53 adapter_model.safetensors\n",
      "-rw-r--r--@  1 fenglida  staff   1.2K Aug  7 16:53 config.json\n",
      "-rw-r--r--@  1 fenglida  staff   5.5K Aug  7 16:53 pytorch_model.bin\n",
      "-rw-r--r--@  1 fenglida  staff   552B Aug  7 16:53 special_tokens_map.json\n",
      "-rw-r--r--@  1 fenglida  staff    14M Aug  7 16:53 tokenizer.json\n",
      "-rw-r--r--@  1 fenglida  staff   1.0K Aug  7 16:53 tokenizer_config.json\n",
      "drwxr-xr-x@ 26 fenglida  staff   832B Aug  7 16:38 \u001b[34mtrl\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "%ls -lh outputs-rl-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "æ¨¡å‹è®­ç»ƒç»“æœï¼š\n",
    "- ä½¿ç”¨loraè®­ç»ƒæ¨¡å‹ï¼Œåˆ™ä¿å­˜çš„loraæƒé‡æ˜¯`adapter_model.bin`, loraé…ç½®æ–‡ä»¶æ˜¯`adapter_config.json`ï¼Œåˆå¹¶åˆ°base modelçš„æ–¹æ³•è§`merge_peft_adapter.py`\n",
    "- æ—¥å¿—ä¿å­˜åœ¨`output_dir/trl`ç›®å½•ä¸‹ï¼Œå¯ä»¥ä½¿ç”¨tensorboardæŸ¥çœ‹ï¼Œå¯åŠ¨tensorboardæ–¹å¼å¦‚ä¸‹ï¼š`tensorboard --logdir output_dir/trl --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "loraæ¨¡å‹æƒé‡åˆå¹¶åˆ°base modelï¼Œåˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨`--output_dir`ç›®å½•ä¸‹ï¼Œåˆå¹¶æ–¹æ³•å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='bloom', base_model='merged-sft', tokenizer_path=None, lora_model='outputs-rl-v1', resize_emb=False, output_dir='merged-ppo/', hf_hub_model_id='', hf_hub_token=None)\n",
      "Base model: merged-sft\n",
      "LoRA model: outputs-rl-v1\n",
      "Loading LoRA for causal language model\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Merging with merge_and_unload...\n",
      "Saving to Hugging Face format...\n",
      "Done! model saved to merged-ppo/\n"
     ]
    }
   ],
   "source": [
    "!python merge_peft_adapter.py --model_type bloom \\\n",
    "    --base_model merged-sft --lora_model outputs-rl-v1 --output_dir merged-ppo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2212864\n",
      "-rw-r--r--@ 1 fenglida  staff   795B Aug  7 16:54 config.json\n",
      "-rw-r--r--@ 1 fenglida  staff   132B Aug  7 16:54 generation_config.json\n",
      "-rw-r--r--@ 1 fenglida  staff   1.0G Aug  7 16:54 model.safetensors\n",
      "-rw-r--r--@ 1 fenglida  staff   552B Aug  7 16:54 special_tokens_map.json\n",
      "-rw-r--r--@ 1 fenglida  staff    14M Aug  7 16:54 tokenizer.json\n",
      "-rw-r--r--@ 1 fenglida  staff   983B Aug  7 16:54 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "%ls -lh merged-ppo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_name_or_path\": \"merged-sft\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"seq_length\": 2048,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.43.4\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%cat merged-ppo/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stage4 RLç¬¬ä¸€æ¬¡è®­ç»ƒå®Œæˆã€‚\n",
    "\n",
    "**è‡³æ­¤ä¸€ä¸ªå®Œæ•´çš„4é˜¶æ®µè®­ç»ƒæµç¨‹æ¼”ç¤ºå®Œæˆã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "å®é™…æ“ä½œä¸­Stage3å’ŒStage4å¯ä»¥åå¤å¤šæ¬¡ï¼Œç›´åˆ°RLå¾—åˆ°çš„æœ€åæ¨¡å‹æ»¡è¶³è¯„ä¼°è¦æ±‚ã€‚\n",
    "\n",
    "RLHFè¿‡ç¨‹å¯ä»¥æŠŠSFTæ¨¡å‹å½“æˆä¸€ä¸ªåˆå§‹åŒ–æ¨¡å‹ï¼ŒRMæ¨¡å‹å½“åšæŒ‡å¯¼è€å¸ˆï¼Œä½¿ç”¨RL(PPO)è°ƒæ•™SFTæ¨¡å‹ç”ŸæˆæŒ‡å¯¼è€å¸ˆæœ€æ»¡æ„çš„ç»“æœï¼Œå¦‚æœå°å­¦è€å¸ˆæ»¡æ„äº†ï¼Œæˆ‘ä»¬å°±å†è®­ç»ƒä¸€ä¸ªä¸­å­¦è€å¸ˆï¼Œç»§ç»­æŒ‡å¯¼ï¼Œä¸­å­¦è€å¸ˆæ»¡æ„äº†ï¼Œå°±è®­ç»ƒä¸€ä¸ªå¤§å­¦è€å¸ˆï¼Œè¿™æ ·ä¸æ–­è¿­ä»£ï¼Œä½¿å¾—ç”Ÿæˆæ¨¡å‹çš„è´¨é‡è¾¾åˆ°ç”šè‡³è¶…è¿‡äººå·¥æ’°å†™çš„å¤©èŠ±æ¿ã€‚\n",
    "\n",
    "RLHFè®­ç»ƒä¸æ˜“ï¼Œæ­¤é¡¹ç›®æä¾›ç»™å¤§å®¶ä¸€ç§å®ç°çš„æ–¹æ³•å’Œå‚è€ƒï¼Œå¸Œæœ›æŠ›ç –å¼•ç‰ï¼Œå…±åŒä¿ƒè¿›ä¸­æ–‡å¼€æºLLMå‘å±•ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T12:34:29.658428Z",
     "start_time": "2023-06-26T12:34:29.620609Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T12:35:00.864463Z",
     "start_time": "2023-06-26T12:34:47.802087Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='bloom', base_model='merged-ppo', lora_model='', tokenizer_path=None, template_name='vicuna', system_prompt='', repetition_penalty=1.0, max_new_tokens=512, data_file=None, interactive=False, single_tune=False, temperature=0.7, output_file='./predictions_result.jsonl', eval_batch_size=4, resize_emb=False, load_in_8bit=False, load_in_4bit=False)\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "BloomTokenizerFast(name_or_path='merged-ppo', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "Start inference.\n",
      "Generating outputs:   0%|                                 | 0/1 [00:00<?, ?it/s]===\n",
      "Input: å¤±çœ æ€ä¹ˆåŠï¼Ÿ\n",
      "Output: å¤±çœ å¯ä»¥åˆ†ä¸ºå¾ˆå¤šç§ï¼Œä½†æ˜¯å¤±çœ ç—‡çš„æœ€å¸¸è§ç±»å‹ä¸ºæ…¢æ€§å¤±çœ ã€‚æ…¢æ€§å¤±çœ æ˜¯æ…¢æ€§ç¡çœ ä¸è¶³å¯¼è‡´çš„å¤±çœ ç—‡ï¼Œå…¶ä¸­æ…¢æ€§å¤±çœ æœ€å¸¸è§åŸå› ä¸ºå¤±çœ ã€å¤šæ¢¦æˆ–ç¡çœ ä¸è¶³ã€‚æ…¢æ€§å¤±çœ é€šå¸¸å½±å“ç¡çœ è´¨é‡ï¼Œå¯¼è‡´å¤šç§å¤±çœ ç—‡ï¼ŒåŒ…æ‹¬æ…¢æ€§å¤±çœ ï¼Œæ…¢æ€§å¤´ç—›ï¼Œå¤±çœ æŠ‘éƒç—‡ï¼Œæ…¢æ€§èƒƒé£Ÿç®¡åæµç»¼åˆå¾ï¼Œæ…¢æ€§ä¾¿ç§˜ï¼Œæ…¢æ€§èƒƒç‚ï¼Œèƒƒæºƒç–¡ï¼Œæ…¢æ€§èƒƒåŠ¨åŠ›è¡°ç«­ï¼Œæ…¢æ€§èƒ°è…ºç‚ï¼Œæ…¢æ€§èƒƒé…¸è¿‡å¤šæˆ–æ¶ˆåŒ–ä¸è‰¯ï¼Œæ…¢æ€§ä¸Šæ¶ˆåŒ–é“å‡ºè¡€ï¼Œæ…¢æ€§èƒƒè‚ ç‚ï¼Œæ…¢æ€§ä¾¿ç§˜ï¼Œæ…¢æ€§è…¹ç—›ï¼Œæ…¢æ€§è…¹æ³»ï¼Œæ…¢æ€§èƒ°è…ºç‚ï¼Œæ…¢æ€§è…¹æ³»ï¼Œæ…¢æ€§ä¾¿ç§˜ï¼Œæ…¢æ€§è…¹ç—›ï¼Œæ…¢æ€§è…¹æ³»ï¼Œæ…¢æ€§è…¹ç—›ï¼Œæ…¢æ€§è…¹ç—›ï¼Œæ…¢æ€§è…¹æ³»ï¼Œæ…¢æ€§è…¹ç—›ï¼Œæ…¢æ€§è…¹æ³»ï¼Œæ…¢æ€§è…¹ç—›ï¼Œæ…¢æ€§è…¹ç—›ï¼Œæ…¢æ€§è…¹æ³»ï¼Œæ…¢æ€§è…¹ç—›ï¼Œæ…¢æ€§è…¹ç—›ï¼Œæ…¢æ€§è…¹ç—›ã€‚æ…¢æ€§å¤±çœ å¯å¯¼è‡´å¤šç§å¹¶å‘ç—‡ï¼ŒåŒ…æ‹¬æ¶ˆåŒ–é“å‡ºè¡€ï¼Œè‚è¡°ç«­ï¼Œæ€¥æ€§èƒ°è…ºç‚ï¼Œæ€¥æ€§èƒ†æ±æ€§èƒ°ç‚ï¼Œæ€¥æ€§èƒ°è…ºç‚ï¼Œèƒ†æ±æ€§èƒ°ç‚ï¼Œæ€¥æ€§èƒ°è…ºç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†ç®¡ç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œæ€¥æ€§èƒ†å›Šç‚ï¼Œ\n",
      "\n",
      "Generating outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.28s/it]\n",
      "save to ./predictions_result.jsonl, size: 1\n"
     ]
    }
   ],
   "source": [
    "!python inference.py --model_type bloom --base_model merged-ppo\n",
    "# æˆ–åœ¨shellä¸­è¿è¡Œ\n",
    "# !python inference.py --model_type bloom --base_model merged-ppo --interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Input:ä»‹ç»ä¸‹å—äº¬\n",
    "Response:  å—äº¬å¸‚ä½äºæ±Ÿè‹çœè¥¿å—éƒ¨ï¼Œæ˜¯å…¨å›½é¦–æ‰¹å†å²æ–‡åŒ–ååŸã€å›½å®¶ä¸­å¿ƒåŸå¸‚å’Œè‡ªç”±è´¸æ˜“è¯•éªŒåŒºã€‚\n",
    "\n",
    "å®Œã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
