{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training Pipeline\n",
    "[run_training_dpo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stage 1: Continue Pretraining\n",
    "\n",
    "第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练GPT模型，以适配领域数据分布\n",
    "\n",
    "注意：\n",
    "1. 此阶段是可选的，如果你没有海量领域文本，可以跳过此阶段，直接进行SFT阶段的有监督微调\n",
    "2. 我实验发现：做领域知识注入，SFT比PT更高效，也可以跳过PT阶段\n",
    "\n",
    "| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m`\n",
    "2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置运行环境\n",
    "\n",
    "本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境\n",
    "\n",
    "colab建议使用T4 GPU训练，设置方式：`代码执行程序 -> 更改运行时类型 -> 运行时类型：Python3，硬件加速器：GPU，GPU类型：T4 -> 保存`\n",
    "\n",
    "步骤：\n",
    "1. 下载最新代码到本地\n",
    "2. 安装依赖包\n",
    "\n",
    "依赖包如下，保证最新版本：\n",
    "\n",
    "```\n",
    "loguru\n",
    "transformers\n",
    "sentencepiece\n",
    "datasets\n",
    "tensorboard\n",
    "tqdm\n",
    "peft\n",
    "trl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITATION.cff                       merge_peft_adapter.py\n",
      "CONTRIBUTING.md                    merge_tokenizers.py\n",
      "DISCLAIMER                         openai_api.py\n",
      "LICENSE                            orpo_training.py\n",
      "README.md                          ppo_training.py\n",
      "README_EN.md                       pretraining.py\n",
      "_config.yml                        requirements.txt\n",
      "build_domain_tokenizer.py          reward_modeling.py\n",
      "chatpdf.py                         \u001b[34mrole_play_data\u001b[m\u001b[m/\n",
      "convert_dataset.py                 run_dpo.sh\n",
      "\u001b[34mdata\u001b[m\u001b[m/                              run_full_sft.sh\n",
      "deepspeed_zero_stage2_config.json  run_orpo.sh\n",
      "deepspeed_zero_stage3_config.json  run_ppo.sh\n",
      "\u001b[34mdocs\u001b[m\u001b[m/                              run_pt.sh\n",
      "dpo_training.py                    run_rm.sh\n",
      "fastapi_server_demo.py             run_sft.sh\n",
      "full_supervised_finetuning.py      run_training_dpo_pipeline.ipynb\n",
      "gradio_demo.py                     run_training_ppo_pipeline.ipynb\n",
      "inference.py                       supervised_finetuning.py\n",
      "inference_multigpu_demo.py         template.py\n",
      "Requirement already satisfied: accelerate~=0.27.2 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.27.2)\n",
      "Requirement already satisfied: datasets>=2.14.6 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.20.0)\n",
      "Requirement already satisfied: loguru in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.7.2)\n",
      "Requirement already satisfied: peft~=0.10.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (1.5.1)\n",
      "Requirement already satisfied: tensorboard in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.17.0)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (4.66.5)\n",
      "Requirement already satisfied: transformers>=4.39.3 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (4.43.4)\n",
      "Requirement already satisfied: trl~=0.8.3 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.8.6)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (0.24.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (0.4.4)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.10.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 7)) (1.65.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 7)) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 7)) (4.25.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 7)) (69.5.1)\n",
      "Requirement already satisfied: six>1.9 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 7)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 7)) (3.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from transformers>=4.39.3->-r requirements.txt (line 9)) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from transformers>=4.39.3->-r requirements.txt (line 9)) (0.19.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from trl~=0.8.3->-r requirements.txt (line 10)) (0.8.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (2.3.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (24.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from huggingface-hub->accelerate~=0.27.2->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2024.7.4)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10)) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10)) (13.7.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10)) (1.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 7)) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10)) (2.18.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/llama3-ft/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10)) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# !git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
    "# %cd MedicalGPT\n",
    "%ls\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage1 咱们开始吧\n",
    "\n",
    "训练步骤如下：\n",
    "\n",
    "1. 确认训练集\n",
    "2. 执行训练脚本\n",
    "\n",
    "训练脚本的执行逻辑如下：\n",
    "1. 导入依赖包\n",
    "2. 设置参数\n",
    "3. 定义各函数并加载训练集\n",
    "4. 加载模型和tokenizer\n",
    "5. 开始训练并评估\n",
    "6. 查看训练结果\n",
    "\n",
    "**以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_article_tail500.txt  fever.txt               tianlongbabu.txt\n"
     ]
    }
   ],
   "source": [
    "%ls ./data/pretrain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1586, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 221, in <module>\n",
      "    from accelerate.utils import (\n",
      "ImportError: cannot import name 'is_mlu_available' from 'accelerate.utils' (/opt/anaconda3/lib/python3.12/site-packages/accelerate/utils/__init__.py). Did you mean: 'is_mps_available'?\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fenglida/SourceCode/opensource/MedicalGPT/MedicalGPT/pretraining.py\", line 33, in <module>\n",
      "    from transformers import (\n",
      "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1576, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1588, in _get_module\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):\n",
      "cannot import name 'is_mlu_available' from 'accelerate.utils' (/opt/anaconda3/lib/python3.12/site-packages/accelerate/utils/__init__.py)\n"
     ]
    }
   ],
   "source": [
    "!python pretraining.py \\\n",
    "    --model_type bloom \\\n",
    "    --model_name_or_path bigscience/bloomz-560m \\\n",
    "    --train_file_dir ./data/pretrain \\\n",
    "    --validation_file_dir ./data/pretrain \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --per_device_eval_batch_size 3 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --seed 42 \\\n",
    "    --fp16 \\\n",
    "    --max_train_samples 20000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 50 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --save_steps 500 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 1 \\\n",
    "    --block_size 128 \\\n",
    "    --group_by_length True \\\n",
    "    --output_dir outputs-pt-v1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float16 \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --ddp_find_unused_parameters False \\\n",
    "    --gradient_checkpointing True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls -lh outputs-pt-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
    "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python merge_peft_adapter.py --model_type bloom \\\n",
    "    --base_model bigscience/bloomz-560m --lora_model outputs-pt-v1 --output_dir merged-pt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls -lh merged-pt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cat merged-pt/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage1 增量预训练完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:56:17.081153Z",
     "start_time": "2023-06-15T13:56:17.032821Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 2: Supervised FineTuning\n",
    "\n",
    "第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\n",
    "\n",
    "| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m` 或者 Stage1得到的预训练模型\n",
    "2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage2 咱们开始吧\n",
    "\n",
    "训练步骤如下：\n",
    "\n",
    "1. 确认训练集\n",
    "2. 执行训练脚本\n",
    "\n",
    "训练脚本的执行逻辑如下：\n",
    "1. 导入依赖包\n",
    "2. 设置参数\n",
    "3. 定义各函数并加载训练集\n",
    "4. 加载模型和tokenizer\n",
    "5. 开始训练并评估\n",
    "6. 查看训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:58:38.966506Z",
     "start_time": "2023-06-15T13:58:38.778132Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medical_sft_1K_format.jsonl  sharegpt_zh_1K_format.jsonl\n"
     ]
    }
   ],
   "source": [
    "%ls ./data/finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python supervised_finetuning.py \\\n",
    "    --model_type bloom \\\n",
    "    --model_name_or_path merged-pt \\\n",
    "    --train_file_dir ./data/finetune \\\n",
    "    --validation_file_dir ./data/finetune \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --fp16 \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.05 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 50 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --save_steps 500 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 1 \\\n",
    "    --output_dir outputs-sft-v1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float16 \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --ddp_find_unused_parameters False \\\n",
    "    --gradient_checkpointing True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls -lh outputs-sft-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
    "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python merge_peft_adapter.py --model_type bloom \\\n",
    "    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir ./merged-sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls -lh merged-sft/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cat merged-sft/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stage2 SFT训练完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T14:07:40.752635Z",
     "start_time": "2023-06-15T14:07:40.731186Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 3: DPO(Direct Preference Optimization)\n",
    "\n",
    "第三阶段：DPO(Direct Preference Optimization)直接偏好优化，DPO通过直接优化语言模型来实现对其行为的精确控制，而无需使用复杂的强化学习，也可以有效学习到人类偏好，DPO相较于RLHF更容易实现且易于训练，效果更好\n",
    "\n",
    "| Stage 3: Direct Preference Optimization        |  [dpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/dpo_training.py) | [run_dpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_dpo.sh)    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m` 或者 Stage2得到的SFT模型\n",
    "2. 数据集：DPO阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage3 咱们开始吧\n",
    "\n",
    "训练步骤如下：\n",
    "\n",
    "1. 确认训练集\n",
    "2. 执行训练脚本\n",
    "\n",
    "训练脚本的执行逻辑如下：\n",
    "1. 导入依赖包\n",
    "2. 设置参数\n",
    "3. 定义各函数并加载训练集\n",
    "4. 加载模型和tokenizer\n",
    "5. 开始训练并评估\n",
    "6. 查看训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls ./data/reward/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python dpo_training.py \\\n",
    "    --model_type bloom \\\n",
    "    --model_name_or_path ./merged-sft \\\n",
    "    --train_file_dir ./data/reward \\\n",
    "    --validation_file_dir ./data/reward \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --max_eval_samples 500 \\\n",
    "    --max_steps 100 \\\n",
    "    --eval_steps 10 \\\n",
    "    --save_steps 50 \\\n",
    "    --max_source_length 256 \\\n",
    "    --max_target_length 256 \\\n",
    "    --output_dir outputs-dpo-v1 \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float16 \\\n",
    "    --fp16 True \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --remove_unused_columns False \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --cache_dir ./cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls -lh outputs-dpo-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
    "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python merge_peft_adapter.py --model_type bloom \\\n",
    "    --base_model merged-sft --lora_model outputs-dpo-v1 --output_dir merged-dpo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls -lh merged-dpo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cat merged-dpo/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stage3 偏好建模第一次训练完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**至此一个完整的训练流程演示完成。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T12:34:29.658428Z",
     "start_time": "2023-06-26T12:34:29.620609Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T12:35:00.864463Z",
     "start_time": "2023-06-26T12:34:47.802087Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python inference.py --model_type bloom --base_model merged-dpo\n",
    "# 或在shell中运行\n",
    "# python inference.py --model_type bloom --base_model merged-dpo --interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Input:介绍下南京\n",
    "Response:  南京市位于江苏省西南部，是全国首批历史文化名城、国家中心城市和自由贸易试验区。\n",
    "\n",
    "完。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
